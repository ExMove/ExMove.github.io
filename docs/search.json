[
  {
    "objectID": "FAQs.html",
    "href": "FAQs.html",
    "title": "FAQ’s",
    "section": "",
    "text": "Below is a list of common issues/errors that you might come across when using the workflow we provide. It’s not exhaustive, but hopefully helpful for solving problems that are likely to occur if you are analysing your own data."
  },
  {
    "objectID": "FAQs.html#introduction",
    "href": "FAQs.html#introduction",
    "title": "FAQ’s",
    "section": "",
    "text": "Below is a list of common issues/errors that you might come across when using the workflow we provide. It’s not exhaustive, but hopefully helpful for solving problems that are likely to occur if you are analysing your own data."
  },
  {
    "objectID": "FAQs.html#getting-the-workflow-up-and-running",
    "href": "FAQs.html#getting-the-workflow-up-and-running",
    "title": "FAQ’s",
    "section": "1 Getting the workflow up and running",
    "text": "1 Getting the workflow up and running\n\n1.1 GitHub issues\nUsing GitHub is not essential for using this workflow — you can download the repo from  &gt; Download Zip, unzip to your machine, and start by opening the .Rproj file.\nHowever, if you want to contribute improvements to the repo, or are just interested in getting to grips with GitHub for your own research projects, there are great resources online for getting started (see: setting up GitHub | cloning a repo | using GitHub with RStudio).\n\n\n1.2 Using RStudio projects and here for reproducible filepaths\nWhen we open a .Rproj file (which should be the starting point for this workflow), this opens up a fresh instance of RStudio, with a dedicated project environment and access to all the folders and files contained inside that project folder. If you’re not familiar with using RStudio projects, have a look at this section of R for Data Science for a quick explained.\nBy combining Rstudio projects with the here package, we can create relative filepaths that work across machines, so you won’t have to edit a complicated filepath to get the code to run (for more info see the ‘Getting started’ section of the here documentation)."
  },
  {
    "objectID": "FAQs.html#reporting-in-a-paper",
    "href": "FAQs.html#reporting-in-a-paper",
    "title": "FAQ’s",
    "section": "2 Reporting in a paper",
    "text": "2 Reporting in a paper\n\n2.1 How to report usage of this workflow in a paper\nIf you have used this workflow and you wish to report this in a scientific publication then we provide the text below as a suggestion as to how the user might do so.\n“Animal tracking data in this study was cleaned following the key principles of clean data in Langley et al. (2023). Duplicated and erroneous data points were removed, a maximum speed filter of XXm/s applied and data YYhours after initial deployment removed”"
  },
  {
    "objectID": "FAQs.html#data-problems",
    "href": "FAQs.html#data-problems",
    "title": "FAQ’s",
    "section": "3 Data problems",
    "text": "3 Data problems\n\n3.1 What if I have multiple data files for an individual?\nLots of data sets include multiple files for a single individual, when individuals are tracked more than once. The workflow can filter raw tracking data over multiple deployments. For it to work, make sure to check:\n\nThe raw data contain individual ID, read in from the file name\nThe metadata includes one row per unique tag deployment per individual\nThe metadata contains a column with a unique value for each deployment (e.g., DeployID/Year)\nThis identifying column is included in df_metadataslim\n\nWhen joining the raw data and metadata, the raw data will be duplicated for each row of ID in the metadata (i.e., for each DeployID/Year). When we filter the data to the deployment period only (df_clean) the duplicated rows, where tracking DateTimes are outside of the individual’s deployment period, are removed.\nAn example of how to use multiple ID columns in this way is included in the troubleshooting script, named Troubleshoot - multiple ID columns.R.\n\n\n3.2 What if my tags have been re-deployed on multiple individuals?\nSometimes tags are used multiple times on different individuals, but data are downloaded in a single file per tag. It is much better practice to run all processing in R, rather than splitting the files prior to this workflow. With a couple of tweaks, the workflow can filter raw tracking data files containing multiple deployments:\n\nMake sure TagID is included in the file name, instead of individual ID\nWhen reading in the raw data, replace ID with TagID\nMake sure the metadata includes one row per unique tag deployment per individual\nMake sure the metadata file and df_metadataslim contain both ID and TagID\nWhen combining raw and metadata, left_join by TagID, instead of ID:\n\n\ndf_metamerged &lt;- df_raw %&gt;%\n  left_join(., df_metadataslim, by=\"TagID\") \n\nWhen joining the raw data and metadata, the raw data will be duplicated for each row of TagID in the metadata (i.e., for each ID). When we filter the data to the deployment period only (df_clean) the duplicated rows, where tracking DateTimes are outside of the individual’s deployment period, are removed.\nAn example of how to use multiple ID columns in this way is included in the troubleshooting script, named Troubleshoot - multiple ID columns.R."
  },
  {
    "objectID": "FAQs.html#geographic-projection-issues",
    "href": "FAQs.html#geographic-projection-issues",
    "title": "FAQ’s",
    "section": "4 Geographic projection issues",
    "text": "4 Geographic projection issues\n\n4.1 What is a CRS?\nCoordinate Reference Systems (or CRS) are the framework we use to represent locations from a spherical earth on a two-dimensional plot. Usually we want to “project” these 3D points to a 2D plot in different ways, depending on the range and scale of our spatial data, and we use CRS codes to do this easily. Two common ESPG codes we use in our workflow are LatLon (4326) and Spherical mercator/WGS (3857), but you can also use the crssuggest package to automatically determine the appropriate CRS for your data.\nCRS for sf objects are usually defined at creation (e.g. the crs = argument of st_as_sf), but can also be retrieved or replaced with st_crs, and transformed from one projection to another with st_transform.\nMore information on these concepts can be found in the sf documentation (see: The flat earth model | spherical geometry | miscellaneous)\n\n\n4.2 What does this st_intersects warning mean?\n\n\n## although coordinates are longitude/latitude, st_intersects assumes that they are planar\n\n\nMiscellaneous sf questions that could either be addressed here, or just pointed to the sf misc page"
  },
  {
    "objectID": "FAQs.html#timezones",
    "href": "FAQs.html#timezones",
    "title": "FAQ’s",
    "section": "5 Timezones",
    "text": "5 Timezones\n\n5.1 How do I find a list of timezone codes?\nTo find the code for a specific timezone, you can search the full list of tz’s that R supports by using Olsonnames():\n\nhead(OlsonNames())\n\n[1] \"Africa/Abidjan\"     \"Africa/Accra\"       \"Africa/Addis_Ababa\"\n[4] \"Africa/Algiers\"     \"Africa/Asmara\"      \"Africa/Asmera\"     \n\n\n\nLists of supported timezone codes can also be found online (e.g. wikipedia\n\n\n\n5.2 Converting between timezones\nIf you have data from different timezones and you want to convert everything to a common format, you can use with_tz from the lubridate package (example below taken from the function page):\n\nx &lt;- ymd_hms(\"2009-08-07 00:00:01\", tz = \"America/New_York\")\nwith_tz(x, \"GMT\") #converts US DatTime to UK DateTime\n\n[1] \"2009-08-07 04:00:01 GMT\""
  },
  {
    "objectID": "FAQs.html#resource-limitations",
    "href": "FAQs.html#resource-limitations",
    "title": "FAQ’s",
    "section": "6 Resource limitations",
    "text": "6 Resource limitations\n\n6.1 Vector memory exhausted\nWhen you’re working with thousands or even millions of tracking data points, sometimes you might come across the following message:\n\n\nError: vector memory exhausted (limit reached?)\n\n\nThis essentially means you’ve run out of memory (RAM), and R like a big baby is refusing to carry on. While we’d recommend first trying code testing and optimization, an easy brute-force solution is to increase the amount of memory R can access, by allocating some of your disk alongside existing RAM. We can do this by editing the .Renviron file (which R reads when it starts up):\n\nlibrary(usethis) #usethis is a package with a selection of handy functions\nusethis::edit_r_environ() #this function allows us to directly edit the .Renviron file from R Studio\n\nThen add R_MAX_VSIZE=32Gb as a line to the .Renviron file that opens up (32Gb can be changed to whatever you want, but make sure this value includes the amount of RAM on your machine), then save it and restart the R session for changes to take effect. To return to default allocation, run the code a second time and remove the line you added. More detail (and the source of the above solution) can be found on Stackoverflow."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ExMove",
    "section": "",
    "text": "Welcome to the home page of ExMove. Here, you can find the resources to use our toolkit for processing biologging data from tag downloads to online archive."
  },
  {
    "objectID": "Glossary.html",
    "href": "Glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page contains definitions for all of the user input parameters that are in the user guide.\n\n\n\nspecies_code – Throughout the workflow we use this parameter as a file/folder identifier for saving outputs. We suggest use a species or study site abbreviation. I.e. the data set for immature red-footed boobies became “RFB_IMM”.\nfilepath – This object specifies the location where the raw data files are stored. We use the here function to define the filepath for universality and to avoid issues when switching between operating systems. Ideally in this folder only the RAW tracking data files should be stored\nfilepattern – This specifies the common file pattern for the raw data files with the folder that is specified using the filepath user input parameter. If only the RAW data files are in this folder then the user can just use the file type as the common file pattern e.g. “.txt” or “.csv”. The asterisk “*” is used to match any preceding characters in the filename.\nIDstart; IDend – When setting up the data structure, we recommend saving a file for each individual animal with a unique identifier in the filename. The IDstart and IDend parameters represent numeric values denoting the start and end position of the unique individual identifier within the filenames. NB: the individual identifier should therefore be in the same location within the file name across all files.\nskiplines – this parameter denotes the number of lines at the top of the file to skip when it is read into R. The default value for this parameter is 0 so no lines are skipped. If for example you have a text file with additional info at the top that does not need to be read into R then you can increase this to the appropriate number of rows.\ndate_formats; datetime_formats – These parameters are used to specify the date format and datetime formats of the tracking data respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\ntrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones. The user should know what timezone there data was recorded in.\ncolnames – If column names are not present within the raw tracking data files, this parameter can be used to specify column names as a vector, e.g. c(“Date”, “Time”, “Lat”, “Long”). The default value is TRUE, which assumes column names are specified within the raw files.\nuser_delim - set the eliminator for your RAW data files, this will depend on the file type that your data are saved as. Text files are often tab delaminated (“) and .csv are comma separated (“,”). Other common eliminators also include “;”, “/”, “” or “|”.\nuser_trim_ws - Specify as either TRUE or FALSE. Allows white space at the bottom of files to be trimmed, e.g. empty cells at the bottom of an excel document.\nID_type – data needs a unique individual identifier to differentiate between individuals. This user input parameter allows you to specify the column name that the individual identifier will be stored in. The individual identifier will be taken from the file name using the IDstart and IDend input parameters.\ndatetime_colnames – this parameter defines the column(s) in which date and time information are stored. If date and time are stored in separate columns then specify two columns, e.g. c(“Date”, “Time”). NB: these have to be in the same order as they are stored in the data set.\n\n\n\n\n\nfilepath_meta – this parameter defines the filepath for the metadata file. Again this parameter is specified using the here function for universality.\nmetadate_formats; metadatetime_formats – These parameters are used to specify the date format and datetime formats of the metadata respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\nmetatrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones.\n\n\n\n\n\nNo_data_vals – this parameter is used to specify any values within the location (X, Y) columns that represent missing data. This is often tag-specific with common examples being “0” or “-999” but these may not always be applicable.\nna_cols – this parameter is used to define a vector of column names which cannot contain NA values in order for a row to be retained. We suggest that columns containing information relating to X location, Y location, datetime and ID should not be allowed to contain NA values.\n\n\n\n\n\ntracking_crs; meta_crs – These parameters are used to specify the co-ordinate reference systems for the tracking and metadata respectively using EPSG codes. The default is lat/long which is EPSG code 4326. For a full list of EPSG cdoes see here: https://spatialreference.org/ref/epsg/\ntransform_crs - Specify metric co-ordinate projection system to transform the data into for distance calculations with units in metres. WE STRONGLY RECCOMEND THAT YOU CHANGE THIS FOR YOUR STUDY SYSTEM/LOCATION. For advice on what to change this to see our FAQ document as the choice is nuanced and can have important implications on the metric calculated.\n\n\n\n\n\nfilepath_dfout – this parameter is used to define the file path to save out the processed tracking data file again using the here function for universality. The data is read out at this point so that it can be read into the shiny app.\nfilename_dfout – this parameter specifies the filename of the saved tracking data file.\n\n\n\n\n\nfilter_cutoff – users may want to remove a section of the tracking data immediately following tag deployment while the animal recovers from the tagging process and adjusts to the device. This parameter is used to specify the length of this period along with the units of time – e.g. minutes, hours, days.\nfilter_speed – this parameter is used to define a numeric value for the speed above which locations are classified as erroneous points and removed based on unrealistic values. The units are in metres per second as long as transform_crs was set to a projection that is in metres. The user should use ecological knowledge of their study system to set this parameter and should check the effect it has using summaries and visualisations.\nFilter_nestdisp_dist; filter_netdist_units – these parameters are used to define a threshold value for net squared displacement above which locations are considered as erroneous and select the distance units for this value e.g. metres/kilometers.\n\n\n\n\n\nsampleRateUnits – this parameter is used to define the units of time for the sampling rate calculating which is displayed in the summary table e.g. minutes/hours. Can be specified as one of the following: “secs”, “mins”, “hours”, “days” or “weeks”.\ngrouping_factors_poplevel – this parameter is used to define the grouping factors for the population-level summary table e.g. species/population/colony. This parameter can be made into a vector to include additional grouping factors such as age or sex.\ngrouping_factors_indlevel - this parameter is used to define the grouping factors for the individual-level summary table, usually individual ID. This parameter can be made into a vector to include additional grouping factors such as temporal window (e.g. month).\n\n\n\n\n\nfilepath_filtered_out - this parameter is used to define the file path to save the filtered tracking data file again using the here function for universality.\nfilepath_summary_out - this parameter is used to define the file path to save out the summary data frames again using the here function for universality.\nfilename_filtered_out; filename_summary_pop_out; filename_summary_ind_out – these parameters are used to specify the filenames for the filtered tracking data file and the population-level and individual-level summary files respectively.\n\n\n\n\n\ndevice – this parameter is used to specify the filetype for saving out visualization plots e.g. “jpeg”, “png” or ”tiff”\nunits – this parameter defines the units for saving out the visualization plots e.g. “mm” or “cm”.\ndpi – this parameter is a numeric value which specifies the resolution for saving plots in dpi. The minimum value is usually 300.\nOut_path – this parameter specifies the filepath for reading out plots using the here function.\ntopo_label – we plot maps of the telemetry data over a topography base map. This parameter specifies the legend label for the topography data depending on the study system e.g. “depth (m)” or ”elevation (m)”\n\n\n\n\n\nfilepath_final – this parameter specifies the filepath for reading the final data frame back into the main workflow once the any optional post-processing has been performed. This parameter is specified using the “here” function for universality. If no optional post-processing is performed this can be set to read in the filtered data file saved in section 9.\n\n\n\n\n\ntz_data – timezone of the final tracking data to be uploaded. In R run the function OlsonNames() to get a full list of time zones.\ngrouping_factors – a vector of grouping factors for the final reformatted tracking data. Each level will be saved as a separate file e.g. ID/Age/Sex/Species\nfilepath_dfout – the filepath for saving out the reformatted data for database upload created using the here function.\n\n\n\n\nNOTE: there are a number of user input parameters in this script that are repeats of the ones above. If you can’t find the user input parameter below then use the search function to find it above.\n\nthreshold_dist – this parameter is used to define a threshold buffer distance in metres from the central place to label points as “at the central place” or “away from the central place” in order to define foraging trips. The choice of this parameter is dependent on the study species. For instance, seabirds can sit on the sea some distance from the central place while not having started a foraging trip.\nthreshold_time – this parameter is used to define the minimum length of time in minutes required for an absence from the central location to be classified as a trip. The duration of each consecutive series of locations classified as “away from the central place” is calculated and those series with a duration less than threshold_time are removed.\nshapefilepath – this parameter is used to define the filepath to read in a shapefile used to define the extent of a central place or colony for the purposes of defining trips. This is optional and the central place can be specified alternatively using a single X, Y location.\n\n\n\n\n\nTime_unit - select the time unit to summaries and visualise sampling intervals at. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_unit – this parameter defines the time unit for the resampling the tracking data to. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_res – this parameter is a numerical value which defines the resolution which the data is resampled to. NOTE: if sub-sampling to an interval greater than 60 minutes (e.g. 2 hours) then change the unit to “hours” and resolution to 2. Do not use 120 “mins”.\n\n\n\n\n\nunits_df_datetime – this parameter specifies the units of the time difference column that will be created in the data. Due to earlier filtering the duration between successive locations is recalculated again in this script.\nthreshold_time – this parameter defines the threshold time value above which gaps in the data are split and labelled as separate segments. Therefore if the difftime column in the data set exceeds this duration then the track will be split at that point.\nthreshold_points – this parameter is a numerical value which defines the minimum number of data points required for a valid segment. If a segment is below this threshold then all the locations contained within it are removed from the data set. These small segments are often not of use for further analysis that requires segmented data with consistent sampling intervals, e.g. hidden markov models."
  },
  {
    "objectID": "Glossary.html#read-in-data-files",
    "href": "Glossary.html#read-in-data-files",
    "title": "Glossary",
    "section": "",
    "text": "species_code – Throughout the workflow we use this parameter as a file/folder identifier for saving outputs. We suggest use a species or study site abbreviation. I.e. the data set for immature red-footed boobies became “RFB_IMM”.\nfilepath – This object specifies the location where the raw data files are stored. We use the here function to define the filepath for universality and to avoid issues when switching between operating systems. Ideally in this folder only the RAW tracking data files should be stored\nfilepattern – This specifies the common file pattern for the raw data files with the folder that is specified using the filepath user input parameter. If only the RAW data files are in this folder then the user can just use the file type as the common file pattern e.g. “.txt” or “.csv”. The asterisk “*” is used to match any preceding characters in the filename.\nIDstart; IDend – When setting up the data structure, we recommend saving a file for each individual animal with a unique identifier in the filename. The IDstart and IDend parameters represent numeric values denoting the start and end position of the unique individual identifier within the filenames. NB: the individual identifier should therefore be in the same location within the file name across all files.\nskiplines – this parameter denotes the number of lines at the top of the file to skip when it is read into R. The default value for this parameter is 0 so no lines are skipped. If for example you have a text file with additional info at the top that does not need to be read into R then you can increase this to the appropriate number of rows.\ndate_formats; datetime_formats – These parameters are used to specify the date format and datetime formats of the tracking data respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\ntrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones. The user should know what timezone there data was recorded in.\ncolnames – If column names are not present within the raw tracking data files, this parameter can be used to specify column names as a vector, e.g. c(“Date”, “Time”, “Lat”, “Long”). The default value is TRUE, which assumes column names are specified within the raw files.\nuser_delim - set the eliminator for your RAW data files, this will depend on the file type that your data are saved as. Text files are often tab delaminated (“) and .csv are comma separated (“,”). Other common eliminators also include “;”, “/”, “” or “|”.\nuser_trim_ws - Specify as either TRUE or FALSE. Allows white space at the bottom of files to be trimmed, e.g. empty cells at the bottom of an excel document.\nID_type – data needs a unique individual identifier to differentiate between individuals. This user input parameter allows you to specify the column name that the individual identifier will be stored in. The individual identifier will be taken from the file name using the IDstart and IDend input parameters.\ndatetime_colnames – this parameter defines the column(s) in which date and time information are stored. If date and time are stored in separate columns then specify two columns, e.g. c(“Date”, “Time”). NB: these have to be in the same order as they are stored in the data set."
  },
  {
    "objectID": "Glossary.html#merge-with-metadata",
    "href": "Glossary.html#merge-with-metadata",
    "title": "Glossary",
    "section": "",
    "text": "filepath_meta – this parameter defines the filepath for the metadata file. Again this parameter is specified using the here function for universality.\nmetadate_formats; metadatetime_formats – These parameters are used to specify the date format and datetime formats of the metadata respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\nmetatrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones."
  },
  {
    "objectID": "Glossary.html#cleaning",
    "href": "Glossary.html#cleaning",
    "title": "Glossary",
    "section": "",
    "text": "No_data_vals – this parameter is used to specify any values within the location (X, Y) columns that represent missing data. This is often tag-specific with common examples being “0” or “-999” but these may not always be applicable.\nna_cols – this parameter is used to define a vector of column names which cannot contain NA values in order for a row to be retained. We suggest that columns containing information relating to X location, Y location, datetime and ID should not be allowed to contain NA values."
  },
  {
    "objectID": "Glossary.html#processing",
    "href": "Glossary.html#processing",
    "title": "Glossary",
    "section": "",
    "text": "tracking_crs; meta_crs – These parameters are used to specify the co-ordinate reference systems for the tracking and metadata respectively using EPSG codes. The default is lat/long which is EPSG code 4326. For a full list of EPSG cdoes see here: https://spatialreference.org/ref/epsg/\ntransform_crs - Specify metric co-ordinate projection system to transform the data into for distance calculations with units in metres. WE STRONGLY RECCOMEND THAT YOU CHANGE THIS FOR YOUR STUDY SYSTEM/LOCATION. For advice on what to change this to see our FAQ document as the choice is nuanced and can have important implications on the metric calculated."
  },
  {
    "objectID": "Glossary.html#save-df_diagnostic",
    "href": "Glossary.html#save-df_diagnostic",
    "title": "Glossary",
    "section": "",
    "text": "filepath_dfout – this parameter is used to define the file path to save out the processed tracking data file again using the here function for universality. The data is read out at this point so that it can be read into the shiny app.\nfilename_dfout – this parameter specifies the filename of the saved tracking data file."
  },
  {
    "objectID": "Glossary.html#filtering",
    "href": "Glossary.html#filtering",
    "title": "Glossary",
    "section": "",
    "text": "filter_cutoff – users may want to remove a section of the tracking data immediately following tag deployment while the animal recovers from the tagging process and adjusts to the device. This parameter is used to specify the length of this period along with the units of time – e.g. minutes, hours, days.\nfilter_speed – this parameter is used to define a numeric value for the speed above which locations are classified as erroneous points and removed based on unrealistic values. The units are in metres per second as long as transform_crs was set to a projection that is in metres. The user should use ecological knowledge of their study system to set this parameter and should check the effect it has using summaries and visualisations.\nFilter_nestdisp_dist; filter_netdist_units – these parameters are used to define a threshold value for net squared displacement above which locations are considered as erroneous and select the distance units for this value e.g. metres/kilometers."
  },
  {
    "objectID": "Glossary.html#summarise-cleaned-filtered-tracking-data",
    "href": "Glossary.html#summarise-cleaned-filtered-tracking-data",
    "title": "Glossary",
    "section": "",
    "text": "sampleRateUnits – this parameter is used to define the units of time for the sampling rate calculating which is displayed in the summary table e.g. minutes/hours. Can be specified as one of the following: “secs”, “mins”, “hours”, “days” or “weeks”.\ngrouping_factors_poplevel – this parameter is used to define the grouping factors for the population-level summary table e.g. species/population/colony. This parameter can be made into a vector to include additional grouping factors such as age or sex.\ngrouping_factors_indlevel - this parameter is used to define the grouping factors for the individual-level summary table, usually individual ID. This parameter can be made into a vector to include additional grouping factors such as temporal window (e.g. month)."
  },
  {
    "objectID": "Glossary.html#save-df_filtered-and-summary-data",
    "href": "Glossary.html#save-df_filtered-and-summary-data",
    "title": "Glossary",
    "section": "",
    "text": "filepath_filtered_out - this parameter is used to define the file path to save the filtered tracking data file again using the here function for universality.\nfilepath_summary_out - this parameter is used to define the file path to save out the summary data frames again using the here function for universality.\nfilename_filtered_out; filename_summary_pop_out; filename_summary_ind_out – these parameters are used to specify the filenames for the filtered tracking data file and the population-level and individual-level summary files respectively."
  },
  {
    "objectID": "Glossary.html#visualisation",
    "href": "Glossary.html#visualisation",
    "title": "Glossary",
    "section": "",
    "text": "device – this parameter is used to specify the filetype for saving out visualization plots e.g. “jpeg”, “png” or ”tiff”\nunits – this parameter defines the units for saving out the visualization plots e.g. “mm” or “cm”.\ndpi – this parameter is a numeric value which specifies the resolution for saving plots in dpi. The minimum value is usually 300.\nOut_path – this parameter specifies the filepath for reading out plots using the here function.\ntopo_label – we plot maps of the telemetry data over a topography base map. This parameter specifies the legend label for the topography data depending on the study system e.g. “depth (m)” or ”elevation (m)”"
  },
  {
    "objectID": "Glossary.html#post-processing-optional-steps",
    "href": "Glossary.html#post-processing-optional-steps",
    "title": "Glossary",
    "section": "",
    "text": "filepath_final – this parameter specifies the filepath for reading the final data frame back into the main workflow once the any optional post-processing has been performed. This parameter is specified using the “here” function for universality. If no optional post-processing is performed this can be set to read in the filtered data file saved in section 9."
  },
  {
    "objectID": "Glossary.html#reformat-data-for-upload-to-public-databases",
    "href": "Glossary.html#reformat-data-for-upload-to-public-databases",
    "title": "Glossary",
    "section": "",
    "text": "tz_data – timezone of the final tracking data to be uploaded. In R run the function OlsonNames() to get a full list of time zones.\ngrouping_factors – a vector of grouping factors for the final reformatted tracking data. Each level will be saved as a separate file e.g. ID/Age/Sex/Species\nfilepath_dfout – the filepath for saving out the reformatted data for database upload created using the here function."
  },
  {
    "objectID": "Glossary.html#optional-processing_central-place-trips.r",
    "href": "Glossary.html#optional-processing_central-place-trips.r",
    "title": "Glossary",
    "section": "",
    "text": "NOTE: there are a number of user input parameters in this script that are repeats of the ones above. If you can’t find the user input parameter below then use the search function to find it above.\n\nthreshold_dist – this parameter is used to define a threshold buffer distance in metres from the central place to label points as “at the central place” or “away from the central place” in order to define foraging trips. The choice of this parameter is dependent on the study species. For instance, seabirds can sit on the sea some distance from the central place while not having started a foraging trip.\nthreshold_time – this parameter is used to define the minimum length of time in minutes required for an absence from the central location to be classified as a trip. The duration of each consecutive series of locations classified as “away from the central place” is calculated and those series with a duration less than threshold_time are removed.\nshapefilepath – this parameter is used to define the filepath to read in a shapefile used to define the extent of a central place or colony for the purposes of defining trips. This is optional and the central place can be specified alternatively using a single X, Y location."
  },
  {
    "objectID": "Glossary.html#optional-processing_resampling.r",
    "href": "Glossary.html#optional-processing_resampling.r",
    "title": "Glossary",
    "section": "",
    "text": "Time_unit - select the time unit to summaries and visualise sampling intervals at. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_unit – this parameter defines the time unit for the resampling the tracking data to. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_res – this parameter is a numerical value which defines the resolution which the data is resampled to. NOTE: if sub-sampling to an interval greater than 60 minutes (e.g. 2 hours) then change the unit to “hours” and resolution to 2. Do not use 120 “mins”."
  },
  {
    "objectID": "Glossary.html#optional-processing_segmentation.r",
    "href": "Glossary.html#optional-processing_segmentation.r",
    "title": "Glossary",
    "section": "",
    "text": "units_df_datetime – this parameter specifies the units of the time difference column that will be created in the data. Due to earlier filtering the duration between successive locations is recalculated again in this script.\nthreshold_time – this parameter defines the threshold time value above which gaps in the data are split and labelled as separate segments. Therefore if the difftime column in the data set exceeds this duration then the track will be split at that point.\nthreshold_points – this parameter is a numerical value which defines the minimum number of data points required for a valid segment. If a segment is below this threshold then all the locations contained within it are removed from the data set. These small segments are often not of use for further analysis that requires segmented data with consistent sampling intervals, e.g. hidden markov models."
  }
]