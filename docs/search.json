[
  {
    "objectID": "User_guide.html",
    "href": "User_guide.html",
    "title": "ExMove user guide",
    "section": "",
    "text": "This user guide can be used as a walkthrough for reading and processing tracking data files with the Workflow.R script. You can use the example datasets provided in Data, or try with your own tracking data (see Pre-flight checks for details on data requirements and structure).\nThe following diagram gives an overview of the workflow (boxes link to relevant section):\n\n\n\n\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing': 30}}}%%\nflowchart LR\n  S1[Read in data] ==&gt; S3(Merge)\n  S2[Read in metadata] ==&gt; S3 ==&gt; A(Clean)\n  subgraph shiny [\"(Parameter determination in Shiny app)\"]\n  style shiny fill:#fbfbfb, stroke:#d3d3d3, stroke-width:px\n  A(Clean) ==&gt; B(Process) ==&gt; C(Filter)\n  D(Optional&lt;br/&gt;scripts)\n  end\n  C ==&gt; S{Standardised&lt;br/&gt;dataset}\n  C --&gt; D --&gt; S\n  S --&gt; E(Analyses)\n  S ==&gt; F(Archive)\n  linkStyle 6,7 stroke-dasharray: 4 4\n%% NOTE: remember to update links for finalised user guide!!\n  click S1 \"#read-in-data-files\";\n  click S2 \"#merge-with-metadata\";\n  click S3 \"#merge-with-metadata\";\n  click A \"#cleaning\";\n  click B \"#processing\";\n  click C \"#filtering\";\n  click S \"#save-data\";\n%%  click ? \"#summarise-data\";\n%%  click ? \"#visualisation\";\n\n\nFigure 1: Diagram of workflow used for analysing movement data (thick line denotes core path of code)\n\n\n\n\n\nThis workflow uses the R programming language, run via the R Studio IDE\n\nAll code embraces the core principles of how to structure ‚Äòtidy data‚Äô\n\nWe use RStudio projects and the here package to build relative filepaths that are reproducible\nRequires tidyverse, data.table, sf and here packages to be installed\nUse our example data sets in the Data folder (RFB_IMM, RFB, GWFG, TRPE) or provide your own data\n\nSome code chunks require editing by the user to match the specific dataset being used (particularly if you are using your own data), and are highlighted as below (the üß† indicates you will need to think about the structure and format of your data when making these edits!):\n\n\n\n\n\n\nüß† User input required\n\n\n\n\n#--------------------#\n## USER INPUT START ##\n#--------------------#\nexample_input &lt;- \"uservalue\" # In the R code, user input sections appear like this\n#------------------#\n## USER INPUT END ##\n#------------------#"
  },
  {
    "objectID": "User_guide.html#introduction",
    "href": "User_guide.html#introduction",
    "title": "ExMove user guide",
    "section": "",
    "text": "This user guide can be used as a walkthrough for reading and processing tracking data files with the Workflow.R script. You can use the example datasets provided in Data, or try with your own tracking data (see Pre-flight checks for details on data requirements and structure).\nThe following diagram gives an overview of the workflow (boxes link to relevant section):\n\n\n\n\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing': 30}}}%%\nflowchart LR\n  S1[Read in data] ==&gt; S3(Merge)\n  S2[Read in metadata] ==&gt; S3 ==&gt; A(Clean)\n  subgraph shiny [\"(Parameter determination in Shiny app)\"]\n  style shiny fill:#fbfbfb, stroke:#d3d3d3, stroke-width:px\n  A(Clean) ==&gt; B(Process) ==&gt; C(Filter)\n  D(Optional&lt;br/&gt;scripts)\n  end\n  C ==&gt; S{Standardised&lt;br/&gt;dataset}\n  C --&gt; D --&gt; S\n  S --&gt; E(Analyses)\n  S ==&gt; F(Archive)\n  linkStyle 6,7 stroke-dasharray: 4 4\n%% NOTE: remember to update links for finalised user guide!!\n  click S1 \"#read-in-data-files\";\n  click S2 \"#merge-with-metadata\";\n  click S3 \"#merge-with-metadata\";\n  click A \"#cleaning\";\n  click B \"#processing\";\n  click C \"#filtering\";\n  click S \"#save-data\";\n%%  click ? \"#summarise-data\";\n%%  click ? \"#visualisation\";\n\n\nFigure 1: Diagram of workflow used for analysing movement data (thick line denotes core path of code)\n\n\n\n\n\nThis workflow uses the R programming language, run via the R Studio IDE\n\nAll code embraces the core principles of how to structure ‚Äòtidy data‚Äô\n\nWe use RStudio projects and the here package to build relative filepaths that are reproducible\nRequires tidyverse, data.table, sf and here packages to be installed\nUse our example data sets in the Data folder (RFB_IMM, RFB, GWFG, TRPE) or provide your own data\n\nSome code chunks require editing by the user to match the specific dataset being used (particularly if you are using your own data), and are highlighted as below (the üß† indicates you will need to think about the structure and format of your data when making these edits!):\n\n\n\n\n\n\nüß† User input required\n\n\n\n\n#--------------------#\n## USER INPUT START ##\n#--------------------#\nexample_input &lt;- \"uservalue\" # In the R code, user input sections appear like this\n#------------------#\n## USER INPUT END ##\n#------------------#"
  },
  {
    "objectID": "User_guide.html#pre-flight-checks",
    "href": "User_guide.html#pre-flight-checks",
    "title": "ExMove user guide",
    "section": "0. Pre-flight checks",
    "text": "0. Pre-flight checks\nHow to use this workflow:\n\nWe will inspect the data before reading it in, so there is no need to open it in another program (e.g., excel, which can corrupt dates and times)\nUser-defined parameters (see user inputs) are called within the subsequent processing steps\nWhere you see: ## ** Option ** ##, there is an alternative version of the code to fit some common alternative data formats\nThroughout, we will use some key functions to inspect the data (e.g., head for top rows, str for column types, and names for column names)\nData and directory structure:\n\nData files should all be stored within the Data folder\nFolders and files are best named in snakecase_format as spaces in filepaths can cause issues\nTracking data for each deployment/individual should be in a separate file\nTracking data filenames should include an ID which is the same length for all individuals\nTracking data must contain a timestamp and at least one other sensor column\nMetadata file should be in the parent directory of data files\nMetadata should contain one row per individual per deployment NB: if you have multiple animal movement projects, these should have completely separate directories\n\nThe importance of ID:\n\nThroughout this workflow, we use ID to refer to the unique code for an individual animal\nIn certain cases, you might have additional ID columns in the metadata (e.g., DeployID),\nor read in data with a unique TagID instead of ID.\nThis code will work as long as all of the relevant info is included in the metadata\nFor more info and helpful code, see the FAQ document & troubleshooting script\nHow to troubleshoot problems if something doesn‚Äôt work with your data:\n\nRefer to the FAQ document in the GitHub page, which signposts to helpful resources online (e.g., CRS)\nSee the troubleshooting code scripts that we‚Äôve written to accompany this workflow (e.g., using multiple ID columns for re-deployments of tags/individuals)\nAll functions in code chunks are automatically hyperlinked to their documentation, so feel free to explore this if you want to understand more about how this code works!\nLoad required libraries\nJust before starting we load in all the packages we will need for the workflow (also referenced in the Dependencies section).\n\nlibrary(data.table) # data manipulation\nlibrary(tidyverse) # data reading, manipulation and plotting\nlibrary(lubridate) # working with date-time data\nlibrary(sf) # spatial data handling and manipulation\nlibrary(here) # reproducible filepaths"
  },
  {
    "objectID": "User_guide.html#read-in-data-files",
    "href": "User_guide.html#read-in-data-files",
    "title": "ExMove user guide",
    "section": "1. Read in data files",
    "text": "1. Read in data files\n\n\n\n\n\n\nüß† User input required\n\n\n\nThroughout the script, we‚Äôll be saving files using a species code as a file/folder identifier. Our code is taken from the first letters of the species name (Red Footed Booby) but can be anything you choose (shorter the better as long as it‚Äôs unique per species). Let‚Äôs define this object first for consistency:\n\nspecies_code &lt;- \"RFB\"\n\nSet filepath for the folder containing raw data files (this code will try to list and open all files matching the file pattern within this folder, so it is best if this folder contains only the raw data files). NB: if you are working outside of a project, you‚Äôll need to check that here is using the correct working directory.\n\nfilepath &lt;- here(\"Data\", species_code)  #create relative filepath using folder name and species code\n\nDefine common file pattern to look for. An asterisk (*) is the wildcard, will match any character except a forward-slash (e.g.¬†*.csv will import all files that end with ‚Äú.csv‚Äù).\n\nfilepattern &lt;- \"*.csv\" # data file format\n\nLet‚Äôs view the file names, to check that we have the files we want & find ID position (this list will include names of sub-folders).\n\nls_filenames &lt;- list.files(path = filepath, recursive = TRUE, pattern = filepattern)\nls_filenames\n\n[1] \"GV37501_201606_DG_RFB.csv\" \"GV37503_201606_DG_RFB.csv\"\n[3] \"GV37734_201807_NI_RFB.csv\"\n\n\nAdjust these numbers for extracting the ID number from file name using stringr (e.g.¬†to extract GV37501 from ‚ÄúGV37501_201606_DG_RFB.csv‚Äù, we want characters 1-7).  NB: this approach only works if all ID‚Äôs are the same length and in the same position ‚Äî see the str_sub documentation for other options.\n\nIDstart &lt;- 1 #start position of the ID in the filename \nIDend &lt;- 7 #end position of the ID in the filename\n\nNow, let‚Äôs inspect the data by reading in the top of the first data file as raw text. To inspect the first row of all data files (if you wanted to check column names), you can remove the [1] and change n_max = 1).\n\ntest &lt;- fs::dir_ls(path = filepath, recurse = TRUE, type = \"file\",  glob = filepattern)[1]\nread_lines(test, n_max = 5)  # change n_max to change the number of rows to read in\n\n[1] \"Date, Time, Latitude, Longitude, Altitude, Speed, Course, Type, Distance\"\n[2] \"24/06/2016,5.06,-7.261829,72.376091,56.18,3636,93,-2,0.00\"               \n[3] \"24/06/2016, 05:21:39,-7.261829,72.376091,56.18,3636,93,0,0.00\"           \n[4] \"24/06/2016, 05:29:50,-7.261744,72.376221,64.91,5112,32,0,17.17\"          \n[5] \"24/06/2016, 05:54:28,-7.261623,72.376137,-15.77,936,291,0,16.34\"         \n\n\nDefine number of lines at top of file to skip (e.g.¬†if importing a text file with additional info at top).\n\nskiplines &lt;- 0\n\nDefine date format(s) used (for passing to lubridate) (d = day as decimal, m = month as decimal, y = year without century, Y = year with century). Parsing will work the same for different date delimiters (e.g.¬†‚ÄúdmY‚Äù will work for both 01-12-2022 and 01/12/2022).  lubridate can even parse more than one date/time format within a dataframe, so if your data include multiple formats, make sure they are all included. Here, we‚Äôve included some common combinations ‚Äî modify if your data include a different format\n\ndate_formats &lt;- c(\"dmY\", \"Ymd\") #specify date formats \ndatetime_formats &lt;- c(\"dmY HMS\", \"Ymd HMS\") #specify date & time format \n\nDefine time zone for tracking data.\n\ntrackingdatatimezone &lt;- \"GMT\"\n\nBy default, the below code will find column names from the first row of data. If you want to specify your own column names, do so here as a character vector, or use set colnames &lt;- FALSE to automatically number columns.\n\ncolnames &lt;- TRUE\n\nHere, we use the function read_delim and specify the delimiter to make this code more universal (you can find extra information on this in the readr documentation).\nSome delimiter examples:\n\n\n\",\" = comma delimited (equivalent to using read_csv ‚Äì saved as extension .csv)\n\n\"\\t\" = tab delimited (equivalent to using read_tsv ‚Äî saved as extension .tsv)\n\n\" \" = whitespace delimited (equivalent to using read_table)\n\nLet‚Äôs inspect the data again, this time skipping rows if set, to check the file delimiter.\n\nread_lines(test, n_max = 5, skip = skiplines)\n\n[1] \"Date, Time, Latitude, Longitude, Altitude, Speed, Course, Type, Distance\"\n[2] \"24/06/2016,5.06,-7.261829,72.376091,56.18,3636,93,-2,0.00\"               \n[3] \"24/06/2016, 05:21:39,-7.261829,72.376091,56.18,3636,93,0,0.00\"           \n[4] \"24/06/2016, 05:29:50,-7.261744,72.376221,64.91,5112,32,0,17.17\"          \n[5] \"24/06/2016, 05:54:28,-7.261623,72.376137,-15.77,936,291,0,16.34\"         \n\n\nSet delimiter to use within read_delim.\n\nuser_delim &lt;- \",\"\nuser_trim_ws &lt;- TRUE # Should leading/trailing whitespaces be trimmed\n\nFinally, data need an ID column, either be the tag ID (‚ÄúTagID‚Äù) or individual ID (‚ÄúID‚Äù). Specify ID type here, for later matching with the same column in the metadata:\n\nID_type &lt;- \"ID\"\n\n\n\nRead in and merge all tracking data files\n\n\nMerge using ID in filename\nOption: Merge using ID already in column\n\n\n\nWith the user inputs specified in the previous section, we‚Äôll now read in and merge all tracking data files directly from the github repository, extracting the ID from the filename of each file.\n\ndf_combined &lt;- fs::dir_ls(path = filepath, # use our defined filepath\n                          glob = filepattern, # use file pattern\n                          type = \"file\",  # only list files\n                          recurse = TRUE # look inside sub-folders\n                          ) %&gt;% \n  purrr::set_names(nm = basename(.)) %&gt;% # remove path prefix\n  purrr::map_dfr(read_delim, # use read_delim function\n                 .id = \"filename\", # use filename as ID column\n                 col_types = cols(.default = \"c\"), # as character by default\n                 col_names = colnames, # use colnames object made above\n                 skip = skiplines, # how many lines to skip\n                 delim = user_delim, # define delimiter\n                 trim_ws = user_trim_ws) %&gt;% # trim characters or not\n  mutate(\"{ID_type}\" := str_sub(string = filename, # extract ID from filename \n                                start = IDstart, end = IDend), # ID position\n         .after = filename) # move the new ID column after filename column\ndf_combined\n\n# A tibble: 6,650 √ó 12\n   filename     ID    Date  Time  Latitude Longitude Altitude Speed Course Type \n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 5.06  -7.2618‚Ä¶ 72.376091 56.18    3636  93     -2   \n 2 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 05:2‚Ä¶ -7.2618‚Ä¶ 72.376091 56.18    3636  93     0    \n 3 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 05:2‚Ä¶ -7.2617‚Ä¶ 72.376221 64.91    5112  32     0    \n 4 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 05:5‚Ä¶ -7.2616‚Ä¶ 72.376137 -15.77   936   291    0    \n 5 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 06:2‚Ä¶ -7.2616‚Ä¶ 72.376335 30.91    1080  124    0    \n 6 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 06:2‚Ä¶ -7.2616‚Ä¶ 72.376289 26.8     0     280    0    \n 7 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 06:3‚Ä¶ -7.2615‚Ä¶ 72.376198 21.77    0     183    0    \n 8 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 06:3‚Ä¶ -7.2615‚Ä¶ 72.376205 30.78    2844  268    0    \n 9 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 06:4‚Ä¶ -7.2616‚Ä¶ 72.376259 27.1     1476  44     0    \n10 GV37501_201‚Ä¶ GV37‚Ä¶ 24/0‚Ä¶ 06:4‚Ä¶ -7.2614‚Ä¶ 72.375732 -35.04   0     260    0    \n# ‚Ñπ 6,640 more rows\n# ‚Ñπ 2 more variables: Distance &lt;chr&gt;, Essential &lt;chr&gt;\n\ncolnames(df_combined)\n\n [1] \"filename\"  \"ID\"        \"Date\"      \"Time\"      \"Latitude\"  \"Longitude\"\n [7] \"Altitude\"  \"Speed\"     \"Course\"    \"Type\"      \"Distance\"  \"Essential\"\n\n\n\n\nIf your data are combined into one or multiple csv files containing an ID column, use the following approach instead (this is the same code, but doesn‚Äôt create a new ID column from the file name):\n\n# ** Option **\ndf_combined &lt;- fs::dir_ls(path = filepath, #use filepath\n                          glob = filepattern, # use file pattern\n                          type = \"file\",  # only list files\n                          recurse = TRUE # look inside sub-folders\n                          ) %&gt;% \n  purrr::map_dfr(read_delim, # use read_delim function\n                 col_types = cols(.default = \"c\"), # as character by default\n                 col_names = colnames, # use colnames object made above\n                 skip = skiplines, # how many lines to skip\n                 delim = user_delim, # define delimiter\n                 trim_ws = user_trim_ws) # trim characters or not\ndf_combined\n\n\n\n\nSlim down dataset\n\n\nSelect normal columns\nOption: Select custom columns\n\n\n\n\n\n\n\n\n\nüß† User input required\n\n\n\nFirst, data need a time stamp, either in separate columns (e.g., ‚ÄúDate‚Äù and ‚ÄúTime‚Äù) or combined (‚ÄúDateTime‚Äù). Below we specify which column‚Äôs date and time info are stored in the data.  NB: These have to be in the same order as specified in earlier user input, i.e.¬†‚ÄúDate‚Äù and ‚ÄúTime‚Äù have to be the right way round\n\ndatetime_formats # previously specified datetime formats\n\n[1] \"dmY HMS\" \"Ymd HMS\"\n\ndatetime_colnames &lt;- c(\"Date\", \"Time\") # or c(\"DateTime\") \n\nYou can also have additional columns depending on the type of logger used, for example:\n\n## lc = Argos fix quality\n## Lat2/Lon2 = additional location fixes from Argos tag\n## laterr/lonerr = location error information provided by some GLS processing packages\n\nHere we‚Äôre going to slim down the dataset by selecting the necessary columns & coercing some column names. You should change column names below to those present in your tracking data, additional columns can be added (see above examples). This process standardises important column names for the rest of the workflow (e.g., TagID, Lat, Lon)\n\ndf_slim &lt;- data.frame(ID = as.character(df_combined$ID),\n                      Date = df_combined$Date,\n                      Time = df_combined$Time,\n                      Y = df_combined$Latitude,\n                      X = df_combined$Longitude)\n\n\n\n\n\n\n\n\n\n\n\nüß† User input required\n\n\n\nHere‚Äôs an example of how to change the above code for data with different columns and column names. This code works with immersion data recorded by a GLS logger (no location data)\n\ndf_slim &lt;- data.frame(ID = df_combined$ID,\n                      Date = df_combined$`DD/MM/YYYY`,\n                      Time = df_combined$`HH:MM:SS`,\n                      Immersion = df_combined$`wets0-20`)\n\n\n\n\n\n\nParse dates, create datetime, date and year columns\nNow our df_slim is ready, we need to create a DateTime column. Using the datetime_colnames object we made previously, we‚Äôll combine columns (if needed), and then parse a single DateTime column using the lubridate package:\n\ndf_slim &lt;- df_slim %&gt;%\n  tidyr::unite(col = \"DateTime_unparsed\", # united column name\n               all_of(datetime_colnames), # which columns to unite\n               sep = \" \",  # separator between values in new column\n               remove = FALSE # remove original columns?\n               ) %&gt;% \n  mutate(DateTime = lubridate::parse_date_time(DateTime_unparsed, # parse DateTime \n                                               orders = datetime_formats, # formats\n                                               tz = trackingdatatimezone), # timezone\n         Date = lubridate::as_date(DateTime),\n         Year = lubridate::year(DateTime)) %&gt;%\n  select(-DateTime_unparsed)\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `DateTime = lubridate::parse_date_time(...)`.\nCaused by warning:\n!  1 failed to parse.\n\n\n\n\n\n\n\n\nNote\n\n\n\nn failed to parse warnings means a date or time was not in the correct format for lubridate to create a date_time object, producing NAs. We can look at the failing rows using the following code:\n\nFails &lt;- df_slim %&gt;% filter(is.na(DateTime)==T)\nhead(Fails)\n\n       ID Date Time         Y         X DateTime Year\n1 GV37501 &lt;NA&gt; 5.06 -7.261829 72.376091     &lt;NA&gt;   NA\n\n\nNow we can see the issue: Date is empty, and Time is saved as a number. We‚Äôll remove this row in the @cleaning section, so don‚Äôt need to do anything else for the moment.\n\n\nLastly, we make a df_raw dataframe by sorting using ID and DateTime, dropping NA‚Äôs in DateTime column\n\ndf_raw &lt;- df_slim %&gt;% \n  arrange(across(all_of(c(ID_type, \"DateTime\")))) %&gt;%\n  drop_na(DateTime) #remove NA's in datetime column\nhead(df_raw)\n\n       ID       Date     Time         Y         X            DateTime Year\n1 GV37501 2016-06-24 05:21:39 -7.261829 72.376091 2016-06-24 05:21:39 2016\n2 GV37501 2016-06-24 05:29:50 -7.261744 72.376221 2016-06-24 05:29:50 2016\n3 GV37501 2016-06-24 05:54:28 -7.261623 72.376137 2016-06-24 05:54:28 2016\n4 GV37501 2016-06-24 06:22:02 -7.261651 72.376335 2016-06-24 06:22:02 2016\n5 GV37501 2016-06-24 06:27:16 -7.261618 72.376289 2016-06-24 06:27:16 2016\n6 GV37501 2016-06-24 06:32:27 -7.261584 72.376198 2016-06-24 06:32:27 2016\n\n\nWe can clean up intermediate files/objects by listing everything we want to keep (i.e.¬†remove everything else)\n\nrm(list=ls()[!ls() %in% c(\"df_raw\",\n                          \"date_formats\",\"datetime_formats\",\"trackingdatatimezone\", \n                          \"ID_type\", \"species_code\")])"
  },
  {
    "objectID": "User_guide.html#merge-with-metadata",
    "href": "User_guide.html#merge-with-metadata",
    "title": "ExMove user guide",
    "section": "2. Merge with metadata",
    "text": "2. Merge with metadata\nMetadata are an essential piece of information for any tracking study, as they contain important information about each data file, such as tag ID, animal ID, or deployment information, that we can add back into to our raw data when needed. For example, the table below shows what the first few columns of the metadata file looks like for our example red-footed booby data:\n\n\n\n\n\n\n\nTagID\nBirdID\nDeployID\nSpecies\nPopulation\nAge\nBreedingStage\nDeploymentDate\nDeploymentTime\nRetrievalDate\nRetrievalTime\nNestLat\nNestLong\n\n\n\n5\nGV37501\n1\nRFB\nDG\nAdult\nChick rearing\n25/06/2016\n16:25:00\n29/06/2016\n09:32:00\n-7.2386\n72.4347\n\n\n46\nGV37503\n1\nRFB\nDG\nAdult\nChick rearing\n26/06/2016\n08:30:00\n01/07/2016\n07:55:00\n-7.2386\n72.4347\n\n\nFW352Cs5\nGV37734\n1\nRFB\nNI\nAdult\nChick rearing\n08/07/2018\n09:48:00\n11/07/2018\n09:05:00\n-5.6812\n72.3165\n\n\n\n\n\n\nSelect file and date/time formats\n\n\n\n\n\n\nüß† User input required\n\n\n\nFirst we define the path to our metadata file:\n\nfilepath_meta &lt;- here(\"Data\",\"RFB_Metadata.csv\")\n\nThen much like in Step 1, we define the date format(s) used (for passing to lubridate) (d = day as decimal, m = month as decimal, y = year without century - 2 digits, Y = year with century - 4 digits). Here, we‚Äôve included common combinations, which you‚Äôll need to modify if your metadata include a different format (run OlsonNames() to return a full list of time zones names).\n\nmetadate_formats &lt;- c(\"dmY\", \"Ymd\") #specify date format used in metadata\nmetadatetime_formats &lt;- c(\"dmY HMS\", \"Ymd HMS\") #specify date & time format\nmetadatatimezone &lt;- \"Indian/Chagos\" #specify timezone used for metadata\n\n\n\nNext we read in the metadata file (make sure to check the read_ function you‚Äôre using matches your data format!).\n\ndf_metadata &lt;- readr::read_csv(filepath_meta) # Read in metadata file\nnames(df_metadata)\n\nSelect metadata columns\n\n\n\n\n\n\nüß† User input required\n\n\n\nThen we select necessary columns & create a new complete dataframe, making sure to provide four compulsory columns: ID ‚Äî as defined in tracking data (individual ID or TagID), deployment date & deployment time. We can also provide optional columns depending on sensor type: e.g.¬†colony, sex, age. You can add or delete other columns where appropriate.\nIf you have multiple ID columns like TagID/DeployID, include them here (for example, if one individual was tracked over multiple deployments/years, or if one tag was re-deployed on multiple individuals). For more information and helpful code, see the FAQ document and troubleshooting script.\nDeployment and retrieval dates: Different tags types sometimes require specific approaches for dealing with data collected outside of deployment period (e.g., before deployment or after retrieval). If data need to be filtered for one or both of these scenarios, we need to sort out these columns in the metadata, and if not relevant for the data, set the column name to ‚ÄúNA‚Äù.\nCentral Place foragers: If you are working with a central place forager (e.g., animals returning to a breeding location) and you have individual breeding location information in your metadata, here is a good place to add this info to the tracking data (e.g., breeding seabirds with known individual nest location, or seals returning to known haul-out location). We recommend adding these columns as: CPY = Central place Y coordinate column & CPX = Central place X coordinate column\n\ndf_metadataslim &lt;- data.frame(ID = as.character(df_metadata$BirdID), # compulsory column\n                              TagID = as.character(df_metadata$TagID),\n                              DeployID = as.character(df_metadata$DeployID),\n                              DeployDate_local = df_metadata$DeploymentDate, # compulsory column (make NA if irrelevant)\n                              DeployTime_local = df_metadata$DeploymentTime, # compulsory column (make NA if irrelevant)\n                              RetrieveDate_local = df_metadata$RetrievalDate, # compulsory column (make NA if irrelevant)\n                              RetrieveTime_local = df_metadata$RetrievalTime, # compulsory column (make NA if irrelevant)\n                              CPY = df_metadata$NestLat,\n                              CPX = df_metadata$NestLong,\n                              Species = \"RFB\",\n                              Population = df_metadata$Population,\n                              Age = df_metadata$Age,\n                              BreedStage = df_metadata$BreedingStage)\n\n\n\n\n\n\n\nOption: select alternative columns\n\n\n\n\n\nFor the example dataset RFB_IMM (immature red-footed boobies), we can use the following:\n\ndf_metadataslim &lt;- data.frame(ID = as.character(df_metadata$bird_id), # compulsory column\n                               TagID = as.character(df_metadata$Tag_ID),\n                               DeployID = as.character(df_metadata$Deploy_ID),\n                               DeployDate_local = df_metadata$capture_date, # compulsory column (set to NA if irrelevant)\n                               DeployTime_local = df_metadata$capture_time, # compulsory column (set to NA if irrelevant)\n                               RetrieveDate_local = NA, # compulsory column (set to NA if irrelevant)\n                               RetrieveTime_local = NA, # compulsory column (set to NA if irrelevant)\n                               DeployY = df_metadata$lat,\n                               DeployX = df_metadata$long,\n                               Species = \"RFB\",\n                               Age = df_metadata$age)\n\n\n\n\n\n\nFormat all dates and times, combine them and specify timezone (NA‚Äôs in deployment/retrieval date times will throw warnings, but these are safe to ignore if you know there are NA‚Äôs in these columns).\n\ndf_metadataslim &lt;- df_metadataslim %&gt;%\n  mutate(Deploydatetime = \n           lubridate::parse_date_time(\n             paste(DeployDate_local, DeployTime_local),# make deploy datetime\n                                          order = metadatetime_formats, \n                                          tz = metadatatimezone),\n         Retrievedatetime = \n           lubridate::parse_date_time(\n             paste(RetrieveDate_local, RetrieveTime_local), # make retrieve datetime\n                                            order=metadatetime_formats,\n                                            tz=metadatatimezone)\n         ) %&gt;%\n  select(-any_of(c(\"DeployDate_local\", \n                   \"DeployTime_local\", \n                   \"RetrieveDate_local\", \n                   \"RetrieveTime_local\"))\n         ) %&gt;%\n  mutate(across(contains('datetime'), # for chosen datetime column\n                ~with_tz(., tzone = trackingdatatimezone)) #format to different tz\n         )\n\nHere we‚Äôll create a dataframe of temporal extents of our data to use in absence of deploy/retrieve times (this is also useful for basic data checks and for writing up methods).\n\ndf_temporalextents &lt;- df_raw %&gt;%\n  group_by(across(all_of(ID_type))) %&gt;%\n  summarise(min_datetime = min(DateTime),\n            max_datetime = max(DateTime))\n\nThen we use these temporal extents of our data to fill in any NA‚Äôs in the deploy/retrieve times.\n\ndf_metadataslim &lt;- df_metadataslim %&gt;%\n  left_join(., df_temporalextents, by = ID_type) %&gt;%\n  mutate(Deploydatetime = case_when(!is.na(Deploydatetime) ~ Deploydatetime,\n                                      is.na(Deploydatetime) ~ min_datetime),\n         Retrievedatetime = case_when(!is.na(Retrievedatetime) ~ Retrievedatetime,\n                                      is.na(Retrievedatetime) ~ max_datetime)) %&gt;%\n  select(-c(min_datetime, max_datetime))\n\nNext we merge metadata with raw data using the ID column.\n\ndf_metamerged &lt;- df_raw %&gt;%\n  left_join(., df_metadataslim, by=ID_type) \n\nFinally, we can remove intermediate files/objects by specifying objects to keep.\n\nrm(list=ls()[!ls() %in% c(\"df_metamerged\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#cleaning",
    "href": "User_guide.html#cleaning",
    "title": "ExMove user guide",
    "section": "3. Cleaning",
    "text": "3. Cleaning\n\n\n\n\n\n\nüß† User input required\n\n\n\nDefine your own no/empty/erroneous data values in Lat and Lon columns (e.g.¬†‚Äúbad‚Äù values specified by the tag manufacturer).\n\nNo_data_vals &lt;- c(0, -999)\n\nDefine a vector of columns which can‚Äôt have NAs (if there are NAs in one of these columns the problematic row will be removed).\n\nna_cols &lt;- c(\"X\", \"Y\", \"DateTime\", \"ID\")\n\n\n\nNow we pipe the data through a series of functions to drop NAs in specified columns, filter out user-defined no_data_values in Lat Lon columns, remove duplicates, remove undeployed locations and filter out locations within temporal cut-off following deployment.\n\ndf_clean &lt;- df_metamerged %&gt;%\n  drop_na(all_of(na_cols)) %&gt;% \n  filter(!X %in% No_data_vals & !Y %in% No_data_vals) %&gt;% # remove bad Lat/Lon values\n  distinct(DateTime, ID, .keep_all = TRUE) %&gt;% # NB: might be an issue for ACC without ms\n  filter(\n    case_when(!is.na(Retrievedatetime) # for all valid datetimes\n              ~ Deploydatetime &lt; DateTime & # keep if datetime after deployment...\n                DateTime &lt; Retrievedatetime, # ...and before retrieval\n              .default = Deploydatetime &lt; DateTime)) # filter deployment only if retrieve date is NA (i.e., sat tags) \nhead(df_clean)\n\n       ID       Date     Time         Y         X            DateTime Year\n1 GV37501 2016-06-25 10:27:56 -7.238936 72.435043 2016-06-25 10:27:56 2016\n2 GV37501 2016-06-25 10:33:13 -7.238903 72.435005 2016-06-25 10:33:13 2016\n3 GV37501 2016-06-25 10:38:19 -7.238754 72.434944 2016-06-25 10:38:19 2016\n4 GV37501 2016-06-25 10:43:46 -7.238957 72.435188 2016-06-25 10:43:46 2016\n5 GV37501 2016-06-25 10:49:02 -7.239048 72.435059 2016-06-25 10:49:02 2016\n6 GV37501 2016-06-25 10:54:38 -7.238441 72.434708 2016-06-25 10:54:38 2016\n  TagID DeployID     CPY     CPX Species Population   Age    BreedStage\n1     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n2     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n3     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n4     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n5     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n6     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n       Deploydatetime    Retrievedatetime\n1 2016-06-25 10:25:00 2016-06-29 03:32:00\n2 2016-06-25 10:25:00 2016-06-29 03:32:00\n3 2016-06-25 10:25:00 2016-06-29 03:32:00\n4 2016-06-25 10:25:00 2016-06-29 03:32:00\n5 2016-06-25 10:25:00 2016-06-29 03:32:00\n6 2016-06-25 10:25:00 2016-06-29 03:32:00\n\n\n\n\n\n\n\n\nOption: Filter by fix quality\n\n\n\n\n\nArgos fix quality can be used to filter the data set to remove locations with too much uncertainty. If you know the error classes that you want to retain in a dataset, you can run this filter below.  NB: If you want to do further exploration of location quality (e.g., from GPS PTT tags to compare locations with contemporaneous GPS locations), keep all location classes by skipping this step.\nIn this example we define a vector of location classes to keep (typically, location classes 1, 2, and 3 are of sufficient certainty), and filter out everything else.\n\nlc_keep &lt;- c(\"1\", \"2\", \"3\")\n\ndf_clean &lt;- df_clean %&gt;%\nfilter(lc %in% lc_keep) # filter data to retain only the best lc classes\n\n\n\n\nFinally we remove intermediate files/objects:\n\nrm(list=ls()[!ls() %in% c(\"df_clean\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#processing",
    "href": "User_guide.html#processing",
    "title": "ExMove user guide",
    "section": "4. Processing",
    "text": "4. Processing\nPerform some useful temporal and spatial calculations on the data\n\n\n\n\n\n\nüß† User input required\n\n\n\nFirst we need to specify the co-ordinate projection systems for the tracking data and meta data. The default here is lon/lat for both tracking data & metadata, for which the EPSG code is 4326. For more information see the CRS section of the FAQ‚Äôs or have a look at the ESPG.io database.\n\ntracking_crs &lt;- 4326 # Only change if data are in a different coordinate system\nmeta_crs &lt;- 4326 # Only change if data are in a different coordinate system\n\nNext we transform coordinates of data, and perform spatial calculations. This requires spatial analysis, and so it is good practice to run all spatial analyses in a coordinate reference system that uses metres as a unit.\nThe default CRS for this workflow is the Spherical Mercator projection ‚Äî (crs = 3857), which is used by Google Maps/OpenStreetMap and works worldwide. However, this CRS can over-estimate distance calculations in some cases, so it‚Äôs important to consider the location and scale of your data (e.g., equatorial/polar/local scale/global scale) and choose a projection system to match. Other options include (but are not limited to) UTM, and Lambert Azimuthal Equal-Area (LAEA) projections.\n\ntransform_crs &lt;- 3857\n\n\n\nHere we‚Äôll calculate some useful movement metrics from the tracking data, including distance between fixes, time between fixes, and net displacement from the first fix.\n\ndf_diagnostic &lt;-  df_clean %&gt;%\n  ungroup() %&gt;% #need to ungroup to extract geometry of the whole dataset\n  mutate(geometry_GPS = st_transform( # transform X/Y coordinates\n            st_as_sf(., coords=c(\"X\",\"Y\"), crs = tracking_crs), #from original format\n            crs = transform_crs)$geometry # to the new transform_crs format\n         ) %&gt;%\n  group_by(ID) %&gt;% #back to grouping by ID for calculations per individual\n  mutate(dist = st_distance(geometry_GPS, # distance travelled from previous fix, \n                            lag(geometry_GPS), \n                            by_element = T), # calculations are done by row\n         difftime = difftime(DateTime, lag(DateTime), # time passed since previous fix\n                             units = \"secs\"), # in seconds\n         netdisp = st_distance(geometry_GPS, # dist. between 1st and current location\n                               geometry_GPS[1], \n                               by_element = F)[,1], # dense matrix w/ pairwise distances\n         speed = as.numeric(dist)/as.numeric(difftime), # calculate speed (distance/time)\n         dX = as.numeric(X)-lag(as.numeric(X)), #diff. in lon  relative to prev. location\n         dY = as.numeric(Y)-lag(as.numeric(Y)), #diff. in lat relative to prev. location\n         turnangle = atan2(dX, dY)*180/pi + (dX &lt; 0)*360) %&gt;% # angle from prev. to current location\n  ungroup() %&gt;% \n  select(-c(geometry_GPS, dX, dY)) # ungroup and remove excess geometries\n\nAdd latitude and longitude column ‚Äî this can be useful for plotting and is a common coordinate system used in the shiny app\n\ndf_diagnostic &lt;- st_coordinates(st_transform(st_as_sf(df_diagnostic, \n                                                      coords = c(\"X\",\"Y\"), \n                                                      crs=tracking_crs), \n                                             crs = 4326)) %&gt;% \n                  as.data.frame() %&gt;% \n                  rename(\"Lon\" = \"X\", \"Lat\" = \"Y\") %&gt;% \n                  cbind(df_diagnostic, .)"
  },
  {
    "objectID": "User_guide.html#save-for-shiny",
    "href": "User_guide.html#save-for-shiny",
    "title": "ExMove user guide",
    "section": "5. Save for Shiny",
    "text": "5. Save for Shiny\nHere we‚Äôre going to save df_diagnostic to use in the Shiny app provided. The app is designed to explore how further filtering and processing steps affect the data.\n\n\n\n\n\n\nüß† User input required\n\n\n\nFirst, we use here to create a file path for saving the working dataframe files, and create the folder if missing\n\nfilepath_dfout &lt;- here(\"DataOutputs\",\"WorkingDataFrames\") # create filepath\ndir.create(filepath_dfout) # create folder if it doesn't exist\n\nNext we define file name for the saved file by pasting the species code before _diagnostic (can change this if you want to use a different naming system).\n\nfilename_dfout &lt;- paste0(species_code, \"_diagnostic\")\n\nIf not added from the metadata, add a species column and any other columns here relevant to your data (optional)\n\n## ** Option ** ##\ndf_diagnostic$Species &lt;- species_code\n\n\n\nFinally we save the df_diagnostic as a csv file using the variables created above.\n\nwrite_csv(df_diagnostic, file = here(filepath_dfout, paste0(filename_dfout,\".csv\")))\n\nRemove everything except df_diagnostic ahead of the next step.\n\nrm(list=ls()[!ls() %in% c(\"df_diagnostic\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#filtering",
    "href": "User_guide.html#filtering",
    "title": "ExMove user guide",
    "section": "6. Filtering",
    "text": "6. Filtering\nThis second filtering stage is designed to remove outliers in the data, and you can use outputs from the Shiny app to inform these choices. If you don‚Äôt need to filter for outliers, skip this step and keep using df_diagnostic in the next steps.\nAccessing the Shiny app\nOption 1:\nAccess the Shiny app online at the following link: https://lukeozsanlav.shinyapps.io/exmove_explorer/\nOption 2:\nAlternatively run the app from your local R session with the following code\n\nif (!require(\"shiny\")) install.packages(\"shiny\")\nlibrary(shiny)\nrunGitHub(\"ExMoveApp\", username = \"LukeOzsanlav\",\n          ref = \"master\", subdir = \"app\")\n\nApp usage:\n\nUpload your csv version of df_diagnostic to the app by clicking the Upload data button in the top left.\nAt the bottom of each app page are printed code chunks that can be copied into subsequent user input section. These code chunks contain the user input values you manually select in the app\nDefine threshold values\nFor this section, you can either use the code chunks produced by the Shiny app, or manually define the threshold values yourself. If you‚Äôre using the Shiny app, you can copy the code chunks from the bottom of each page to replace the user input section below. If you‚Äôre manually defining the threshold values, you can edit each of the variables below as you would for any of the user input sections (we have suggested some for the RFB dataset).\n\n\n\n\n\n\nüß† User input required\n\n\n\nFirst we define a period to filter after tag deployment, when all points before the cutoff will be removed (e.g.¬†to remove potentially unnatural behaviour following the tagging event). We define this period using the as.period function, by providing an integer value and time unit (e.g.¬†hours/days/years). This code below specifies a period of 30 minutes:\n\nfilter_cutoff &lt;- as.period(30, unit=\"minutes\") \n\nThen we define speed threshold in m/s, which we will use to remove any points with faster speeds.\n\nfilter_speed &lt;- 20\n\nNext we define a net displacement (distance from first point) threshold and specify units. Any points further away from the first tracking point will be removed (see commented code for how to retain all points):\n\nfilter_netdisp_dist &lt;- 300\nfilter_netdist_units &lt;- \"km\" # e.g., \"m\", \"km\"\n\n#If you want to retain points no matter the net displacement value, use these values instead:\n#filter_netdisp_dist &lt;- max(df_diagnostic$netdisp)\n#filter_netdist_units &lt;- \"m\"\n\n\n\nImplement filters\nCreate net displacement filter using distance and units\n\nfilter_netdisp &lt;- units::as_units(filter_netdisp_dist, filter_netdist_units)\n\nFilter df_diagnostic\n\ndf_filtered &lt;- df_diagnostic %&gt;%\n  filter(Deploydatetime + filter_cutoff &lt; DateTime, # keep times after cutoff\n         speed &lt; filter_speed, # keep speeds slower than speed filter\n         netdisp &lt;= filter_netdisp) # keep distances less than net displacement filter\nhead(df_filtered)\n\n       ID       Date     Time         Y         X            DateTime Year\n1 GV37501 2016-06-25 11:01:04 -7.238462 72.434708 2016-06-25 11:01:04 2016\n2 GV37501 2016-06-25 11:08:50 -7.238486 72.434738 2016-06-25 11:08:50 2016\n3 GV37501 2016-06-25 11:17:13 -7.238597 72.435005 2016-06-25 11:17:13 2016\n4 GV37501 2016-06-25 11:25:16 -7.231193 72.434624 2016-06-25 11:25:16 2016\n5 GV37501 2016-06-25 11:30:32  -7.23855 72.434731 2016-06-25 11:30:32 2016\n6 GV37501 2016-06-25 11:36:43 -7.238533 72.434761 2016-06-25 11:36:43 2016\n  TagID DeployID     CPY     CPX Species Population   Age    BreedStage\n1     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n2     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n3     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n4     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n5     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n6     5        1 -7.2386 72.4347     RFB         DG Adult Chick rearing\n       Deploydatetime    Retrievedatetime           dist difftime       netdisp\n1 2016-06-25 10:25:00 2016-06-29 03:32:00   2.356490 [m] 386 secs  64.96002 [m]\n2 2016-06-25 10:25:00 2016-06-29 03:32:00   4.290196 [m] 466 secs  60.84931 [m]\n3 2016-06-25 10:25:00 2016-06-29 03:32:00  32.226707 [m] 503 secs  38.27498 [m]\n4 2016-06-25 10:25:00 2016-06-29 03:32:00 831.906239 [m] 483 secs 870.11594 [m]\n5 2016-06-25 10:25:00 2016-06-29 03:32:00 825.636254 [m] 316 secs  55.51973 [m]\n6 2016-06-25 10:25:00 2016-06-29 03:32:00   3.846024 [m] 371 secs  55.05007 [m]\n        speed turnangle      Lon       Lat\n1 0.006104896 180.00000 72.43471 -7.238462\n2 0.009206429 128.65981 72.43474 -7.238486\n3 0.064069000 112.57414 72.43501 -7.238597\n4 1.722373165 357.05423 72.43462 -7.231193\n5 2.612772957 179.16675 72.43473 -7.238550\n6 0.010366640  60.46122 72.43476 -7.238533\n\n\nRemove intermediate files/objects\n\nrm(list=ls()[!ls() %in% c(\"df_filtered\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#summarise-data",
    "href": "User_guide.html#summarise-data",
    "title": "ExMove user guide",
    "section": "7. Summarise cleaned & filtered tracking data",
    "text": "7. Summarise cleaned & filtered tracking data\n\n\n\n\n\n\nüß† User input required\n\n\n\nSet the units to display sampling rate in the summary table\n\nsampleRateUnits &lt;- \"mins\" \n\nDefine levels of grouping factors to summarise over\nFirstly, down to population level. Here, we are working on data from one population & year, and so use Species as the grouping factor. Add any other relevant grouping factors here (e.g.¬†Country / Year / Season / Age).\n\ngrouping_factors_poplevel &lt;- c(\"Species\")\n\nSecondly, down to individual level (add DeployID for example if relevant).\n\ngrouping_factors_indlevel &lt;- c(\"ID\")\n\n\n\n\nCreate summary tables\nCreate a small function to calculate standard error.\n\nse &lt;- function(x) sqrt(var(x, na.rm = T) / length(x[!is.na(x)]))\n\nCreate a summary table of individual-level summary statistics:\n\ndf_summary_ind &lt;- df_filtered %&gt;%\n  group_by(across(c(all_of(grouping_factors_poplevel), all_of(grouping_factors_indlevel)))) %&gt;%\n  summarise(NoPoints = NROW(ID), # number of fixes\n            NoUniqueDates = length(unique(Date)), # number of tracking dates\n            FirstDate = as.Date(min(Date)), # first tracking date\n            LastDate = as.Date(max(Date)), # last tracking date\n            SampleRate = mean(as.numeric(difftime, units = sampleRateUnits), na.rm = T), # sample rate mean\n            SampleRate_se = se(as.numeric(difftime, units = sampleRateUnits))) # sample rate standard error\n\n`summarise()` has grouped output by 'Species'. You can override using the\n`.groups` argument.\n\ndf_summary_ind\n\n# A tibble: 3 √ó 8\n# Groups:   Species [1]\n  Species ID      NoPoints NoUniqueDates FirstDate  LastDate   SampleRate\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;         &lt;int&gt; &lt;date&gt;     &lt;date&gt;          &lt;dbl&gt;\n1 RFB     GV37501      989             5 2016-06-25 2016-06-29       5.37\n2 RFB     GV37503     1280             6 2016-06-26 2016-07-01       5.58\n3 RFB     GV37734      820             4 2018-07-08 2018-07-11       5.18\n# ‚Ñπ 1 more variable: SampleRate_se &lt;dbl&gt;\n\n\nCreate a table of population-level summary statistics:\n\ndf_summary_pop &lt;- df_summary_ind %&gt;% # use the individual-level summary data\n  group_by(across(grouping_factors_poplevel)) %&gt;%\n  summarise(NoInds = length(unique(ID)), # number of unique individuals\n            NoPoints_total = sum(NoPoints), # total number of tracking locations\n            FirstDate = as.Date(min(FirstDate)), # first tracking date\n            LastDate = as.Date(max(LastDate)), # last tracking date\n            PointsPerBird = mean(NoPoints), # number of locations per individual: mean\n            PointsPerBird_se = se(NoPoints), # number of locations per individual: standard error\n            DatesPerBird = mean(NoUniqueDates), # number of tracking days per bird: mean\n            DatesPerBird_se = se(NoUniqueDates), # number of tracking days per bird: standard error\n            SampleRate_mean = mean(SampleRate), # sample rate mean\n            SampleRate_se = se(SampleRate)) # sample rate standard error\ndf_summary_pop\n\n# A tibble: 1 √ó 11\n  Species NoInds NoPoints_total FirstDate  LastDate   PointsPerBird\n  &lt;chr&gt;    &lt;int&gt;          &lt;int&gt; &lt;date&gt;     &lt;date&gt;             &lt;dbl&gt;\n1 RFB          3           3089 2016-06-25 2018-07-11         1030.\n# ‚Ñπ 5 more variables: PointsPerBird_se &lt;dbl&gt;, DatesPerBird &lt;dbl&gt;,\n#   DatesPerBird_se &lt;dbl&gt;, SampleRate_mean &lt;dbl&gt;, SampleRate_se &lt;dbl&gt;\n\n\nRemove intermediate files/objects by specifying which objects to keep:\n\nrm(list=ls()[!ls() %in% c(\"df_filtered\",\n                          \"df_summary_ind\", \"df_summary_pop\", \n                          \"species_code\")])"
  },
  {
    "objectID": "User_guide.html#save-data",
    "href": "User_guide.html#save-data",
    "title": "ExMove user guide",
    "section": "8. Save filtered and summary data",
    "text": "8. Save filtered and summary data\n\n\n\n\n\n\nüß† User input required\n\n\n\nFirst we define the folder file path for saving our filtered data and create folder if not already present\n\nfilepath_filtered_out &lt;- here(\"DataOutputs\",\"WorkingDataFrames\")\ndir.create(filepath_filtered_out)\n\nThen we define the file path for saving summary dataframes, again creating folder if needed\n\nfilepath_summary_out &lt;- here(\"DataOutputs\",\"SummaryDataFrames\")\ndir.create(filepath_summary_out)\n\nHere we define file names for saved files, and paste the species code to _summary_, followed by ind (individual level) or pop (population level). You can change this if you want to use a different naming system.\n\nfilename_filtered_out &lt;- paste0(species_code, \"_filtered\")\nfilename_summary_ind_out &lt;- paste0(species_code, \"_summary_ind\")\nfilename_summary_pop_out &lt;- paste0(species_code, \"_summary_pop\")\n\n\n\nNow we can save all our dataframes as .csv files using our defined values\n\nwrite_csv(df_filtered, file = here(filepath_filtered_out, paste0(filename_filtered_out,\".csv\")))\nwrite_csv(df_summary_ind, file = here(filepath_summary_out, paste0(filename_summary_ind_out,\".csv\")))\nwrite_csv(df_summary_pop, file = here(filepath_summary_out, paste0(filename_summary_pop_out,\".csv\")))\n\nLastly we remove intermediate files/objects\n\nrm(list=ls()[!ls() %in% c(\"df_filtered\", \n                          \"df_summary_ind\", \"df_summary_pop\", \n                          \"species_code\")])"
  },
  {
    "objectID": "User_guide.html#visualisation",
    "href": "User_guide.html#visualisation",
    "title": "ExMove user guide",
    "section": "9. Visualisation",
    "text": "9. Visualisation\n\n\n\n\n\n\nUser input required\n\n\n\nDefine parameters for reading out plots, and define device to read plots out as e.g.¬†tiff/jpeg\n\ndevice &lt;- \"tiff\"\n\nDefine units for plot size (usually mm)\n\nunits &lt;- \"mm\"\n\nDefine plot resolution in dpi (300 is usually good minimum)\n\ndpi &lt;- 300\n\nDefine filepath to read out plots and create folder if absent\n\nout_path &lt;- here(\"DataOutputs\",\"Figures\")\ndir.create(out_path)\n\nWe plot maps over a topography base-layer which can include terrestrial (elevation) and marine (bathymetry/water depth) data. To set legend label for topography data, relevant to your data.\n\ntopo_label = \"Depth (m)\"\n\n\n\nLoad additional libraries for spatial visualisation (optional)\n\n\n\n\n\n\nIf you see a masking warning these are fine. Watch out for packages that aren‚Äôt installed yet\n\n\n\n\nlibrary(rnaturalearth)\nlibrary(marmap)\nlibrary(plotly)\n\n\n\nCreate version of data for plotting by transforming required columns to numeric and creating time elapsed columns\n\ndf_plotting &lt;- df_filtered %&gt;%\n  group_by(ID) %&gt;%\n  mutate(diffsecs = as.numeric(difftime),\n         secs_elapsed = cumsum(replace_na(diffsecs, 0)),\n         time_elapsed = as.duration(secs_elapsed),\n         days_elapsed = as.numeric(time_elapsed, \"days\")) %&gt;%\n  mutate(across(c(dist,speed, Lat, Lon), as.numeric))\n\nCreate a map of all points. Set the plot limits as the max and min lat/longs as the tracking data\nFirst set up a basemap to plot over: - Use rnaturalearth low resolution countries basemap - co-ordinates in lat/lon to match other spatial data\n\ncountries &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nDefine min and max co-ordinates based on extent of tracking data, for adding bathymetry extracted from NOAA database.\n\nminlon &lt;- min(df_plotting$Lon)\nmaxlon &lt;- max(df_plotting$Lon)\n\nminlat &lt;- min(df_plotting$Lat)\nmaxlat &lt;- max(df_plotting$Lat)\n\nLoad in bathymetry basemap. Set limits slightly beyond tracking data to make a buffer so no gaps when plotting\n\nbase_topography_map &lt;- getNOAA.bathy(\n  lon1 = minlon - 0.1, lon2 = maxlon + 0.1,\n  lat1 = minlat - 0.1, lat2 = maxlat + 0.1, \n  resolution = 1)\n\nFortify bathymetry data for plotting\n\nbase_topography_fort = fortify(base_topography_map)\n\nCreate base map with correct extent, topography, country outlines, etc.,\n\nmap_base &lt;- ggplot() + \n  geom_raster(data = base_topography_fort, aes(x=x, y=y, fill=z), alpha = 0.9) +\n  # add colour scheme for the fill\n  scale_fill_viridis_c(option=\"mako\", name = topo_label) + \n  # add map of countries over the top\n  geom_sf(data = countries, aes(geometry = geometry), fill = NA) + \n  # set plot limits\n  coord_sf(xlim = c(minlon-0.1, maxlon+0.1), \n           ylim = c(minlat-0.1, maxlat+0.1), crs = 4326, expand = F) +\n  # add labels\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  theme(axis.text=element_text(colour=\"black\"),\n        axis.title.x = element_text(size = 15),\n        axis.text.x = element_text(hjust=0.7),\n        axis.title.y = element_text(angle=90, vjust = 0.4, size = 15),\n        axis.text.y = element_text(hjust=0.7, angle=90, vjust=0.3)) +\n  # set a theme\n  theme_light()\nmap_base\n\n\n\n\n\n\nPopulation\nSeveral individuals\nOption: Many individuals\n\n\n\nPlot a combined map of all tracking locations:\n\nmap_alllocs &lt;- map_base + \n  # add GPS points\n  geom_point(data = df_plotting, \n             aes(x = Lon, y = Lat), \n             alpha = 0.8, size = 0.5, col = \"violetred3\") \nmap_alllocs\n\n\n\n\n\n\nPlot a map of individual locations, colouring points by speed, and faceting by ID\n\nmap_individuals &lt;- map_base + \n  # add GPS points and paths between them\n  geom_point(data = df_plotting, \n             aes(x = Lon, y = Lat, col = speed), \n             alpha = 0.8, size = 0.5 \n             ) +\n  geom_path(data = df_plotting, \n            aes(x = Lon, y = Lat, col = speed), \n            alpha = 0.8, size = 0.5 \n            ) +\n  # colour birds using scale_colour_gradient2\n  scale_colour_gradient2(name = \"Speed\", \n                         low = \"blue\", mid = \"white\", high = \"red\", \n                         midpoint = (max(df_plotting$speed,na.rm=TRUE)/2) # use `midpoint` for nice colour transition\n                         ) + \n  facet_wrap(~ ID, # facet for individual\n             ncol = round(sqrt(n_distinct(df_plotting$ID))))  \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\nmap_individuals\n\n\n\n\n\n\nIn previous plots, we‚Äôve split the population into individual facets. This works fine on the example code, where we only have a few individuals, but if you have more individuals and the facets are too small, you can split the plot onto multiple pages. Use the below code to use facet_wrap_paginate from the ggforce package:\n\n## ** Option ** ##\n## save plot as object to later extract number of pages\n## e.g., with 2 per page:\nmap_individuals &lt;- map_base +\n  geom_point(data = df_plotting, # add GPS points\n             aes(x = Lon, y = Lat, col = speed), \n             alpha = 0.8, size = 0.5 \n             ) +\n  geom_path(data = df_plotting, #and paths between them\n            aes(x = Lon, y = Lat, col = speed), \n            alpha = 0.8, size = 0.5 \n            ) +\n  scale_colour_gradient2(name = \"Speed\", # colour speed w/ scale_colour_gradient2\n                         low = \"blue\", mid = \"white\", high = \"red\", \n                         midpoint = (max(df_plotting$speed,na.rm=TRUE)/2)\n                         ) +\n  facet_wrap_paginate(~ID, # facet for individual\n                      ncol = 2, nrow= 1, page = 1)\n\nHow many pages of plots?\n\nn_pages(map_individuals)\n\nRun through different values of page to show each page in turn\n\nmap_individuals\n\n\n\n\nSave maps for further use using ggsave function.\n\nggsave(plot = map_alllocs, \n       filename = paste0(species_code, \"_map_all_locs.\", device),\n       device = device,\n       path = out_path, \n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nggsave(plot = map_individuals, \n       filename = paste0(species_code, \"_map_individuals.\", device),\n       device = device,\n       path = out_path, \n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nCreate a time series plot of speed, faceted for each individual.\n\nspeed_time_plot &lt;- df_plotting %&gt;% #speed over time\n  ggplot(data = ., \n         aes(x = days_elapsed, y = speed, group = ID)\n         ) +\n  geom_line() +   # add line of speed over time\n  xlab(\"time elapsed (days)\") + \n  ylab(\"speed (m/s)\") +\n  facet_wrap(~ID, # facet by individual\n             nrow = round(sqrt(n_distinct(df_plotting$ID)))) +\n  theme_light() + # set plotting theme\n  theme(axis.text = element_text(colour=\"black\")) #adjust theme\nspeed_time_plot\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWarnings about non-finite values for speed/step length plots are expected and usually refer to the first location for each individual (i.e.¬†number of non-finite values should be equal to number of individuals)\n\n\nSave plot for further use\n\nggsave(plot = speed_time_plot, \n       filename = paste0(species_code, \"_speed_timeseries_plot.\", device),\n       device = device,\n       path = out_path, \n       units = units, width = 200, height = 175, dpi = dpi  \n)\n\nCreate a histogram of point to point speeds (can adjust binwidth and x limits manually)\n\nspeed_hist &lt;- df_plotting %&gt;% \n  ggplot(data = ., aes(speed)) +\n  geom_histogram(binwidth = 0.1, alpha=0.7) + # can adjust binwidth to suite your needs\n  geom_density(aes(y = 0.1*..count..)) +\n  xlab(\"speed (m/s)\") + \n  ylab(\"count\") +\n  facet_wrap(~ID, # facet by individual\n             nrow = round(sqrt(n_distinct(df_plotting$ID)))) +\n  theme_light() + # set plotting theme\n  theme(axis.text = element_text(colour=\"black\")) #adjust theme\nspeed_hist\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(count)` instead.\n\n\n\n\n\nSave plot for further use\n\nggsave(plot = speed_hist, \n       filename = paste0(species_code, \"_speed_histogram\", device),\n       device = device,\n       path = out_path,\n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nCreate a time series plot of step lengths (faceted for each individual)\n\nstep_time_plot &lt;- df_plotting %&gt;% #step length over time\n  ggplot(data = ., \n         aes(x = days_elapsed, y = as.numeric(netdisp), group = ID)) +\n  geom_line() +\n  # add plot labels\n  xlab(\"time elapsed (days)\") + ylab(\"Distance from first fix (m)\") +\n\n  facet_wrap(~ID, # facet by individual\n             nrow= round(sqrt(n_distinct(df_plotting$ID)))) +\n  theme_light() + # set plotting theme\n  theme(axis.text = element_text(colour=\"black\")) #adjust theme\nstep_time_plot\n\n\n\n\nSave plot for further use\n\nggsave(plot = step_time_plot, \n       filename = paste0(species_code, \"_step_time_plot.\", device),\n       device = device,\n       path = out_path,\n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nCreate a histogram of step lengths (can adjust binwidth and x limits manually)\n\nstep_hist &lt;- df_plotting %&gt;% #step histogram\n  ggplot(data = ., \n         aes(as.numeric(dist))) +\n  geom_histogram(binwidth = 1, alpha = 0.7) + # can adjust binwidth to suite your needs\n  geom_density(aes(y = 1*..count..)) +\n  xlab(\"step length (m)\") + \n  ylab(\"count\") +\n  facet_wrap(~ID, # facet by individual\n             nrow = round(sqrt(n_distinct(df_plotting$ID))))+\n  theme_light() # set plotting theme\n\nSave plot for further use\n\nggsave(plot = step_hist, \n       filename = paste0(species_code, \"_step_hist.\", device),\n       device = device,\n       path = out_path,\n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nLastly, we remove intermediate files/objects if necessary to speed up any post-processing steps\n\nrm(list=ls()[!ls() %in% c(\"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ExMove",
    "section": "",
    "text": "Welcome to the home page of ExMove. Here, you can find the resources to use our toolkit for processing biologging data from tag downloads to online archive.\nBelow is an overview of the workflow and what you can do with it:\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing': 30}}}%%\nflowchart LR\n  S1[Read in data] ==&gt; S3(Merge)\n  S2[Read in metadata] ==&gt; S3 ==&gt; A(Clean)\n  subgraph shiny [\"(Parameter determination in Shiny app)\"]\n  style shiny fill:#fbfbfb, stroke:#d3d3d3, stroke-width:px\n  A(Clean) ==&gt; B(Process) ==&gt; C(Filter)\n  D(Optional&lt;br/&gt;scripts)\n  end\n  C ==&gt; S{Standardised&lt;br/&gt;dataset}\n  C --&gt; D --&gt; S\n  S --&gt; E(Analyses)\n  S ==&gt; F(Archive)\n  linkStyle 6,7 stroke-dasharray: 4 4"
  },
  {
    "objectID": "FAQs.html",
    "href": "FAQs.html",
    "title": "FAQ‚Äôs",
    "section": "",
    "text": "Below is a list of common issues/errors that you might come across when using the workflow we provide. It‚Äôs not exhaustive, but hopefully helpful for solving problems that are likely to occur if you are analysing your own data."
  },
  {
    "objectID": "FAQs.html#introduction",
    "href": "FAQs.html#introduction",
    "title": "FAQ‚Äôs",
    "section": "",
    "text": "Below is a list of common issues/errors that you might come across when using the workflow we provide. It‚Äôs not exhaustive, but hopefully helpful for solving problems that are likely to occur if you are analysing your own data."
  },
  {
    "objectID": "FAQs.html#getting-the-workflow-up-and-running",
    "href": "FAQs.html#getting-the-workflow-up-and-running",
    "title": "FAQ‚Äôs",
    "section": "1 Getting the workflow up and running",
    "text": "1 Getting the workflow up and running\n\nGitHub issues\nUsing GitHub is not essential for using this workflow ‚Äî you can download the repo from  &gt; Download Zip, unzip to your machine, and start by opening the .Rproj file.\nHowever, if you want to contribute improvements to the repo, or are just interested in getting to grips with GitHub for your own research projects, there are great resources online for getting started (see: setting up GitHub | cloning a repo | using GitHub with RStudio).\n\n\nUsing RStudio projects and here for reproducible filepaths\nWhen we open a .Rproj file (which should be the starting point for this workflow), this opens up a fresh instance of RStudio, with a dedicated project environment and access to all the folders and files contained inside that project folder. If you‚Äôre not familiar with using RStudio projects, have a look at this section of R for Data Science for a quick explained.\nBy combining Rstudio projects with the here package, we can create relative filepaths that work across machines, so you won‚Äôt have to edit a complicated filepath to get the code to run (for more info see the ‚ÄòGetting started‚Äô section of the here documentation)."
  },
  {
    "objectID": "FAQs.html#reporting-in-a-paper",
    "href": "FAQs.html#reporting-in-a-paper",
    "title": "FAQ‚Äôs",
    "section": "2 Reporting in a paper",
    "text": "2 Reporting in a paper\n\nHow to report usage of this workflow in a paper\nIf you have used this workflow and you wish to report this in a scientific publication then we provide the text below as a suggestion as to how the user might do so.\n‚ÄúAnimal tracking data in this study was cleaned following the key principles of clean data in Langley et al. (2023). Duplicated and erroneous data points were removed, a maximum speed filter of XXm/s applied and data YYhours after initial deployment removed‚Äù"
  },
  {
    "objectID": "FAQs.html#data-problems",
    "href": "FAQs.html#data-problems",
    "title": "FAQ‚Äôs",
    "section": "3 Data problems",
    "text": "3 Data problems\n\nWhat if I have multiple data files for an individual?\nLots of data sets include multiple files for a single individual, when individuals are tracked more than once. The workflow can filter raw tracking data over multiple deployments. For it to work, make sure to check:\n\nThe raw data contain individual ID, read in from the file name\nThe metadata includes one row per unique tag deployment per individual\nThe metadata contains a column with a unique value for each deployment (e.g., DeployID/Year)\nThis identifying column is included in df_metadataslim\n\nWhen joining the raw data and metadata, the raw data will be duplicated for each row of ID in the metadata (i.e., for each DeployID/Year). When we filter the data to the deployment period only (df_clean) the duplicated rows, where tracking DateTimes are outside of the individual‚Äôs deployment period, are removed.\nAn example of how to use multiple ID columns in this way is included in the troubleshooting script, named Troubleshoot - multiple ID columns.R.\n\n\nWhat if my tags have been re-deployed on multiple individuals?\nSometimes tags are used multiple times on different individuals, but data are downloaded in a single file per tag. It is much better practice to run all processing in R, rather than splitting the files prior to this workflow. With a couple of tweaks, the workflow can filter raw tracking data files containing multiple deployments:\n\nMake sure TagID is included in the file name, instead of individual ID\nWhen reading in the raw data, replace ID with TagID\nMake sure the metadata includes one row per unique tag deployment per individual\nMake sure the metadata file and df_metadataslim contain both ID and TagID\nWhen combining raw and metadata, left_join by TagID, instead of ID:\n\n\ndf_metamerged &lt;- df_raw %&gt;%\n  left_join(., df_metadataslim, by=\"TagID\") \n\nWhen joining the raw data and metadata, the raw data will be duplicated for each row of TagID in the metadata (i.e., for each ID). When we filter the data to the deployment period only (df_clean) the duplicated rows, where tracking DateTimes are outside of the individual‚Äôs deployment period, are removed.\nAn example of how to use multiple ID columns in this way is included in the troubleshooting script, named Troubleshoot - multiple ID columns.R."
  },
  {
    "objectID": "FAQs.html#geographic-projection-issues",
    "href": "FAQs.html#geographic-projection-issues",
    "title": "FAQ‚Äôs",
    "section": "4 Geographic projection issues",
    "text": "4 Geographic projection issues\n\nWhat is a CRS?\nCoordinate Reference Systems (or CRS) are the framework we use to represent locations from a spherical earth on a two-dimensional plot. Usually we want to ‚Äúproject‚Äù these 3D points to a 2D plot in different ways, depending on the range and scale of our spatial data, and we use CRS codes to do this easily. Two common ESPG codes we use in our workflow are LatLon (4326) and Spherical mercator/WGS (3857), but you can also use the crssuggest package to automatically determine the appropriate CRS for your data.\nCRS for sf objects are usually defined at creation (e.g.¬†the crs = argument of st_as_sf), but can also be retrieved or replaced with st_crs, and transformed from one projection to another with st_transform.\nMore information on these concepts can be found in the sf documentation (see: The flat earth model | spherical geometry | miscellaneous)\n\n\nWhat does this st_intersects warning mean?\n\n\n## although coordinates are longitude/latitude, st_intersects assumes that they are planar\n\n\nThis warning message relates to the way that sf deals with geographic coordinates and how it assumes they are projected. For some types of geometric operations, it is very important to be aware of the current projection (see this r-spatial post for a nice explanation of the issue). For more general information on sf error messages, see the sf misc page."
  },
  {
    "objectID": "FAQs.html#timezones",
    "href": "FAQs.html#timezones",
    "title": "FAQ‚Äôs",
    "section": "5 Timezones",
    "text": "5 Timezones\n\nHow do I find a list of timezone codes?\nTo find the code for a specific timezone, you can search the full list of tz‚Äôs that R supports by using Olsonnames():\n\nhead(OlsonNames())\n\n[1] \"Africa/Abidjan\"     \"Africa/Accra\"       \"Africa/Addis_Ababa\"\n[4] \"Africa/Algiers\"     \"Africa/Asmara\"      \"Africa/Asmera\"     \n\n\n\nLists of supported timezone codes can also be found online (e.g.¬†wikipedia\n\n\n\nConverting between timezones\nIf you have data from different timezones and you want to convert everything to a common format, you can use with_tz from the lubridate package (example below taken from the function page):\n\nx &lt;- ymd_hms(\"2009-08-07 00:00:01\", tz = \"America/New_York\")\nwith_tz(x, \"GMT\") #converts US DatTime to UK DateTime\n\n[1] \"2009-08-07 04:00:01 GMT\""
  },
  {
    "objectID": "FAQs.html#resource-limitations",
    "href": "FAQs.html#resource-limitations",
    "title": "FAQ‚Äôs",
    "section": "6 Resource limitations",
    "text": "6 Resource limitations\n\nVector memory exhausted\nWhen you‚Äôre working with thousands or even millions of tracking data points, sometimes you might come across the following message:\n\n\nError: vector memory exhausted (limit reached?)\n\n\nThis essentially means you‚Äôve run out of memory (RAM), and R like a big baby is refusing to carry on. While we‚Äôd recommend first trying code testing and optimization, an easy brute-force solution is to increase the amount of memory R can access, by allocating some of your disk alongside existing RAM. We can do this by editing the .Renviron file (which R reads when it starts up):\n\nlibrary(usethis) #usethis is a package with a selection of handy functions\nusethis::edit_r_environ() #this function allows us to directly edit the .Renviron file from R Studio\n\nThen add R_MAX_VSIZE=32Gb as a line to the .Renviron file that opens up (32Gb can be changed to whatever you want, but make sure this value includes the amount of RAM on your machine), then save it and restart the R session for changes to take effect. To return to default allocation, run the code a second time and remove the line you added. More detail (and the source of the above solution) can be found on Stackoverflow."
  },
  {
    "objectID": "FAQs.html#p.s-why-isnt-exmove-a-package",
    "href": "FAQs.html#p.s-why-isnt-exmove-a-package",
    "title": "FAQ‚Äôs",
    "section": "7 P.S: Why isn‚Äôt ExMove a package?",
    "text": "7 P.S: Why isn‚Äôt ExMove a package?\nAs sp is gradually replaced by sf, and tidyverse gains traction in the R community, researchers (ourselves included) working with animal movement data are in a position where they need to either update their existing code, or find access to modern learning resources. Unfortunately, there are only limited resources that show how use sf and tidyverse for animal movement in this way. Rather than create a new package (given that sf provides nearly all the required functions already), we opted to provide this code directly, in the same workflow format that we‚Äôd use it. This is so users can see and understand what is happening at each step, and can modify the workflow to suit their own needs. With the increasing importance of reproducibility in the scientific community, we hope that an open-access approach will help to make animal movement analysis more accessible and transparent."
  },
  {
    "objectID": "Glossary.html",
    "href": "Glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page contains definitions for all of the user input parameters that are in the user guide.\n\n\n\nspecies_code ‚Äì Throughout the workflow we use this parameter as a file/folder identifier for saving outputs. We suggest use a species or study site abbreviation. I.e. the data set for immature red-footed boobies became ‚ÄúRFB_IMM‚Äù.\nfilepath ‚Äì This object specifies the location where the raw data files are stored. We use the here function to define the filepath for universality and to avoid issues when switching between operating systems. Ideally in this folder only the RAW tracking data files should be stored\nfilepattern ‚Äì This specifies the common file pattern for the raw data files with the folder that is specified using the filepath user input parameter. If only the RAW data files are in this folder then the user can just use the file type as the common file pattern e.g.¬†‚Äú.txt‚Äù or ‚Äù.csv‚Äù. The asterisk ‚Äú*‚Äù is used to match any preceding characters in the filename.\nIDstart; IDend ‚Äì When setting up the data structure, we recommend saving a file for each individual animal with a unique identifier in the filename. The IDstart and IDend parameters represent numeric values denoting the start and end position of the unique individual identifier within the filenames. NB: the individual identifier should therefore be in the same location within the file name across all files.\nskiplines ‚Äì this parameter denotes the number of lines at the top of the file to skip when it is read into R. The default value for this parameter is 0 so no lines are skipped. If for example you have a text file with additional info at the top that does not need to be read into R then you can increase this to the appropriate number of rows.\ndate_formats; datetime_formats ‚Äì These parameters are used to specify the date format and datetime formats of the tracking data respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\ntrackingdatatimezone ‚Äì this parameter is used to specify the timezone which the tracking data is collected in e.g.¬†‚ÄúGMT‚Äù. In R run the function OlsonNames() to get a full list of time zones. The user should know what timezone there data was recorded in.\ncolnames ‚Äì If column names are not present within the raw tracking data files, this parameter can be used to specify column names as a vector, e.g.¬†c(‚ÄúDate‚Äù, ‚ÄúTime‚Äù, ‚ÄúLat‚Äù, ‚ÄúLong‚Äù). The default value is TRUE, which assumes column names are specified within the raw files.\nuser_delim - set the eliminator for your RAW data files, this will depend on the file type that your data are saved as. Text files are often tab delaminated (‚Äú) and .csv are comma separated (‚Äù,‚Äú). Other common eliminators also include‚Äù;‚Äú,‚Äù/‚Äú,‚Äù‚Äù or ‚Äú|‚Äù.\nuser_trim_ws - Specify as either TRUE or FALSE. Allows white space at the bottom of files to be trimmed, e.g.¬†empty cells at the bottom of an excel document.\nID_type ‚Äì data needs a unique individual identifier to differentiate between individuals. This user input parameter allows you to specify the column name that the individual identifier will be stored in. The individual identifier will be taken from the file name using the IDstart and IDend input parameters.\ndatetime_colnames ‚Äì this parameter defines the column(s) in which date and time information are stored. If date and time are stored in separate columns then specify two columns, e.g.¬†c(‚ÄúDate‚Äù, ‚ÄúTime‚Äù). NB: these have to be in the same order as they are stored in the data set.\n\n\n\n\n\nfilepath_meta ‚Äì this parameter defines the filepath for the metadata file. Again this parameter is specified using the here function for universality.\nmetadate_formats; metadatetime_formats ‚Äì These parameters are used to specify the date format and datetime formats of the metadata respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\nmetatrackingdatatimezone ‚Äì this parameter is used to specify the timezone which the tracking data is collected in e.g.¬†‚ÄúGMT‚Äù. In R run the function OlsonNames() to get a full list of time zones.\n\n\n\n\n\nNo_data_vals ‚Äì this parameter is used to specify any values within the location (X, Y) columns that represent missing data. This is often tag-specific with common examples being ‚Äú0‚Äù or ‚Äú-999‚Äù but these may not always be applicable.\nna_cols ‚Äì this parameter is used to define a vector of column names which cannot contain NA values in order for a row to be retained. We suggest that columns containing information relating to X location, Y location, datetime and ID should not be allowed to contain NA values.\n\n\n\n\n\ntracking_crs; meta_crs ‚Äì These parameters are used to specify the co-ordinate reference systems for the tracking and metadata respectively using EPSG codes. The default is lat/long which is EPSG code 4326. For a full list of EPSG cdoes see here: https://spatialreference.org/ref/epsg/\ntransform_crs - Specify metric co-ordinate projection system to transform the data into for distance calculations with units in metres. WE STRONGLY RECCOMEND THAT YOU CHANGE THIS FOR YOUR STUDY SYSTEM/LOCATION. For advice on what to change this to see our FAQ document as the choice is nuanced and can have important implications on the metric calculated.\n\n\n\n\n\nfilepath_dfout ‚Äì this parameter is used to define the file path to save out the processed tracking data file again using the here function for universality. The data is read out at this point so that it can be read into the shiny app.\nfilename_dfout ‚Äì this parameter specifies the filename of the saved tracking data file.\n\n\n\n\n\nfilter_cutoff ‚Äì users may want to remove a section of the tracking data immediately following tag deployment while the animal recovers from the tagging process and adjusts to the device. This parameter is used to specify the length of this period along with the units of time ‚Äì e.g.¬†minutes, hours, days.\nfilter_speed ‚Äì this parameter is used to define a numeric value for the speed above which locations are classified as erroneous points and removed based on unrealistic values. The units are in metres per second as long as transform_crs was set to a projection that is in metres. The user should use ecological knowledge of their study system to set this parameter and should check the effect it has using summaries and visualisations.\nFilter_nestdisp_dist; filter_netdist_units ‚Äì these parameters are used to define a threshold value for net squared displacement above which locations are considered as erroneous and select the distance units for this value e.g.¬†metres/kilometers.\n\n\n\n\n\nsampleRateUnits ‚Äì this parameter is used to define the units of time for the sampling rate calculating which is displayed in the summary table e.g.¬†minutes/hours. Can be specified as one of the following: ‚Äúsecs‚Äù, ‚Äúmins‚Äù, ‚Äúhours‚Äù, ‚Äúdays‚Äù or ‚Äúweeks‚Äù.\ngrouping_factors_poplevel ‚Äì this parameter is used to define the grouping factors for the population-level summary table e.g.¬†species/population/colony. This parameter can be made into a vector to include additional grouping factors such as age or sex.\ngrouping_factors_indlevel - this parameter is used to define the grouping factors for the individual-level summary table, usually individual ID. This parameter can be made into a vector to include additional grouping factors such as temporal window (e.g.¬†month).\n\n\n\n\n\nfilepath_filtered_out - this parameter is used to define the file path to save the filtered tracking data file again using the here function for universality.\nfilepath_summary_out - this parameter is used to define the file path to save out the summary data frames again using the here function for universality.\nfilename_filtered_out; filename_summary_pop_out; filename_summary_ind_out ‚Äì these parameters are used to specify the filenames for the filtered tracking data file and the population-level and individual-level summary files respectively.\n\n\n\n\n\ndevice ‚Äì this parameter is used to specify the filetype for saving out visualization plots e.g.¬†‚Äújpeg‚Äù, ‚Äúpng‚Äù or ‚Äútiff‚Äù\nunits ‚Äì this parameter defines the units for saving out the visualization plots e.g.¬†‚Äúmm‚Äù or ‚Äúcm‚Äù.\ndpi ‚Äì this parameter is a numeric value which specifies the resolution for saving plots in dpi. The minimum value is usually 300.\nout_path ‚Äì this parameter specifies the filepath for reading out plots using the here function.\ntopo_label ‚Äì we plot maps of the telemetry data over a topography base map. This parameter specifies the legend label for the topography data depending on the study system e.g.¬†‚Äúdepth (m)‚Äù or ‚Äúelevation (m)‚Äù\n\n\n\n\n\nfilepath_final ‚Äì this parameter specifies the filepath for reading the final data frame back into the main workflow once the any optional post-processing has been performed. This parameter is specified using the ‚Äúhere‚Äù function for universality. If no optional post-processing is performed this can be set to read in the filtered data file saved in section 9.\n\n\n\n\n\ntz_data ‚Äì timezone of the final tracking data to be uploaded. In R run the function OlsonNames() to get a full list of time zones.\ngrouping_factors ‚Äì a vector of grouping factors for the final reformatted tracking data. Each level will be saved as a separate file e.g.¬†ID/Age/Sex/Species\nfilepath_dfout ‚Äì the filepath for saving out the reformatted data for database upload created using the here function.\n\n\n\n\nNOTE: there are a number of user input parameters in this script that are repeats of the ones above. If you can‚Äôt find the user input parameter below then use the search function to find it above.\n\nthreshold_dist ‚Äì this parameter is used to define a threshold buffer distance in metres from the central place to label points as ‚Äúat the central place‚Äù or ‚Äúaway from the central place‚Äù in order to define foraging trips. The choice of this parameter is dependent on the study species. For instance, seabirds can sit on the sea some distance from the central place while not having started a foraging trip.\nthreshold_time ‚Äì this parameter is used to define the minimum length of time in minutes required for an absence from the central location to be classified as a trip. The duration of each consecutive series of locations classified as ‚Äúaway from the central place‚Äù is calculated and those series with a duration less than threshold_time are removed.\nshapefilepath ‚Äì this parameter is used to define the filepath to read in a shapefile used to define the extent of a central place or colony for the purposes of defining trips. This is optional and the central place can be specified alternatively using a single X, Y location.\n\n\n\n\n\ntime_unit - select the time unit to summaries and visualise sampling intervals at. Options include ‚Äúsecs‚Äù, ‚Äúmins‚Äù, ‚Äúhours‚Äù, ‚Äúdays‚Äù or ‚Äúweeks‚Äù.\nsubsampling_unit ‚Äì this parameter defines the time unit for the resampling the tracking data to. Options include ‚Äúsecs‚Äù, ‚Äúmins‚Äù, ‚Äúhours‚Äù, ‚Äúdays‚Äù or ‚Äúweeks‚Äù.\nsubsampling_res ‚Äì this parameter is a numerical value which defines the resolution which the data is resampled to. NOTE: if sub-sampling to an interval greater than 60 minutes (e.g.¬†2 hours) then change the unit to ‚Äúhours‚Äù and resolution to 2. Do not use 120 ‚Äúmins‚Äù.\n\n\n\n\n\nunits_df_datetime ‚Äì this parameter specifies the units of the time difference column that will be created in the data. Due to earlier filtering the duration between successive locations is recalculated again in this script.\nthreshold_time ‚Äì this parameter defines the threshold time value above which gaps in the data are split and labelled as separate segments. Therefore if the difftime column in the data set exceeds this duration then the track will be split at that point.\nthreshold_points ‚Äì this parameter is a numerical value which defines the minimum number of data points required for a valid segment. If a segment is below this threshold then all the locations contained within it are removed from the data set. These small segments are often not of use for further analysis that requires segmented data with consistent sampling intervals, e.g.¬†hidden markov models."
  },
  {
    "objectID": "Glossary.html#read-in-data-files",
    "href": "Glossary.html#read-in-data-files",
    "title": "Glossary",
    "section": "",
    "text": "species_code ‚Äì Throughout the workflow we use this parameter as a file/folder identifier for saving outputs. We suggest use a species or study site abbreviation. I.e. the data set for immature red-footed boobies became ‚ÄúRFB_IMM‚Äù.\nfilepath ‚Äì This object specifies the location where the raw data files are stored. We use the here function to define the filepath for universality and to avoid issues when switching between operating systems. Ideally in this folder only the RAW tracking data files should be stored\nfilepattern ‚Äì This specifies the common file pattern for the raw data files with the folder that is specified using the filepath user input parameter. If only the RAW data files are in this folder then the user can just use the file type as the common file pattern e.g.¬†‚Äú.txt‚Äù or ‚Äù.csv‚Äù. The asterisk ‚Äú*‚Äù is used to match any preceding characters in the filename.\nIDstart; IDend ‚Äì When setting up the data structure, we recommend saving a file for each individual animal with a unique identifier in the filename. The IDstart and IDend parameters represent numeric values denoting the start and end position of the unique individual identifier within the filenames. NB: the individual identifier should therefore be in the same location within the file name across all files.\nskiplines ‚Äì this parameter denotes the number of lines at the top of the file to skip when it is read into R. The default value for this parameter is 0 so no lines are skipped. If for example you have a text file with additional info at the top that does not need to be read into R then you can increase this to the appropriate number of rows.\ndate_formats; datetime_formats ‚Äì These parameters are used to specify the date format and datetime formats of the tracking data respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\ntrackingdatatimezone ‚Äì this parameter is used to specify the timezone which the tracking data is collected in e.g.¬†‚ÄúGMT‚Äù. In R run the function OlsonNames() to get a full list of time zones. The user should know what timezone there data was recorded in.\ncolnames ‚Äì If column names are not present within the raw tracking data files, this parameter can be used to specify column names as a vector, e.g.¬†c(‚ÄúDate‚Äù, ‚ÄúTime‚Äù, ‚ÄúLat‚Äù, ‚ÄúLong‚Äù). The default value is TRUE, which assumes column names are specified within the raw files.\nuser_delim - set the eliminator for your RAW data files, this will depend on the file type that your data are saved as. Text files are often tab delaminated (‚Äú) and .csv are comma separated (‚Äù,‚Äú). Other common eliminators also include‚Äù;‚Äú,‚Äù/‚Äú,‚Äù‚Äù or ‚Äú|‚Äù.\nuser_trim_ws - Specify as either TRUE or FALSE. Allows white space at the bottom of files to be trimmed, e.g.¬†empty cells at the bottom of an excel document.\nID_type ‚Äì data needs a unique individual identifier to differentiate between individuals. This user input parameter allows you to specify the column name that the individual identifier will be stored in. The individual identifier will be taken from the file name using the IDstart and IDend input parameters.\ndatetime_colnames ‚Äì this parameter defines the column(s) in which date and time information are stored. If date and time are stored in separate columns then specify two columns, e.g.¬†c(‚ÄúDate‚Äù, ‚ÄúTime‚Äù). NB: these have to be in the same order as they are stored in the data set."
  },
  {
    "objectID": "Glossary.html#merge-with-metadata",
    "href": "Glossary.html#merge-with-metadata",
    "title": "Glossary",
    "section": "",
    "text": "filepath_meta ‚Äì this parameter defines the filepath for the metadata file. Again this parameter is specified using the here function for universality.\nmetadate_formats; metadatetime_formats ‚Äì These parameters are used to specify the date format and datetime formats of the metadata respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\nmetatrackingdatatimezone ‚Äì this parameter is used to specify the timezone which the tracking data is collected in e.g.¬†‚ÄúGMT‚Äù. In R run the function OlsonNames() to get a full list of time zones."
  },
  {
    "objectID": "Glossary.html#cleaning",
    "href": "Glossary.html#cleaning",
    "title": "Glossary",
    "section": "",
    "text": "No_data_vals ‚Äì this parameter is used to specify any values within the location (X, Y) columns that represent missing data. This is often tag-specific with common examples being ‚Äú0‚Äù or ‚Äú-999‚Äù but these may not always be applicable.\nna_cols ‚Äì this parameter is used to define a vector of column names which cannot contain NA values in order for a row to be retained. We suggest that columns containing information relating to X location, Y location, datetime and ID should not be allowed to contain NA values."
  },
  {
    "objectID": "Glossary.html#processing",
    "href": "Glossary.html#processing",
    "title": "Glossary",
    "section": "",
    "text": "tracking_crs; meta_crs ‚Äì These parameters are used to specify the co-ordinate reference systems for the tracking and metadata respectively using EPSG codes. The default is lat/long which is EPSG code 4326. For a full list of EPSG cdoes see here: https://spatialreference.org/ref/epsg/\ntransform_crs - Specify metric co-ordinate projection system to transform the data into for distance calculations with units in metres. WE STRONGLY RECCOMEND THAT YOU CHANGE THIS FOR YOUR STUDY SYSTEM/LOCATION. For advice on what to change this to see our FAQ document as the choice is nuanced and can have important implications on the metric calculated."
  },
  {
    "objectID": "Glossary.html#save-df_diagnostic",
    "href": "Glossary.html#save-df_diagnostic",
    "title": "Glossary",
    "section": "",
    "text": "filepath_dfout ‚Äì this parameter is used to define the file path to save out the processed tracking data file again using the here function for universality. The data is read out at this point so that it can be read into the shiny app.\nfilename_dfout ‚Äì this parameter specifies the filename of the saved tracking data file."
  },
  {
    "objectID": "Glossary.html#filtering",
    "href": "Glossary.html#filtering",
    "title": "Glossary",
    "section": "",
    "text": "filter_cutoff ‚Äì users may want to remove a section of the tracking data immediately following tag deployment while the animal recovers from the tagging process and adjusts to the device. This parameter is used to specify the length of this period along with the units of time ‚Äì e.g.¬†minutes, hours, days.\nfilter_speed ‚Äì this parameter is used to define a numeric value for the speed above which locations are classified as erroneous points and removed based on unrealistic values. The units are in metres per second as long as transform_crs was set to a projection that is in metres. The user should use ecological knowledge of their study system to set this parameter and should check the effect it has using summaries and visualisations.\nFilter_nestdisp_dist; filter_netdist_units ‚Äì these parameters are used to define a threshold value for net squared displacement above which locations are considered as erroneous and select the distance units for this value e.g.¬†metres/kilometers."
  },
  {
    "objectID": "Glossary.html#summarise-cleaned-filtered-tracking-data",
    "href": "Glossary.html#summarise-cleaned-filtered-tracking-data",
    "title": "Glossary",
    "section": "",
    "text": "sampleRateUnits ‚Äì this parameter is used to define the units of time for the sampling rate calculating which is displayed in the summary table e.g.¬†minutes/hours. Can be specified as one of the following: ‚Äúsecs‚Äù, ‚Äúmins‚Äù, ‚Äúhours‚Äù, ‚Äúdays‚Äù or ‚Äúweeks‚Äù.\ngrouping_factors_poplevel ‚Äì this parameter is used to define the grouping factors for the population-level summary table e.g.¬†species/population/colony. This parameter can be made into a vector to include additional grouping factors such as age or sex.\ngrouping_factors_indlevel - this parameter is used to define the grouping factors for the individual-level summary table, usually individual ID. This parameter can be made into a vector to include additional grouping factors such as temporal window (e.g.¬†month)."
  },
  {
    "objectID": "Glossary.html#save-df_filtered-and-summary-data",
    "href": "Glossary.html#save-df_filtered-and-summary-data",
    "title": "Glossary",
    "section": "",
    "text": "filepath_filtered_out - this parameter is used to define the file path to save the filtered tracking data file again using the here function for universality.\nfilepath_summary_out - this parameter is used to define the file path to save out the summary data frames again using the here function for universality.\nfilename_filtered_out; filename_summary_pop_out; filename_summary_ind_out ‚Äì these parameters are used to specify the filenames for the filtered tracking data file and the population-level and individual-level summary files respectively."
  },
  {
    "objectID": "Glossary.html#visualisation",
    "href": "Glossary.html#visualisation",
    "title": "Glossary",
    "section": "",
    "text": "device ‚Äì this parameter is used to specify the filetype for saving out visualization plots e.g.¬†‚Äújpeg‚Äù, ‚Äúpng‚Äù or ‚Äútiff‚Äù\nunits ‚Äì this parameter defines the units for saving out the visualization plots e.g.¬†‚Äúmm‚Äù or ‚Äúcm‚Äù.\ndpi ‚Äì this parameter is a numeric value which specifies the resolution for saving plots in dpi. The minimum value is usually 300.\nout_path ‚Äì this parameter specifies the filepath for reading out plots using the here function.\ntopo_label ‚Äì we plot maps of the telemetry data over a topography base map. This parameter specifies the legend label for the topography data depending on the study system e.g.¬†‚Äúdepth (m)‚Äù or ‚Äúelevation (m)‚Äù"
  },
  {
    "objectID": "Glossary.html#post-processing-optional-steps",
    "href": "Glossary.html#post-processing-optional-steps",
    "title": "Glossary",
    "section": "",
    "text": "filepath_final ‚Äì this parameter specifies the filepath for reading the final data frame back into the main workflow once the any optional post-processing has been performed. This parameter is specified using the ‚Äúhere‚Äù function for universality. If no optional post-processing is performed this can be set to read in the filtered data file saved in section 9."
  },
  {
    "objectID": "Glossary.html#reformat-data-for-upload-to-public-databases",
    "href": "Glossary.html#reformat-data-for-upload-to-public-databases",
    "title": "Glossary",
    "section": "",
    "text": "tz_data ‚Äì timezone of the final tracking data to be uploaded. In R run the function OlsonNames() to get a full list of time zones.\ngrouping_factors ‚Äì a vector of grouping factors for the final reformatted tracking data. Each level will be saved as a separate file e.g.¬†ID/Age/Sex/Species\nfilepath_dfout ‚Äì the filepath for saving out the reformatted data for database upload created using the here function."
  },
  {
    "objectID": "Glossary.html#optional-processing_central-place-trips.r",
    "href": "Glossary.html#optional-processing_central-place-trips.r",
    "title": "Glossary",
    "section": "",
    "text": "NOTE: there are a number of user input parameters in this script that are repeats of the ones above. If you can‚Äôt find the user input parameter below then use the search function to find it above.\n\nthreshold_dist ‚Äì this parameter is used to define a threshold buffer distance in metres from the central place to label points as ‚Äúat the central place‚Äù or ‚Äúaway from the central place‚Äù in order to define foraging trips. The choice of this parameter is dependent on the study species. For instance, seabirds can sit on the sea some distance from the central place while not having started a foraging trip.\nthreshold_time ‚Äì this parameter is used to define the minimum length of time in minutes required for an absence from the central location to be classified as a trip. The duration of each consecutive series of locations classified as ‚Äúaway from the central place‚Äù is calculated and those series with a duration less than threshold_time are removed.\nshapefilepath ‚Äì this parameter is used to define the filepath to read in a shapefile used to define the extent of a central place or colony for the purposes of defining trips. This is optional and the central place can be specified alternatively using a single X, Y location."
  },
  {
    "objectID": "Glossary.html#optional-processing_resampling.r",
    "href": "Glossary.html#optional-processing_resampling.r",
    "title": "Glossary",
    "section": "",
    "text": "time_unit - select the time unit to summaries and visualise sampling intervals at. Options include ‚Äúsecs‚Äù, ‚Äúmins‚Äù, ‚Äúhours‚Äù, ‚Äúdays‚Äù or ‚Äúweeks‚Äù.\nsubsampling_unit ‚Äì this parameter defines the time unit for the resampling the tracking data to. Options include ‚Äúsecs‚Äù, ‚Äúmins‚Äù, ‚Äúhours‚Äù, ‚Äúdays‚Äù or ‚Äúweeks‚Äù.\nsubsampling_res ‚Äì this parameter is a numerical value which defines the resolution which the data is resampled to. NOTE: if sub-sampling to an interval greater than 60 minutes (e.g.¬†2 hours) then change the unit to ‚Äúhours‚Äù and resolution to 2. Do not use 120 ‚Äúmins‚Äù."
  },
  {
    "objectID": "Glossary.html#optional-processing_segmentation.r",
    "href": "Glossary.html#optional-processing_segmentation.r",
    "title": "Glossary",
    "section": "",
    "text": "units_df_datetime ‚Äì this parameter specifies the units of the time difference column that will be created in the data. Due to earlier filtering the duration between successive locations is recalculated again in this script.\nthreshold_time ‚Äì this parameter defines the threshold time value above which gaps in the data are split and labelled as separate segments. Therefore if the difftime column in the data set exceeds this duration then the track will be split at that point.\nthreshold_points ‚Äì this parameter is a numerical value which defines the minimum number of data points required for a valid segment. If a segment is below this threshold then all the locations contained within it are removed from the data set. These small segments are often not of use for further analysis that requires segmented data with consistent sampling intervals, e.g.¬†hidden markov models."
  }
]