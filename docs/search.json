[
  {
    "objectID": "FAQs.html",
    "href": "FAQs.html",
    "title": "FAQ’s",
    "section": "",
    "text": "Below is a list of common issues/errors that you might come across when using the workflow we provide. It’s not exhaustive, but hopefully helpful for solving problems that are likely to occur if you are analysing your own data."
  },
  {
    "objectID": "FAQs.html#introduction",
    "href": "FAQs.html#introduction",
    "title": "FAQ’s",
    "section": "",
    "text": "Below is a list of common issues/errors that you might come across when using the workflow we provide. It’s not exhaustive, but hopefully helpful for solving problems that are likely to occur if you are analysing your own data."
  },
  {
    "objectID": "FAQs.html#getting-the-workflow-up-and-running",
    "href": "FAQs.html#getting-the-workflow-up-and-running",
    "title": "FAQ’s",
    "section": "1 Getting the workflow up and running",
    "text": "1 Getting the workflow up and running\n\n1.1 GitHub issues\nUsing GitHub is not essential for using this workflow — you can download the repo from  &gt; Download Zip, unzip to your machine, and start by opening the .Rproj file.\nHowever, if you want to contribute improvements to the repo, or are just interested in getting to grips with GitHub for your own research projects, there are great resources online for getting started (see: setting up GitHub | cloning a repo | using GitHub with RStudio).\n\n\n1.2 Using RStudio projects and here for reproducible filepaths\nWhen we open a .Rproj file (which should be the starting point for this workflow), this opens up a fresh instance of RStudio, with a dedicated project environment and access to all the folders and files contained inside that project folder. If you’re not familiar with using RStudio projects, have a look at this section of R for Data Science for a quick explained.\nBy combining Rstudio projects with the here package, we can create relative filepaths that work across machines, so you won’t have to edit a complicated filepath to get the code to run (for more info see the ‘Getting started’ section of the here documentation)."
  },
  {
    "objectID": "FAQs.html#reporting-in-a-paper",
    "href": "FAQs.html#reporting-in-a-paper",
    "title": "FAQ’s",
    "section": "2 Reporting in a paper",
    "text": "2 Reporting in a paper\n\n2.1 How to report usage of this workflow in a paper\nIf you have used this workflow and you wish to report this in a scientific publication then we provide the text below as a suggestion as to how the user might do so.\n“Animal tracking data in this study was cleaned following the key principles of clean data in Langley et al. (2023). Duplicated and erroneous data points were removed, a maximum speed filter of XXm/s applied and data YYhours after initial deployment removed”"
  },
  {
    "objectID": "FAQs.html#data-problems",
    "href": "FAQs.html#data-problems",
    "title": "FAQ’s",
    "section": "3 Data problems",
    "text": "3 Data problems\n\n3.1 What if I have multiple data files for an individual?\nLots of data sets include multiple files for a single individual, when individuals are tracked more than once. The workflow can filter raw tracking data over multiple deployments. For it to work, make sure to check:\n\nThe raw data contain individual ID, read in from the file name\nThe metadata includes one row per unique tag deployment per individual\nThe metadata contains a column with a unique value for each deployment (e.g., DeployID/Year)\nThis identifying column is included in df_metadataslim\n\nWhen joining the raw data and metadata, the raw data will be duplicated for each row of ID in the metadata (i.e., for each DeployID/Year). When we filter the data to the deployment period only (df_clean) the duplicated rows, where tracking DateTimes are outside of the individual’s deployment period, are removed.\nAn example of how to use multiple ID columns in this way is included in the troubleshooting script, named Troubleshoot - multiple ID columns.R.\n\n\n3.2 What if my tags have been re-deployed on multiple individuals?\nSometimes tags are used multiple times on different individuals, but data are downloaded in a single file per tag. It is much better practice to run all processing in R, rather than splitting the files prior to this workflow. With a couple of tweaks, the workflow can filter raw tracking data files containing multiple deployments:\n\nMake sure TagID is included in the file name, instead of individual ID\nWhen reading in the raw data, replace ID with TagID\nMake sure the metadata includes one row per unique tag deployment per individual\nMake sure the metadata file and df_metadataslim contain both ID and TagID\nWhen combining raw and metadata, left_join by TagID, instead of ID:\n\n\ndf_metamerged &lt;- df_raw %&gt;%\n  left_join(., df_metadataslim, by=\"TagID\") \n\nWhen joining the raw data and metadata, the raw data will be duplicated for each row of TagID in the metadata (i.e., for each ID). When we filter the data to the deployment period only (df_clean) the duplicated rows, where tracking DateTimes are outside of the individual’s deployment period, are removed.\nAn example of how to use multiple ID columns in this way is included in the troubleshooting script, named Troubleshoot - multiple ID columns.R."
  },
  {
    "objectID": "FAQs.html#geographic-projection-issues",
    "href": "FAQs.html#geographic-projection-issues",
    "title": "FAQ’s",
    "section": "4 Geographic projection issues",
    "text": "4 Geographic projection issues\n\n4.1 What is a CRS?\nCoordinate Reference Systems (or CRS) are the framework we use to represent locations from a spherical earth on a two-dimensional plot. Usually we want to “project” these 3D points to a 2D plot in different ways, depending on the range and scale of our spatial data, and we use CRS codes to do this easily. Two common ESPG codes we use in our workflow are LatLon (4326) and Spherical mercator/WGS (3857), but you can also use the crssuggest package to automatically determine the appropriate CRS for your data.\nCRS for sf objects are usually defined at creation (e.g. the crs = argument of st_as_sf), but can also be retrieved or replaced with st_crs, and transformed from one projection to another with st_transform.\nMore information on these concepts can be found in the sf documentation (see: The flat earth model | spherical geometry | miscellaneous)\n\n\n4.2 What does this st_intersects warning mean?\n\n\n## although coordinates are longitude/latitude, st_intersects assumes that they are planar\n\n\nMiscellaneous sf questions that could either be addressed here, or just pointed to the sf misc page"
  },
  {
    "objectID": "FAQs.html#timezones",
    "href": "FAQs.html#timezones",
    "title": "FAQ’s",
    "section": "5 Timezones",
    "text": "5 Timezones\n\n5.1 How do I find a list of timezone codes?\nTo find the code for a specific timezone, you can search the full list of tz’s that R supports by using Olsonnames():\n\nhead(OlsonNames())\n\n[1] \"Africa/Abidjan\"     \"Africa/Accra\"       \"Africa/Addis_Ababa\"\n[4] \"Africa/Algiers\"     \"Africa/Asmara\"      \"Africa/Asmera\"     \n\n\n\nLists of supported timezone codes can also be found online (e.g. wikipedia\n\n\n\n5.2 Converting between timezones\nIf you have data from different timezones and you want to convert everything to a common format, you can use with_tz from the lubridate package (example below taken from the function page):\n\nx &lt;- ymd_hms(\"2009-08-07 00:00:01\", tz = \"America/New_York\")\nwith_tz(x, \"GMT\") #converts US DatTime to UK DateTime\n\n[1] \"2009-08-07 04:00:01 GMT\""
  },
  {
    "objectID": "FAQs.html#resource-limitations",
    "href": "FAQs.html#resource-limitations",
    "title": "FAQ’s",
    "section": "6 Resource limitations",
    "text": "6 Resource limitations\n\n6.1 Vector memory exhausted\nWhen you’re working with thousands or even millions of tracking data points, sometimes you might come across the following message:\n\n\nError: vector memory exhausted (limit reached?)\n\n\nThis essentially means you’ve run out of memory (RAM), and R like a big baby is refusing to carry on. While we’d recommend first trying code testing and optimization, an easy brute-force solution is to increase the amount of memory R can access, by allocating some of your disk alongside existing RAM. We can do this by editing the .Renviron file (which R reads when it starts up):\n\nlibrary(usethis) #usethis is a package with a selection of handy functions\nusethis::edit_r_environ() #this function allows us to directly edit the .Renviron file from R Studio\n\nThen add R_MAX_VSIZE=32Gb as a line to the .Renviron file that opens up (32Gb can be changed to whatever you want, but make sure this value includes the amount of RAM on your machine), then save it and restart the R session for changes to take effect. To return to default allocation, run the code a second time and remove the line you added. More detail (and the source of the above solution) can be found on Stackoverflow."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ExMove",
    "section": "",
    "text": "Welcome to the home page of ExMove. Here, you can find the resources to use our toolkit for processing biologging data from tag downloads to online archive.\nBelow is an overview of the workflow and what you can do with it:\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing': 30}}}%%\nflowchart LR\n  S1[Read in data] ==&gt; S3(Merge)\n  S2[Read in metadata] ==&gt; S3 ==&gt; A(Clean)\n  subgraph shiny [\"(Parameter determination in Shiny app)\"]\n  style shiny fill:#fbfbfb, stroke:#d3d3d3, stroke-width:px\n  A(Clean) ==&gt; B(Process) ==&gt; C(Filter)\n  D(Optional&lt;br/&gt;scripts)\n  end\n  C ==&gt; S{Standardised&lt;br/&gt;dataset}\n  C --&gt; D --&gt; S\n  S --&gt; E(Analyses)\n  S ==&gt; F(Archive)\n  linkStyle 6,7 stroke-dasharray: 4 4"
  },
  {
    "objectID": "Glossary.html",
    "href": "Glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page contains definitions for all of the user input parameters that are in the user guide.\n\n\n\nspecies_code – Throughout the workflow we use this parameter as a file/folder identifier for saving outputs. We suggest use a species or study site abbreviation. I.e. the data set for immature red-footed boobies became “RFB_IMM”.\nfilepath – This object specifies the location where the raw data files are stored. We use the here function to define the filepath for universality and to avoid issues when switching between operating systems. Ideally in this folder only the RAW tracking data files should be stored\nfilepattern – This specifies the common file pattern for the raw data files with the folder that is specified using the filepath user input parameter. If only the RAW data files are in this folder then the user can just use the file type as the common file pattern e.g. “.txt” or “.csv”. The asterisk “*” is used to match any preceding characters in the filename.\nIDstart; IDend – When setting up the data structure, we recommend saving a file for each individual animal with a unique identifier in the filename. The IDstart and IDend parameters represent numeric values denoting the start and end position of the unique individual identifier within the filenames. NB: the individual identifier should therefore be in the same location within the file name across all files.\nskiplines – this parameter denotes the number of lines at the top of the file to skip when it is read into R. The default value for this parameter is 0 so no lines are skipped. If for example you have a text file with additional info at the top that does not need to be read into R then you can increase this to the appropriate number of rows.\ndate_formats; datetime_formats – These parameters are used to specify the date format and datetime formats of the tracking data respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\ntrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones. The user should know what timezone there data was recorded in.\ncolnames – If column names are not present within the raw tracking data files, this parameter can be used to specify column names as a vector, e.g. c(“Date”, “Time”, “Lat”, “Long”). The default value is TRUE, which assumes column names are specified within the raw files.\nuser_delim - set the eliminator for your RAW data files, this will depend on the file type that your data are saved as. Text files are often tab delaminated (“) and .csv are comma separated (“,”). Other common eliminators also include “;”, “/”, “” or “|”.\nuser_trim_ws - Specify as either TRUE or FALSE. Allows white space at the bottom of files to be trimmed, e.g. empty cells at the bottom of an excel document.\nID_type – data needs a unique individual identifier to differentiate between individuals. This user input parameter allows you to specify the column name that the individual identifier will be stored in. The individual identifier will be taken from the file name using the IDstart and IDend input parameters.\ndatetime_colnames – this parameter defines the column(s) in which date and time information are stored. If date and time are stored in separate columns then specify two columns, e.g. c(“Date”, “Time”). NB: these have to be in the same order as they are stored in the data set.\n\n\n\n\n\nfilepath_meta – this parameter defines the filepath for the metadata file. Again this parameter is specified using the here function for universality.\nmetadate_formats; metadatetime_formats – These parameters are used to specify the date format and datetime formats of the metadata respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\nmetatrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones.\n\n\n\n\n\nNo_data_vals – this parameter is used to specify any values within the location (X, Y) columns that represent missing data. This is often tag-specific with common examples being “0” or “-999” but these may not always be applicable.\nna_cols – this parameter is used to define a vector of column names which cannot contain NA values in order for a row to be retained. We suggest that columns containing information relating to X location, Y location, datetime and ID should not be allowed to contain NA values.\n\n\n\n\n\ntracking_crs; meta_crs – These parameters are used to specify the co-ordinate reference systems for the tracking and metadata respectively using EPSG codes. The default is lat/long which is EPSG code 4326. For a full list of EPSG cdoes see here: https://spatialreference.org/ref/epsg/\ntransform_crs - Specify metric co-ordinate projection system to transform the data into for distance calculations with units in metres. WE STRONGLY RECCOMEND THAT YOU CHANGE THIS FOR YOUR STUDY SYSTEM/LOCATION. For advice on what to change this to see our FAQ document as the choice is nuanced and can have important implications on the metric calculated.\n\n\n\n\n\nfilepath_dfout – this parameter is used to define the file path to save out the processed tracking data file again using the here function for universality. The data is read out at this point so that it can be read into the shiny app.\nfilename_dfout – this parameter specifies the filename of the saved tracking data file.\n\n\n\n\n\nfilter_cutoff – users may want to remove a section of the tracking data immediately following tag deployment while the animal recovers from the tagging process and adjusts to the device. This parameter is used to specify the length of this period along with the units of time – e.g. minutes, hours, days.\nfilter_speed – this parameter is used to define a numeric value for the speed above which locations are classified as erroneous points and removed based on unrealistic values. The units are in metres per second as long as transform_crs was set to a projection that is in metres. The user should use ecological knowledge of their study system to set this parameter and should check the effect it has using summaries and visualisations.\nFilter_nestdisp_dist; filter_netdist_units – these parameters are used to define a threshold value for net squared displacement above which locations are considered as erroneous and select the distance units for this value e.g. metres/kilometers.\n\n\n\n\n\nsampleRateUnits – this parameter is used to define the units of time for the sampling rate calculating which is displayed in the summary table e.g. minutes/hours. Can be specified as one of the following: “secs”, “mins”, “hours”, “days” or “weeks”.\ngrouping_factors_poplevel – this parameter is used to define the grouping factors for the population-level summary table e.g. species/population/colony. This parameter can be made into a vector to include additional grouping factors such as age or sex.\ngrouping_factors_indlevel - this parameter is used to define the grouping factors for the individual-level summary table, usually individual ID. This parameter can be made into a vector to include additional grouping factors such as temporal window (e.g. month).\n\n\n\n\n\nfilepath_filtered_out - this parameter is used to define the file path to save the filtered tracking data file again using the here function for universality.\nfilepath_summary_out - this parameter is used to define the file path to save out the summary data frames again using the here function for universality.\nfilename_filtered_out; filename_summary_pop_out; filename_summary_ind_out – these parameters are used to specify the filenames for the filtered tracking data file and the population-level and individual-level summary files respectively.\n\n\n\n\n\ndevice – this parameter is used to specify the filetype for saving out visualization plots e.g. “jpeg”, “png” or ”tiff”\nunits – this parameter defines the units for saving out the visualization plots e.g. “mm” or “cm”.\ndpi – this parameter is a numeric value which specifies the resolution for saving plots in dpi. The minimum value is usually 300.\nOut_path – this parameter specifies the filepath for reading out plots using the here function.\ntopo_label – we plot maps of the telemetry data over a topography base map. This parameter specifies the legend label for the topography data depending on the study system e.g. “depth (m)” or ”elevation (m)”\n\n\n\n\n\nfilepath_final – this parameter specifies the filepath for reading the final data frame back into the main workflow once the any optional post-processing has been performed. This parameter is specified using the “here” function for universality. If no optional post-processing is performed this can be set to read in the filtered data file saved in section 9.\n\n\n\n\n\ntz_data – timezone of the final tracking data to be uploaded. In R run the function OlsonNames() to get a full list of time zones.\ngrouping_factors – a vector of grouping factors for the final reformatted tracking data. Each level will be saved as a separate file e.g. ID/Age/Sex/Species\nfilepath_dfout – the filepath for saving out the reformatted data for database upload created using the here function.\n\n\n\n\nNOTE: there are a number of user input parameters in this script that are repeats of the ones above. If you can’t find the user input parameter below then use the search function to find it above.\n\nthreshold_dist – this parameter is used to define a threshold buffer distance in metres from the central place to label points as “at the central place” or “away from the central place” in order to define foraging trips. The choice of this parameter is dependent on the study species. For instance, seabirds can sit on the sea some distance from the central place while not having started a foraging trip.\nthreshold_time – this parameter is used to define the minimum length of time in minutes required for an absence from the central location to be classified as a trip. The duration of each consecutive series of locations classified as “away from the central place” is calculated and those series with a duration less than threshold_time are removed.\nshapefilepath – this parameter is used to define the filepath to read in a shapefile used to define the extent of a central place or colony for the purposes of defining trips. This is optional and the central place can be specified alternatively using a single X, Y location.\n\n\n\n\n\nTime_unit - select the time unit to summaries and visualise sampling intervals at. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_unit – this parameter defines the time unit for the resampling the tracking data to. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_res – this parameter is a numerical value which defines the resolution which the data is resampled to. NOTE: if sub-sampling to an interval greater than 60 minutes (e.g. 2 hours) then change the unit to “hours” and resolution to 2. Do not use 120 “mins”.\n\n\n\n\n\nunits_df_datetime – this parameter specifies the units of the time difference column that will be created in the data. Due to earlier filtering the duration between successive locations is recalculated again in this script.\nthreshold_time – this parameter defines the threshold time value above which gaps in the data are split and labelled as separate segments. Therefore if the difftime column in the data set exceeds this duration then the track will be split at that point.\nthreshold_points – this parameter is a numerical value which defines the minimum number of data points required for a valid segment. If a segment is below this threshold then all the locations contained within it are removed from the data set. These small segments are often not of use for further analysis that requires segmented data with consistent sampling intervals, e.g. hidden markov models."
  },
  {
    "objectID": "Glossary.html#read-in-data-files",
    "href": "Glossary.html#read-in-data-files",
    "title": "Glossary",
    "section": "",
    "text": "species_code – Throughout the workflow we use this parameter as a file/folder identifier for saving outputs. We suggest use a species or study site abbreviation. I.e. the data set for immature red-footed boobies became “RFB_IMM”.\nfilepath – This object specifies the location where the raw data files are stored. We use the here function to define the filepath for universality and to avoid issues when switching between operating systems. Ideally in this folder only the RAW tracking data files should be stored\nfilepattern – This specifies the common file pattern for the raw data files with the folder that is specified using the filepath user input parameter. If only the RAW data files are in this folder then the user can just use the file type as the common file pattern e.g. “.txt” or “.csv”. The asterisk “*” is used to match any preceding characters in the filename.\nIDstart; IDend – When setting up the data structure, we recommend saving a file for each individual animal with a unique identifier in the filename. The IDstart and IDend parameters represent numeric values denoting the start and end position of the unique individual identifier within the filenames. NB: the individual identifier should therefore be in the same location within the file name across all files.\nskiplines – this parameter denotes the number of lines at the top of the file to skip when it is read into R. The default value for this parameter is 0 so no lines are skipped. If for example you have a text file with additional info at the top that does not need to be read into R then you can increase this to the appropriate number of rows.\ndate_formats; datetime_formats – These parameters are used to specify the date format and datetime formats of the tracking data respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\ntrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones. The user should know what timezone there data was recorded in.\ncolnames – If column names are not present within the raw tracking data files, this parameter can be used to specify column names as a vector, e.g. c(“Date”, “Time”, “Lat”, “Long”). The default value is TRUE, which assumes column names are specified within the raw files.\nuser_delim - set the eliminator for your RAW data files, this will depend on the file type that your data are saved as. Text files are often tab delaminated (“) and .csv are comma separated (“,”). Other common eliminators also include “;”, “/”, “” or “|”.\nuser_trim_ws - Specify as either TRUE or FALSE. Allows white space at the bottom of files to be trimmed, e.g. empty cells at the bottom of an excel document.\nID_type – data needs a unique individual identifier to differentiate between individuals. This user input parameter allows you to specify the column name that the individual identifier will be stored in. The individual identifier will be taken from the file name using the IDstart and IDend input parameters.\ndatetime_colnames – this parameter defines the column(s) in which date and time information are stored. If date and time are stored in separate columns then specify two columns, e.g. c(“Date”, “Time”). NB: these have to be in the same order as they are stored in the data set."
  },
  {
    "objectID": "Glossary.html#merge-with-metadata",
    "href": "Glossary.html#merge-with-metadata",
    "title": "Glossary",
    "section": "",
    "text": "filepath_meta – this parameter defines the filepath for the metadata file. Again this parameter is specified using the here function for universality.\nmetadate_formats; metadatetime_formats – These parameters are used to specify the date format and datetime formats of the metadata respectively. These are specified in lubridate format, see here for a guide on how to specify date times in this format https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf\nmetatrackingdatatimezone – this parameter is used to specify the timezone which the tracking data is collected in e.g. “GMT”. In R run the function OlsonNames() to get a full list of time zones."
  },
  {
    "objectID": "Glossary.html#cleaning",
    "href": "Glossary.html#cleaning",
    "title": "Glossary",
    "section": "",
    "text": "No_data_vals – this parameter is used to specify any values within the location (X, Y) columns that represent missing data. This is often tag-specific with common examples being “0” or “-999” but these may not always be applicable.\nna_cols – this parameter is used to define a vector of column names which cannot contain NA values in order for a row to be retained. We suggest that columns containing information relating to X location, Y location, datetime and ID should not be allowed to contain NA values."
  },
  {
    "objectID": "Glossary.html#processing",
    "href": "Glossary.html#processing",
    "title": "Glossary",
    "section": "",
    "text": "tracking_crs; meta_crs – These parameters are used to specify the co-ordinate reference systems for the tracking and metadata respectively using EPSG codes. The default is lat/long which is EPSG code 4326. For a full list of EPSG cdoes see here: https://spatialreference.org/ref/epsg/\ntransform_crs - Specify metric co-ordinate projection system to transform the data into for distance calculations with units in metres. WE STRONGLY RECCOMEND THAT YOU CHANGE THIS FOR YOUR STUDY SYSTEM/LOCATION. For advice on what to change this to see our FAQ document as the choice is nuanced and can have important implications on the metric calculated."
  },
  {
    "objectID": "Glossary.html#save-df_diagnostic",
    "href": "Glossary.html#save-df_diagnostic",
    "title": "Glossary",
    "section": "",
    "text": "filepath_dfout – this parameter is used to define the file path to save out the processed tracking data file again using the here function for universality. The data is read out at this point so that it can be read into the shiny app.\nfilename_dfout – this parameter specifies the filename of the saved tracking data file."
  },
  {
    "objectID": "Glossary.html#filtering",
    "href": "Glossary.html#filtering",
    "title": "Glossary",
    "section": "",
    "text": "filter_cutoff – users may want to remove a section of the tracking data immediately following tag deployment while the animal recovers from the tagging process and adjusts to the device. This parameter is used to specify the length of this period along with the units of time – e.g. minutes, hours, days.\nfilter_speed – this parameter is used to define a numeric value for the speed above which locations are classified as erroneous points and removed based on unrealistic values. The units are in metres per second as long as transform_crs was set to a projection that is in metres. The user should use ecological knowledge of their study system to set this parameter and should check the effect it has using summaries and visualisations.\nFilter_nestdisp_dist; filter_netdist_units – these parameters are used to define a threshold value for net squared displacement above which locations are considered as erroneous and select the distance units for this value e.g. metres/kilometers."
  },
  {
    "objectID": "Glossary.html#summarise-cleaned-filtered-tracking-data",
    "href": "Glossary.html#summarise-cleaned-filtered-tracking-data",
    "title": "Glossary",
    "section": "",
    "text": "sampleRateUnits – this parameter is used to define the units of time for the sampling rate calculating which is displayed in the summary table e.g. minutes/hours. Can be specified as one of the following: “secs”, “mins”, “hours”, “days” or “weeks”.\ngrouping_factors_poplevel – this parameter is used to define the grouping factors for the population-level summary table e.g. species/population/colony. This parameter can be made into a vector to include additional grouping factors such as age or sex.\ngrouping_factors_indlevel - this parameter is used to define the grouping factors for the individual-level summary table, usually individual ID. This parameter can be made into a vector to include additional grouping factors such as temporal window (e.g. month)."
  },
  {
    "objectID": "Glossary.html#save-df_filtered-and-summary-data",
    "href": "Glossary.html#save-df_filtered-and-summary-data",
    "title": "Glossary",
    "section": "",
    "text": "filepath_filtered_out - this parameter is used to define the file path to save the filtered tracking data file again using the here function for universality.\nfilepath_summary_out - this parameter is used to define the file path to save out the summary data frames again using the here function for universality.\nfilename_filtered_out; filename_summary_pop_out; filename_summary_ind_out – these parameters are used to specify the filenames for the filtered tracking data file and the population-level and individual-level summary files respectively."
  },
  {
    "objectID": "Glossary.html#visualisation",
    "href": "Glossary.html#visualisation",
    "title": "Glossary",
    "section": "",
    "text": "device – this parameter is used to specify the filetype for saving out visualization plots e.g. “jpeg”, “png” or ”tiff”\nunits – this parameter defines the units for saving out the visualization plots e.g. “mm” or “cm”.\ndpi – this parameter is a numeric value which specifies the resolution for saving plots in dpi. The minimum value is usually 300.\nOut_path – this parameter specifies the filepath for reading out plots using the here function.\ntopo_label – we plot maps of the telemetry data over a topography base map. This parameter specifies the legend label for the topography data depending on the study system e.g. “depth (m)” or ”elevation (m)”"
  },
  {
    "objectID": "Glossary.html#post-processing-optional-steps",
    "href": "Glossary.html#post-processing-optional-steps",
    "title": "Glossary",
    "section": "",
    "text": "filepath_final – this parameter specifies the filepath for reading the final data frame back into the main workflow once the any optional post-processing has been performed. This parameter is specified using the “here” function for universality. If no optional post-processing is performed this can be set to read in the filtered data file saved in section 9."
  },
  {
    "objectID": "Glossary.html#reformat-data-for-upload-to-public-databases",
    "href": "Glossary.html#reformat-data-for-upload-to-public-databases",
    "title": "Glossary",
    "section": "",
    "text": "tz_data – timezone of the final tracking data to be uploaded. In R run the function OlsonNames() to get a full list of time zones.\ngrouping_factors – a vector of grouping factors for the final reformatted tracking data. Each level will be saved as a separate file e.g. ID/Age/Sex/Species\nfilepath_dfout – the filepath for saving out the reformatted data for database upload created using the here function."
  },
  {
    "objectID": "Glossary.html#optional-processing_central-place-trips.r",
    "href": "Glossary.html#optional-processing_central-place-trips.r",
    "title": "Glossary",
    "section": "",
    "text": "NOTE: there are a number of user input parameters in this script that are repeats of the ones above. If you can’t find the user input parameter below then use the search function to find it above.\n\nthreshold_dist – this parameter is used to define a threshold buffer distance in metres from the central place to label points as “at the central place” or “away from the central place” in order to define foraging trips. The choice of this parameter is dependent on the study species. For instance, seabirds can sit on the sea some distance from the central place while not having started a foraging trip.\nthreshold_time – this parameter is used to define the minimum length of time in minutes required for an absence from the central location to be classified as a trip. The duration of each consecutive series of locations classified as “away from the central place” is calculated and those series with a duration less than threshold_time are removed.\nshapefilepath – this parameter is used to define the filepath to read in a shapefile used to define the extent of a central place or colony for the purposes of defining trips. This is optional and the central place can be specified alternatively using a single X, Y location."
  },
  {
    "objectID": "Glossary.html#optional-processing_resampling.r",
    "href": "Glossary.html#optional-processing_resampling.r",
    "title": "Glossary",
    "section": "",
    "text": "Time_unit - select the time unit to summaries and visualise sampling intervals at. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_unit – this parameter defines the time unit for the resampling the tracking data to. Options include “secs”, “mins”, “hours”, “days” or “weeks”.\nsubsampling_res – this parameter is a numerical value which defines the resolution which the data is resampled to. NOTE: if sub-sampling to an interval greater than 60 minutes (e.g. 2 hours) then change the unit to “hours” and resolution to 2. Do not use 120 “mins”."
  },
  {
    "objectID": "Glossary.html#optional-processing_segmentation.r",
    "href": "Glossary.html#optional-processing_segmentation.r",
    "title": "Glossary",
    "section": "",
    "text": "units_df_datetime – this parameter specifies the units of the time difference column that will be created in the data. Due to earlier filtering the duration between successive locations is recalculated again in this script.\nthreshold_time – this parameter defines the threshold time value above which gaps in the data are split and labelled as separate segments. Therefore if the difftime column in the data set exceeds this duration then the track will be split at that point.\nthreshold_points – this parameter is a numerical value which defines the minimum number of data points required for a valid segment. If a segment is below this threshold then all the locations contained within it are removed from the data set. These small segments are often not of use for further analysis that requires segmented data with consistent sampling intervals, e.g. hidden markov models."
  },
  {
    "objectID": "User_guide.html",
    "href": "User_guide.html",
    "title": "ExMove user guide",
    "section": "",
    "text": "This user guide can be used as a walkthrough for reading and processing tracking data files with the Workflow.R script. You can use the example datasets provided in Data, or try with your own tracking data (see Pre-flight checks for details on data requirements and structure).\nThe following diagram gives an overview of the workflow (boxes link to each section):\n\n\n\n\n%%{init:{'flowchart':{'nodeSpacing': 5, 'rankSpacing': 30}}}%%\nflowchart LR\nsubgraph ALL [\" \"]\nstyle ALL fill:none,stroke:none\n  S1[Read in data] ==&gt; S3{Merge}\n  S2[Read in metadata] ==&gt; S3 ==&gt; A(Clean) ==&gt; B(Process) ==&gt; C(Save) \n  A --&gt; A1((Shiny app)) --&gt; B\n  B ---&gt;|\"&lt;br/&gt; Summarise/&lt;br/&gt;Visualise\"|C\n  B --&gt; B1((Shiny app)) --&gt; C\n  end\n  C --&gt; D\n  C --&gt; C1((Shiny app))\n  C ==&gt; S{Analyses}\n  subgraph OPTIONAL [\"OPTIONAL\"]\n    style OPTIONAL fill:#fbfbfb, stroke:#999, stroke-width:px, stroke-dasharray:5 5\n    D(Filter) --&gt; E(Save)\n    C1 --&gt; D\n    D ---&gt;|\"&lt;br/&gt; Summarise/&lt;br/&gt;Visualise\"|E\n    end\n    E --&gt; S\n    linkStyle 0,1,2,3,4,12 stroke-width:5px %% stroke:#00dca5 (this bit is how you edit colours of lines — no way to change colour of arrow heads)\n%% NOTE: remember to update links for finalised user guide!!\n  click S1 \"#read-in-data-files\";\n  click S2 \"#merge-with-metadata\";\n  click S3 \"#merge-with-metadata\";\n  click A \"#cleaning\";\n  click B \"#processing\";\n  click C \"#save-for-shiny\";\n  click D \"#filtering\";\n  click E \"#save-data\";\n  click A1 \"#shiny-app\"\n  click B1 \"#shiny-app\"\n  click C1 \"#shiny-app\"\n%%  click ? \"#summarise-data\";\n%%  click ? \"#visualisation-i\";\n\n\nFigure 1: Diagram of workflow used for analysing movement data (thick line denotes core path of workflow)\n\n\n\n\n\nThis workflow uses the R programming language, run via the R Studio IDE\n\nAll code embraces the core principles of how to structure ‘tidy data’\n\nWe use RStudio projects and the here package to build relative filepaths that are reproducible\nRequires tidyverse, data.table, sf and here packages to be installed\nUse our example data sets in the Data folder (RFB_IMM, RFB, GWFG, TRPE) or provide your own data\n\nSome code chunks require editing by the user to match the specific dataset being used (particularly if you are using your own data), and are highlighted as below (the 🧠 indicates you will need to think about the structure and format of your data when making these edits!):\n\n\n\n\n\n\n🧠 User input required\n\n\n\n\n#--------------------#\n## USER INPUT START ##\n#--------------------#\nexample_input &lt;- \"uservalue\" # In the R code, user input sections appear like this\n#------------------#\n## USER INPUT END ##\n#------------------#"
  },
  {
    "objectID": "User_guide.html#introduction",
    "href": "User_guide.html#introduction",
    "title": "ExMove user guide",
    "section": "",
    "text": "This user guide can be used as a walkthrough for reading and processing tracking data files with the Workflow.R script. You can use the example datasets provided in Data, or try with your own tracking data (see Pre-flight checks for details on data requirements and structure).\nThe following diagram gives an overview of the workflow (boxes link to each section):\n\n\n\n\n%%{init:{'flowchart':{'nodeSpacing': 5, 'rankSpacing': 30}}}%%\nflowchart LR\nsubgraph ALL [\" \"]\nstyle ALL fill:none,stroke:none\n  S1[Read in data] ==&gt; S3{Merge}\n  S2[Read in metadata] ==&gt; S3 ==&gt; A(Clean) ==&gt; B(Process) ==&gt; C(Save) \n  A --&gt; A1((Shiny app)) --&gt; B\n  B ---&gt;|\"&lt;br/&gt; Summarise/&lt;br/&gt;Visualise\"|C\n  B --&gt; B1((Shiny app)) --&gt; C\n  end\n  C --&gt; D\n  C --&gt; C1((Shiny app))\n  C ==&gt; S{Analyses}\n  subgraph OPTIONAL [\"OPTIONAL\"]\n    style OPTIONAL fill:#fbfbfb, stroke:#999, stroke-width:px, stroke-dasharray:5 5\n    D(Filter) --&gt; E(Save)\n    C1 --&gt; D\n    D ---&gt;|\"&lt;br/&gt; Summarise/&lt;br/&gt;Visualise\"|E\n    end\n    E --&gt; S\n    linkStyle 0,1,2,3,4,12 stroke-width:5px %% stroke:#00dca5 (this bit is how you edit colours of lines — no way to change colour of arrow heads)\n%% NOTE: remember to update links for finalised user guide!!\n  click S1 \"#read-in-data-files\";\n  click S2 \"#merge-with-metadata\";\n  click S3 \"#merge-with-metadata\";\n  click A \"#cleaning\";\n  click B \"#processing\";\n  click C \"#save-for-shiny\";\n  click D \"#filtering\";\n  click E \"#save-data\";\n  click A1 \"#shiny-app\"\n  click B1 \"#shiny-app\"\n  click C1 \"#shiny-app\"\n%%  click ? \"#summarise-data\";\n%%  click ? \"#visualisation-i\";\n\n\nFigure 1: Diagram of workflow used for analysing movement data (thick line denotes core path of workflow)\n\n\n\n\n\nThis workflow uses the R programming language, run via the R Studio IDE\n\nAll code embraces the core principles of how to structure ‘tidy data’\n\nWe use RStudio projects and the here package to build relative filepaths that are reproducible\nRequires tidyverse, data.table, sf and here packages to be installed\nUse our example data sets in the Data folder (RFB_IMM, RFB, GWFG, TRPE) or provide your own data\n\nSome code chunks require editing by the user to match the specific dataset being used (particularly if you are using your own data), and are highlighted as below (the 🧠 indicates you will need to think about the structure and format of your data when making these edits!):\n\n\n\n\n\n\n🧠 User input required\n\n\n\n\n#--------------------#\n## USER INPUT START ##\n#--------------------#\nexample_input &lt;- \"uservalue\" # In the R code, user input sections appear like this\n#------------------#\n## USER INPUT END ##\n#------------------#"
  },
  {
    "objectID": "User_guide.html#pre-flight-checks",
    "href": "User_guide.html#pre-flight-checks",
    "title": "ExMove user guide",
    "section": "0. Pre-flight checks",
    "text": "0. Pre-flight checks\nHow to use this workflow:\n\nWe will inspect the data before reading it in, so there is no need to open it in another program (e.g., excel, which can corrupt dates and times)\nUser-defined parameters (see user inputs) are called within the subsequent processing steps\nWhere you see: ## ** Option ** ##, there is an alternative version of the code to fit some common alternative data formats\nThroughout, we will use some key functions to inspect the data (e.g., head for top rows, str for column types, and names for column names)\nRequired data structure:\n\nData files should all be stored in a specific place — ideally within the Data folder\nTracking data must contain a timestamp and at least one other sensor column\nData for each deployment/individual should be in a separate file\nID should be in tracking data file name, and should be the same length for all individuals\nMetadata file should be in parent directory of data files\nMetadata should contain one row per individual per deployment\nThe importance of ID:\n\nThroughout this workflow, we use ID to refer to the unique code for an individual animal\nIn certain cases, you might have additional ID columns in the metadata (e.g., DeployID),\nor read in data with a unique TagID instead of ID.\nThis code will work as long as all of the relevant info is included in the metadata\nFor more info and helpful code, see the FAQ document & troubleshooting script\nHow to troubleshoot problems if something doesn’t work with your data:\n\nRefer to the FAQ document in the GitHub page\nThis signposts to helpful resources online (e.g., spatial co-ordinate systems)\nSee the troubleshooting code scripts that we’ve written to accompany this workflow (e.g., using multiple ID columns for re-deployments of tags/individuals)\nAll functions in code chunks are automatically hyperlinked to their documentation, so feel free to explore this if you want to understand more about how this code works!\nLoad required libraries\nJust before starting we load in all the packages we will need for the workflow (also referenced in the Dependencies section).\n\nlibrary(data.table) # data manipulation\nlibrary(tidyverse) # data manipulation, date time parsing and plotting\nlibrary(lubridate) # this should be loaded in tidyverse, but adding just in case\nlibrary(sf) # spatial data handling and manipulation\nlibrary(here) # reproducible filepaths"
  },
  {
    "objectID": "User_guide.html#read-in-data-files",
    "href": "User_guide.html#read-in-data-files",
    "title": "ExMove user guide",
    "section": "1. Read in data files",
    "text": "1. Read in data files\n\n\n\n\n\n\n🧠 User input required\n\n\n\nThroughout the script, we’ll be saving files using a species code as a file/folder identifier. Let’s define this object here for consistency:\n\nspecies_code &lt;- \"RFB_IMM\"\n\nSet filepath for the folder containing raw data files (this code will try to list and open all files matching the file pattern within this folder, so it is best if this folder contains only the raw data files).\n\nfilepath &lt;- here(\"Data\", \"RFB_IMM\")  #create relative filepath using folder names\n\nDefine common file pattern to look for. An asterisk (*) is the wildcard, will will match any character except a forward-slash (e.g. *.csv will import all files that end with “.csv”).\n\nfilepattern &lt;- \"*.txt\" # data file format (e.g. we'd use \"*.csv\" to import all csv files within filepath folders)\n\nLet’s view the file names, to check that we have the files we want & find ID position (this list will include names of sub-folders).\n\nls_filenames &lt;- list.files(path = filepath, recursive = TRUE, pattern = filepattern)\nls_filenames\n\n[1] \"229275/229275a.txt\" \"229276/229276a.txt\"\n\n\nAdjust these numbers for extracting the ID number from file name using stringr (e.g. to extract GV37501 from “GV37501_201606_DG_RFB.csv”, we want characters 1-7).  NB: this approach only works if all ID’s are the same length and in the same position — see the str_sub documentation for other options.\n\nIDstart &lt;- 1 #start position of the ID in the filename \nIDend &lt;- 6 #end position of the ID in the filename\n\nNow, let’s inspect the data by reading in the top of the first data file as raw text. To inspect the first row of all data files (if you wanted to check column names), you can remove the [1] and change n_max = 1).\n\ntest &lt;- fs::dir_ls(path = filepath, recurse = TRUE, type = \"file\",  glob = filepattern)[1]\nread_lines(test, n_max = 5)  # change n_max to change the number of rows to read in\n\n[1] \"Date Time\\tFix\\tLat1(N)\\tLong1(E)\\tLat2(N)\\tLong2(E)\\tMsgCount\\tFrequency\\tAverage TI\\tSatellite\\tMax Str\\tSwapped\"\n[2] \"2022-02-07 12:51:16\\tZ\\t 63.903\\t -61.318\\t 63.903\\t -61.318\\t  4\\t697187.5\\t130.0\\t1\\t-121\\t\"                     \n[3] \"2022-02-07 12:55:50\\t3\\t -7.259\\t  72.358\\t -7.818\\t  74.874\\t  5\\t671259.7\\t 65.0\\tR\\t-123\\t1\"                    \n[4] \"2022-02-08 04:09:32\\tA\\t -7.259\\t  72.370\\t -7.259\\t  72.370\\t  3\\t671174.9\\t 67.7\\tC\\t-125\\t\"                     \n[5] \"2022-02-08 04:48:48\\tB\\t -7.237\\t  72.371\\t -7.237\\t  72.371\\t  2\\t671139.5\\t139.0\\tN\\t-133\\t\"                     \n\n\nDefine number of lines at top of file to skip (e.g. if importing a text file with additional info at top).\n\nskiplines &lt;- 0\n\nDefine date format(s) used (for passing to lubridate) (d = day as decimal, m = month as decimal, y = year without century - 2 digits, Y = year with century - 4 digits). lubridate can even parse more than one date/time format within a dataframe, so if your data include multiple formats, make sure they are all included. Here, we’ve included some common combinations — modify if your data include a different format\n\ndate_formats &lt;- c(\"dmY\", \"Ymd\") #specify date formats (e.g. \"dmY\" works for 01-12-2022 and 01/12/2022)\ndatetime_formats &lt;- c(\"dmY HMS\", \"Ymd HMS\") #specify date & time format \n\nDefine time zone for tracking data.\n\ntrackingdatatimezone &lt;- \"GMT\"\n\nBy default, the below code will find column names from the first row of data. If you want to specify your own column names, do so here as a character vector, or use set colnames &lt;- FALSE to automatically number columns.\n\ncolnames &lt;- TRUE\n\nHere, we use the function read_delim and specify the delimiter to make this code more universal (you can find extra information on this in the readr documentation).\nSome delimiter examples:\n\n\n\",\" = comma delimited (equivalent to using read_csv – saved as extension .csv)\n\n\"\\t\" = tab delimited (equivalent to using read_tsv — saved as extension .tsv)\n\n\" \" = whitespace delimited (equivalent to using read_table)\n\nLet’s inspect the data again, this time skipping rows if set, to check the file delimiter.\n\nread_lines(test, n_max = 5, skip = skiplines)\n\n[1] \"Date Time\\tFix\\tLat1(N)\\tLong1(E)\\tLat2(N)\\tLong2(E)\\tMsgCount\\tFrequency\\tAverage TI\\tSatellite\\tMax Str\\tSwapped\"\n[2] \"2022-02-07 12:51:16\\tZ\\t 63.903\\t -61.318\\t 63.903\\t -61.318\\t  4\\t697187.5\\t130.0\\t1\\t-121\\t\"                     \n[3] \"2022-02-07 12:55:50\\t3\\t -7.259\\t  72.358\\t -7.818\\t  74.874\\t  5\\t671259.7\\t 65.0\\tR\\t-123\\t1\"                    \n[4] \"2022-02-08 04:09:32\\tA\\t -7.259\\t  72.370\\t -7.259\\t  72.370\\t  3\\t671174.9\\t 67.7\\tC\\t-125\\t\"                     \n[5] \"2022-02-08 04:48:48\\tB\\t -7.237\\t  72.371\\t -7.237\\t  72.371\\t  2\\t671139.5\\t139.0\\tN\\t-133\\t\"                     \n\n\nSet delimiter to use within read_delim.\n\nuser_delim &lt;- \"\\t\"\nuser_trim_ws &lt;- TRUE # Should leading/trailing whitespaces be trimmed from each field before parsing?\n\nFinally, data need an ID column, either be the tag ID (“TagID”) or individual ID (“ID”). Specify ID type here, for later matching with the same column in the metadata:\n\nID_type &lt;- \"TagID\"\n\n\n\nRead in and merge all tracking data files\n\n\nMerge using ID in filename\nOption: Merge using ID already in column\n\n\n\nWith the user inputs specified in the previous section, we’ll now read in and merge all tracking data files directly from the github repository, extracting the ID from the filename of each file.\n\ndf_combined &lt;- fs::dir_ls(path = filepath, glob = filepattern, #use our defined filepath and pattern\n                          type = \"file\",  recurse = TRUE) %&gt;% # recurse = T searches all sub-folders\n  purrr::set_names(nm = basename(.)) %&gt;% # removing path prefix (makes filename more manageable)\n  purrr::map_dfr(read_delim, .id=\"filename\", #read all the files in using filename as ID column\n                 col_types = cols(.default = \"c\"), col_names = colnames, \n                 skip = skiplines, delim = user_delim, trim_ws = user_trim_ws) %&gt;% \n  mutate(\"{ID_type}\" := str_sub(string = filename, start = IDstart, end = IDend), #substring ID from the filename (start to end of substring)\n         .after = filename) #position the new ID column after filename column\ndf_combined\n\n# A tibble: 326 × 14\n   filename  TagID Date …¹ Fix   Lat1(…² Long1…³ Lat2(…⁴ Long2…⁵ MsgCo…⁶ Frequ…⁷\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1 229275a.… 2292… 2022-0… Z     63.903  -61.318 63.903  -61.318 4       697187…\n 2 229275a.… 2292… 2022-0… 3     -7.259  72.358  -7.818  74.874  5       671259…\n 3 229275a.… 2292… 2022-0… A     -7.259  72.370  -7.259  72.370  3       671174…\n 4 229275a.… 2292… 2022-0… B     -7.237  72.371  -7.237  72.371  2       671139…\n 5 229275a.… 2292… 2022-0… B     -7.142  72.471  -7.142  72.471  1       671257…\n 6 229275a.… 2292… 2022-0… B     -7.144  72.486  -7.144  72.486  2       671155…\n 7 229275a.… 2292… 2022-0… B     -7.140  72.492  -7.140  72.492  1       671120…\n 8 229275a.… 2292… 2022-0… B     -7.343  72.432  -7.343  72.432  2       671136…\n 9 229275a.… 2292… 2022-0… B     -7.325  72.421  -7.325  72.421  1       671127…\n10 229275a.… 2292… 2022-0… A     -7.255  72.470  -7.255  72.470  3       671237…\n# … with 316 more rows, 4 more variables: `Average TI` &lt;chr&gt;, Satellite &lt;chr&gt;,\n#   `Max Str` &lt;chr&gt;, Swapped &lt;chr&gt;, and abbreviated variable names\n#   ¹​`Date Time`, ²​`Lat1(N)`, ³​`Long1(E)`, ⁴​`Lat2(N)`, ⁵​`Long2(E)`, ⁶​MsgCount,\n#   ⁷​Frequency\n\ncolnames(df_combined)\n\n [1] \"filename\"   \"TagID\"      \"Date Time\"  \"Fix\"        \"Lat1(N)\"   \n [6] \"Long1(E)\"   \"Lat2(N)\"    \"Long2(E)\"   \"MsgCount\"   \"Frequency\" \n[11] \"Average TI\" \"Satellite\"  \"Max Str\"    \"Swapped\"   \n\n\n\n\nIf your data are combined into one or multiple csv files containing an ID column, use the following approach instead (this is the same code, but doesn’t create a new ID column from the file name):\n\n# ** Option **\ndf_combined &lt;- fs::dir_ls(path = filepath, recurse = TRUE, type = \"file\",  glob = filepattern) %&gt;% # recurse = T searches all sub-folders\n  purrr::map_dfr(read_delim, col_types = cols(.default = \"c\"), col_names = colnames, \n                 skip = skiplines, delim = user_delim, trim_ws = user_trim_ws) \ndf_combined\n\n\n\n\nSlim down dataset\n\n\nSelect normal columns\nOption: Select custom columns\n\n\n\n\n\n\n\n\n\n🧠 User input required\n\n\n\nFirst, data need a time stamp, either in separate columns (e.g., “Date” and “Time”) or combined (“DateTime”). Below we specify which column’s date and time info are stored in the data.  NB: These have to be in the same order as specified in earlier user input, i.e. “Date” and “Time” have to be the right way round\n\ndatetime_formats # a reminder of the datetime orders previously specified\n\n[1] \"dmY HMS\" \"Ymd HMS\"\n\ndatetime_colnames &lt;- c(\"DateTime\") # or c(\"Date\", \"Time\")\n\nYou can also have additional columns depending on the type of logger used, for example:\n\n## lc = Argos fix quality\n## Lat2/Lon2 = additional location fixes from Argos tag\n## laterr/lonerr = location error information provided by some GLS processing packages\n\nHere we’re going to slim down the dataset by selecting the necessary columns & coercing some column names. You should change column names below to those present in your tracking data, additional columns can be added (see above examples). This process standardises important column names for the rest of the workflow (e.g., TagID, Lat, Lon)\n\ndf_slim &lt;- data.frame(TagID = as.character(df_combined$TagID),\n                      DateTime = df_combined$`Date Time`,\n                      lc = df_combined$Fix, # Argos fix quality\n                      Lat = df_combined$`Lat1(N)`,\n                      Lon = df_combined$`Long1(E)`,\n                      Lat2 = df_combined$`Lat2(N)`,\n                      Lon2 = df_combined$`Long2(E)`)\n\n\n\n\n\n\n\n\n\n\n\n🧠 User input required\n\n\n\nHere’s an example of how to change the above code for data with different columns and column names. This code works with immersion data recorded by a GLS logger (no location data)\n\ndf_slim &lt;- data.frame(ID = df_combined$ID,\n                      Date = df_combined$`DD/MM/YYYY`,\n                      Time = df_combined$`HH:MM:SS`,\n                      Immersion = df_combined$`wets0-20`)\n\n\n\n\n\n\nParse dates, create datetime, date and year columns\nNow our df_slim is ready, we need to create a DateTime column. Using the datetime_colnames object we made previously, we’ll combine columns (if needed), and then parse a single DateTime column using the lubridate package:\n\ndf_slim &lt;- df_slim %&gt;%\n  tidyr::unite(col = \"DateTime_unparsed\", all_of(datetime_colnames), sep = \" \", remove = FALSE) %&gt;% \n  mutate(DateTime = lubridate::parse_date_time(DateTime_unparsed, #use lubridate to parse DateTime \n                                               orders=datetime_formats, #using the datetime_formats object we made earlier\n                                               tz=trackingdatatimezone),\n         Date = as_date(DateTime),\n         Year = year(DateTime)) %&gt;%\n  select(-DateTime_unparsed)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you see any failed to parse warnings, this means a date or time was not in the correct format for lubridate to create a date_time object, producing NA’s (which will be dealt with later). We can look at the failing rows using the following code:\n\nFails &lt;- df_slim %&gt;% filter(is.na(DateTime)==T)\nhead(Fails)\n\n[1] TagID    DateTime lc       Lat      Lon      Lat2     Lon2     Date    \n[9] Year    \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\nLastly, we make a df_raw dataframe by sorting using ID and DateTime, dropping NA’s in DateTime column\n\ndf_raw &lt;- df_slim %&gt;% \n  arrange(across(all_of(c(ID_type, \"DateTime\")))) %&gt;%\n  drop_na(DateTime) #remove NA's in datetime column\nhead(df_raw)\n\n   TagID            DateTime lc    Lat     Lon   Lat2    Lon2       Date Year\n1 229275 2022-02-07 12:51:16  Z 63.903 -61.318 63.903 -61.318 2022-02-07 2022\n2 229275 2022-02-07 12:55:50  3 -7.259  72.358 -7.818  74.874 2022-02-07 2022\n3 229275 2022-02-08 04:09:32  A -7.259  72.370 -7.259  72.370 2022-02-08 2022\n4 229275 2022-02-08 04:48:48  B -7.237  72.371 -7.237  72.371 2022-02-08 2022\n5 229275 2022-02-08 04:58:02  B -7.142  72.471 -7.142  72.471 2022-02-08 2022\n6 229275 2022-02-08 05:49:35  B -7.144  72.486 -7.144  72.486 2022-02-08 2022\n\n\nWe can clean up intermediate files/objects by listing everything we want to keep (i.e. remove everything else)\n\nrm(list=ls()[!ls() %in% c(\"df_raw\",\n                          \"date_formats\",\"datetime_formats\",\"trackingdatatimezone\", \n                          \"ID_type\", \"species_code\")])"
  },
  {
    "objectID": "User_guide.html#merge-with-metadata",
    "href": "User_guide.html#merge-with-metadata",
    "title": "ExMove user guide",
    "section": "2. Merge with metadata",
    "text": "2. Merge with metadata\nMetadata are an essential piece of information for any tracking study, as they contain important information about each of your data files, such as tag ID, animal ID, or deployment information, that we can add back into to our raw data when needed. For example, the table below shows what the first few columns of a metadata file looks like for our example immature red-footed booby data:\n\n\n\n\n\n\nbird_id\ngps_tag\ncapture_date\ncapture_time\nlat\nlong\nage\nTag_ID\nDeploy_ID\n\n\n\nGY02215\nArgos PTT\n10/02/2022\n11:18:00\n-7.250352\n72.45164\nImmature Basic 2\n229276\n1\n\n\nGY02216\nArgos PTT\n10/02/2022\n12:07:00\n-7.247698\n72.45145\nImmature Basic 1\n229275\n1\n\n\n\n\n\n\n\n\n\n\n\n\n🧠 User input required\n\n\n\nFirst we define the path to our metadata file:\n\nfilepath_meta &lt;- here(\"Data\",\"RFB_IMM_Metadata.csv\")\n\nThen much like in Step 1, we define the date format(s) used (for passing to lubridate) (d = day as decimal, m = month as decimal, y = year without century - 2 digits, Y = year with century - 4 digits). Here, we’ve included common combinations, which you’ll need to modify if your metadata include a different format (run OlsonNames() to return a full list of time zones names).\n\nmetadate_formats &lt;- c(\"dmY\", \"Ymd\") #specify date format used in metadata\nmetadatetime_formats &lt;- c(\"dmY HMS\", \"Ymd HMS\") #specify date & time format\nmetadatatimezone &lt;- \"Indian/Chagos\" #specify timezone used for metadata\n\nNext we read in the metadata file (make sure to check the read_ function you’re using matches your ddata format!).\n\ndf_metadata &lt;- readr::read_csv(filepath_meta) # Read in metadata file\nnames(df_metadata)\n\nThen we select necessary comments & coerce column names, making sure to provide four compulsory columns: ID — as defined in tracking data (individual ID or TagID), deployment date & deployment time. We can also provide optional columns depending on sensor type: e.g. colony, sex, age, central place (CP) Lat, CP Lon. Depending on whether you know the CP for each individual, you can use one of the two following approaches, and delete columns where appropriate.  NB: Different tags types sometimes require specific approaches for dealing with data collected outside of deployment period (e.g., before deployment or after retrieval). If data need to be filtered for one or both of these scenarios, we need to sort out these columns in the metadata, and if not relevant for the data, set column name to “NA”.\n\n\nSelect metadata columns\nOption: Individual CP unknown\n\n\n\nIf you have multiple ID columns like TagID/DeployID, include them here (for example, if one individual was tracked over multiple deployments/years, or if one tag was re-deployed on multiple individuals). For more information and helpful code, see the FAQ document and troubleshooting script.\n\ndf_metadataslim &lt;- data.frame(ID = as.character(df_metadata$bird_id), # compulsory column\n                              TagID = as.character(df_metadata$Tag_ID),\n                              DeployID = as.character(df_metadata$Deploy_ID),\n                              DeployDate_local = df_metadata$capture_date, # compulsory column (set to NA if not relevant)\n                              DeployTime_local = df_metadata$capture_time, # compulsory column (set to NA if not relevant)\n                              RetrieveDate_local = NA, # compulsory column (set to NA if not relevant)\n                              RetrieveTime_local = NA, # compulsory column (set to NA if not relevant)\n                              DeployLat = df_metadata$lat,\n                              DeployLon = df_metadata$long,\n                              Species = \"RFB\",\n                              Age = df_metadata$age)\n\n\n\nIf central place for each individual is not known, you can add population-level central places here by first creating a custom dataframe of population CPs:\n\n## ** Option ** ##\ndf_PopCPs &lt;- tribble(\n  ~Population,  ~CPLat,    ~CPLon,\n  \"DG\",         -7.24,     72.43,\n  \"NI\",         -5.68,     71.24\n  )\n\nThen we can merge df_metadataslim and df_PopCPs. The Population column must be present in both dataframes (this codes uses the %&lt;&gt;% pipe, so will overwrite CPLat and CPLon if these are present in df_metadataslim).\n\ndf_metadataslim  %&lt;&gt;%\n  select(matches(setdiff(names(df_metadataslim), c(\"CPLat\", \"CPLon\")))) %&gt;%\n  left_join(., df_PopCPs, by = \"Population\")\n\n\n\n\n\n\nFormat all dates and times, combine them and specify timezone (NA’s in deployment/retrieval date times will throw warnings, but these are safe to ignore if you know there are NA’s in these columns).\n\ndf_metadataslim &lt;- df_metadataslim %&gt;%\n  mutate(Deploydatetime = parse_date_time(paste(DeployDate_local, DeployTime_local),#create deploy datetime\n                                          order=metadatetime_formats, \n                                          tz=metadatatimezone),\n         Retrievedatetime = parse_date_time(paste(RetrieveDate_local, RetrieveTime_local), #create retrieve datetime\n                                            order=metadatetime_formats,\n                                            tz=metadatatimezone)) %&gt;%\n  select(-any_of(c(\"DeployDate_local\",\"DeployTime_local\", \"RetrieveDate_local\", \"RetrieveTime_local\"))) %&gt;%\n  mutate(across(contains('datetime'), #return datetime as it would appear in a different tz\n                ~with_tz(., tzone=trackingdatatimezone)))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Retrievedatetime = parse_date_time(...)`.\nCaused by warning:\n! All formats failed to parse. No formats found.\n\n\nHere we’ll create a dataframe of temporal extents of our data to use in absence of deploy/retrieve times (this is also useful for basic data checks and for writing up methods).\n\ndf_temporalextents &lt;- df_raw %&gt;%\n  group_by(across(all_of(ID_type))) %&gt;%\n  summarise(min_datetime = min(DateTime),\n            max_datetime = max(DateTime))\n\nThen we use these temporal extents of our data to fill in any NA’s in the deploy/retrieve times.\n\ndf_metadataslim &lt;- df_metadataslim %&gt;%\n  left_join(., df_temporalextents, by = ID_type) %&gt;%\n  mutate(Deploydatetime = case_when(!is.na(Deploydatetime) ~ Deploydatetime,\n                                      is.na(Deploydatetime) ~ min_datetime),\n         Retrievedatetime = case_when(!is.na(Retrievedatetime) ~ Retrievedatetime,\n                                      is.na(Retrievedatetime) ~ max_datetime)) %&gt;%\n  select(-c(min_datetime, max_datetime))\n\nNext we merge metadata with raw data using the ID column.\n\ndf_metamerged &lt;- df_raw %&gt;%\n  left_join(., df_metadataslim, by=ID_type) \n\nFinally, we can remove intermediate files/objects by specifying objects to keep.\n\nrm(list=ls()[!ls() %in% c(\"df_metamerged\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#cleaning",
    "href": "User_guide.html#cleaning",
    "title": "ExMove user guide",
    "section": "3. Cleaning",
    "text": "3. Cleaning\n\n\n\n\n\n\n🧠 User input required\n\n\n\nDefine your own no/empty/erroneous data values in Lat and Lon columns (e.g. “bad” values specified by the tag manufacturer).\n\nNo_data_vals &lt;- c(0, -999)\n\nDefine a vector of columns which can’t have NAs (if there are NAs in one of these columns the problematic row will be removed).\n\nna_cols &lt;- c(\"Lat\", \"Lon\", \"DateTime\", \"ID\") #column to check for na's\n\n\n\nNow we pipe the data through a series of functions to drop NAs in specified columns, filter out user-defined no_data_values in Lat Lon columns, remove duplicates, remove undeployed locations and filter out locations within temporal cut-off following deployment.\n\ndf_clean &lt;- df_metamerged %&gt;%\n  drop_na(all_of(na_cols)) %&gt;% \n  filter(!Lat %in% No_data_vals & !Lon %in% No_data_vals) %&gt;% # remove bad data values in Lat Lon columns\n  distinct(DateTime, ID, .keep_all = TRUE) %&gt;% # this might be a problem for ACC data missing milliseconds, so beware if using it for this purpose!\n  filter(case_when(!is.na(Retrievedatetime) ~ Deploydatetime &lt; DateTime & DateTime &lt; Retrievedatetime, # keep deployment period, only\n                   .default = Deploydatetime &lt; DateTime)) # if retrieve date is NA (i.e., tags submit via satellite), only filter by deploy date\nhead(df_clean)\n\n   TagID            DateTime lc    Lat    Lon   Lat2   Lon2       Date Year\n1 229275 2022-02-10 12:59:50  A -7.255 72.470 -7.255 72.470 2022-02-10 2022\n2 229275 2022-02-10 14:42:30  B -7.261 72.476 -7.261 72.476 2022-02-10 2022\n3 229275 2022-02-10 15:00:42  3 -7.249 72.448 -7.249 72.448 2022-02-10 2022\n4 229275 2022-02-10 15:15:24  3 -7.250 72.452 -7.250 72.452 2022-02-10 2022\n5 229275 2022-02-10 16:03:10  B -7.252 72.458 -7.252 72.458 2022-02-10 2022\n6 229275 2022-02-10 16:51:55  B -7.250 72.458 -7.250 72.458 2022-02-10 2022\n       ID DeployID DeployLat DeployLon Species              Age\n1 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n2 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n3 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n4 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n5 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n6 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n       Deploydatetime    Retrievedatetime\n1 2022-02-10 06:07:00 2022-02-23 14:51:37\n2 2022-02-10 06:07:00 2022-02-23 14:51:37\n3 2022-02-10 06:07:00 2022-02-23 14:51:37\n4 2022-02-10 06:07:00 2022-02-23 14:51:37\n5 2022-02-10 06:07:00 2022-02-23 14:51:37\n6 2022-02-10 06:07:00 2022-02-23 14:51:37\n\n\n\n\n\n\n\n\nOption: Filter by fix quality\n\n\n\n\n\nArgos fix quality can be used to filter the data set to remove locations with too much uncertainty. If you know the error classes that you want to retain in this dataset, you can run this filter below.  NB: If you want to do further exploration of location quality (e.g., from GPS PTT tags to compare locations with contemporaneous GPS locations), keep all location classes by skipping this step.\nIn this code we define a vector of location classes to keep (typically, location classes 1, 2, and 3 are of sufficient certainty), and filter out everything else.\n\nlc_keep &lt;- c(\"1\", \"2\", \"3\")\n\ndf_clean &lt;- df_clean %&gt;%\nfilter(lc %in% lc_keep) # filter data to only retain location classes in lc_keep\nhead(df_clean)\n\n   TagID            DateTime lc    Lat    Lon   Lat2   Lon2       Date Year\n1 229275 2022-02-10 15:00:42  3 -7.249 72.448 -7.249 72.448 2022-02-10 2022\n2 229275 2022-02-10 15:15:24  3 -7.250 72.452 -7.250 72.452 2022-02-10 2022\n3 229275 2022-02-10 17:00:47  3 -7.251 72.449 -7.251 72.449 2022-02-10 2022\n4 229275 2022-02-11 13:24:11  3 -7.638 74.659 -7.638 74.659 2022-02-11 2022\n5 229275 2022-02-11 14:06:46  1 -7.653 74.630 -7.653 74.630 2022-02-11 2022\n6 229275 2022-02-11 14:52:05  1 -7.677 74.638 -7.677 74.638 2022-02-11 2022\n       ID DeployID DeployLat DeployLon Species              Age\n1 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n2 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n3 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n4 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n5 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n6 GY02216        1 -7.247698  72.45145     RFB Immature Basic 1\n       Deploydatetime    Retrievedatetime\n1 2022-02-10 06:07:00 2022-02-23 14:51:37\n2 2022-02-10 06:07:00 2022-02-23 14:51:37\n3 2022-02-10 06:07:00 2022-02-23 14:51:37\n4 2022-02-10 06:07:00 2022-02-23 14:51:37\n5 2022-02-10 06:07:00 2022-02-23 14:51:37\n6 2022-02-10 06:07:00 2022-02-23 14:51:37\n\n\n\n\n\nFinally we remove intermediate files/objects\n\nrm(list=ls()[!ls() %in% c(\"df_clean\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#processing",
    "href": "User_guide.html#processing",
    "title": "ExMove user guide",
    "section": "4. Processing",
    "text": "4. Processing\nSome useful temporal and spatial calculations on the data\n\n\n\n\n\n\n🧠 User input required\n\n\n\nFirst we need to specify the co-ordinate projection systems for the tracking data and meta data. The default here is lon/lat for both tracking data & metadata, for which the EPSG code is 4326. For more information see the CRS section of the FAQ’s or have a look at the ESPG.io database.\n\ntracking_crs &lt;- 4326 # Only change if data are in a different coordinate system\nmeta_crs &lt;- 4326 # Only change if data are in a different coordinate system\n\n\n\nNext we transform coordinates of data, and perform spatial calculations. This requires spatial analysis, and so it is good practice to run all spatial analyses in a coordinate reference system that uses metres as a unit.\nAs an example, we will use Spherical Mercator projection — aka “WGS” (crs = 3857). It’s important to consider the location and scale of your data (e.g., equatorial/polar/local scale/global scale) when choosing a projection system. Other options include (but are not limited to) UTM, and Lambert azimuthal equal-area (LAEA).\nHere we’ll calculate bearings relative to first location.\n\ndf_diagnostic &lt;-  df_clean %&gt;%\n  ungroup() %&gt;% #need to ungroup to extract geometry of the whole dataset\n  mutate(geometry_GPS = st_transform( # assign geometry and transform to WGS for distance calculations\n      st_as_sf(., coords=c(\"Lon\",\"Lat\"), crs=tracking_crs), crs = 3857)$geometry,\n    geometry_first = st_transform(\n      st_as_sf(slice(., 1), coords=c(\"Lon\",\"Lat\"), crs=tracking_crs), crs = 3857)$geometry) %&gt;%\n  group_by(ID) %&gt;% #back to grouping by ID for calculations per individual\n  mutate(dist = st_distance(geometry_GPS, lag(geometry_GPS), by_element = T), # distance travelled from previous fix\n         difftime = difftime(DateTime, lag(DateTime), units=\"secs\"),          # time passed since previous fix\n         netdisp = st_distance(geometry_GPS, geometry_first, by_element = T), # calculate distance between first location and current location\n         speed = as.numeric(dist)/as.numeric(difftime),                       # calculate speed (distance/time)\n         dLon = as.numeric(Lon)-lag(as.numeric(Lon)), #difference in longitude, relative to previous location\n         dLat = as.numeric(Lat)-lag(as.numeric(Lat)), #difference in longitude, relative to previous location\n         turnangle = atan2(dLon, dLat)*180/pi + (dLon &lt; 0)*360) %&gt;% #angle (in degrees) from previous to current location using formula theta = atan(y/x), where y = change along y axis & x = change along x axis\n  ungroup() %&gt;% select(-c(geometry_GPS, dLon, dLat)) # ungroup and remove geometries\n\n\n\n\n\n\n\nOption: CP data included\n\n\n\n\n\nIf your data include a Central Place location, you can also run the following code to calculate bearings relative to CP\n\n## ** Option ** ##\ndf_diagnostic &lt;-  df_diagnostic %&gt;%\n  ungroup() %&gt;% #need to ungroup to extract geometry of the whole dataset\n  mutate(geometry_GPS = st_transform( #assign geometry and transform to WGS for dist calcs\n    st_as_sf(., coords=c(\"Lon\",\"Lat\"), crs=tracking_crs), crs = 3857)$geometry,\n    geometry_CP = st_transform( #assign geometry and transform to WGS for dist calcs\n      st_as_sf(., coords=c(\"CPLon\",\"CPLat\"), crs=meta_crs), crs = 3857)$geometry) %&gt;%\n  group_by(ID) %&gt;% #back to grouping by ID for calculations per individual\n  mutate(CPdist = st_distance(geometry_GPS, geometry_CP, by_element = T), #calculate distance between central place and current location\n         dLon_CP = as.numeric(Lon)-CPLon, #difference in longitude between current location and central place\n         dLat_CP = as.numeric(Lat)-CPLat, #difference in longitude between current location and central place\n         CPbearing = atan2(dLon_CP, dLat_CP)*180/pi + (dLon_CP &lt; 0)*360) %&gt;% #bearing (in degrees) from central place to current location using formula theta = atan(y/x), where y = change along y axis from CP & x = change along x axis from CP\n  ungroup() %&gt;% select(-c(geometry_GPS, geometry_CP, dLon, dLat, dLon_CP, dLat_CP)) #ungroup and remove geometries"
  },
  {
    "objectID": "User_guide.html#save-for-shiny",
    "href": "User_guide.html#save-for-shiny",
    "title": "ExMove user guide",
    "section": "5. Save for Shiny",
    "text": "5. Save for Shiny\nHere we’re going to save df_diagnostic to use in the Shiny app provided. The app is designed to explore how further filtering and processing steps affect the data.\n\n\n\n\n\n\n🧠 User input required\n\n\n\nFirst, we use here to create a file path for saving the working dataframe files, and create the folder if missing\n\nfilepath_dfout &lt;- here(\"DataOutputs\",\"WorkingDataFrames\") # create filepath\ndir.create(filepath_dfout) # create folder if it doesn't exist\n\nNext we define file name for the saved file by pasting the species code before _diagnostic (can change this if you want to use a different naming system).\n\nfilename_dfout &lt;- paste0(species_code, \"_diagnostic\")\n\nIf not added from the metadata, add a species column and any other columns here relevant to your data (optional)\n\n## ** Option ** ##\ndf_diagnostic$Species &lt;- species_code\n\n\n\nFinally we save the df_diagnostic as a csv file using the variables created above.\n\nwrite_csv(df_diagnostic, file = here(filepath_dfout, paste0(filename_dfout,\".csv\")))\n\nRemove everything except df_diagnostic ahead of the next step.\n\nrm(list=ls()[!ls() %in% c(\"df_diagnostic\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#filtering",
    "href": "User_guide.html#filtering",
    "title": "ExMove user guide",
    "section": "6. Filtering",
    "text": "6. Filtering\nThis second filtering stage is designed to remove outliers in the data, and you can use outputs from the Shiny app to inform these choices. If you don’t need to filter for outliers, skip this step and keep using df_diagnostic in the next steps.\nAccessing the Shiny app\nOption 1:\nAccess the Shiny app online at the following link: https://lukeozsanlav.shinyapps.io/exmove_explorer/\nOption 2:\nAlternatively run the app from your local R session with the following code\n\nif (!require(\"shiny\")) install.packages(\"shiny\")\nlibrary(shiny)\nrunGitHub(\"ExMoveApp\", username = \"LukeOzsanlav\",\n          ref = \"master\", subdir = \"app\")\n\nApp usage:\n\nUpload your csv version of df_diagnostic to the app by clicking the Upload data button in the top left.\nAt the bottom of each app page are printed code chunks that can be copied into subsequent user input section. These code chunks contain the user input values you manually select in the app\nDefine threshold values\n\n\n\n\n\n\n🧠 User input required\n\n\n\nFirst we define a period to filter after tag deployment, when all points before the cutoff will be removed (e.g. to remove potentially unnatural behaviour following the tagging event). We define this period using the as.period function, by providing an integer value and time unit (e.g. hours/days/years). This code below specifies a period of 30 minutes:\n\nfilter_cutoff &lt;- as.period(30, unit=\"minutes\") \n\nThen we define speed threshold in m/s, which we will use to remove any points with faster speeds.\n\nfilter_speed &lt;- 20\n\nNext we define a net displacement (distance from first point) threshold and specify units. Any points further away from the first tracking point will be removed (see commented code for how to retain all points):\n\nfilter_netdisp_dist &lt;- 200\nfilter_netdist_units &lt;- \"km\" # e.g., \"m\", \"km\"\n\n#If you want to retain points no matter the net displacement value, use these values instead:\n#filter_netdisp_dist &lt;- max(df_diagnostic$netdisp)\n#filter_netdist_units &lt;- \"m\"\n\n\n\nImplement filters\nCreate net displacement filter using distance and units\n\nfilter_netdisp &lt;- units::as_units(filter_netdisp_dist, filter_netdist_units)\n\nFilter df_diagnostic\n\ndf_filtered &lt;- df_diagnostic %&gt;%\n  filter(Deploydatetime + filter_cutoff &lt; DateTime, # keep times after cutoff\n         speed &lt; filter_speed, # keep speeds slower than speed filter\n         netdisp &lt;= filter_netdisp) # keep distances less than net displacement filter\nhead(df_filtered)\n\n# A tibble: 6 × 23\n  TagID DateTime            lc    Lat   Lon   Lat2  Lon2  Date        Year ID   \n  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;\n1 2292… 2022-02-10 15:15:24 3     -7.2… 72.4… -7.2… 72.4… 2022-02-10  2022 GY02…\n2 2292… 2022-02-10 17:00:47 3     -7.2… 72.4… -7.2… 72.4… 2022-02-10  2022 GY02…\n3 2292… 2022-02-13 00:34:48 1     -7.0… 73.1… -3.1… 90.6… 2022-02-13  2022 GY02…\n4 2292… 2022-02-13 01:52:12 2     -7.0… 73.0… -7.0… 73.0… 2022-02-13  2022 GY02…\n5 2292… 2022-02-13 02:07:17 1     -7.0… 72.9… -7.0… 72.9… 2022-02-13  2022 GY02…\n6 2292… 2022-02-13 03:12:58 1     -7.2… 72.8… -7.2… 72.8… 2022-02-13  2022 GY02…\n# … with 13 more variables: DeployID &lt;chr&gt;, DeployLat &lt;dbl&gt;, DeployLon &lt;dbl&gt;,\n#   Species &lt;chr&gt;, Age &lt;chr&gt;, Deploydatetime &lt;dttm&gt;, Retrievedatetime &lt;dttm&gt;,\n#   geometry_first &lt;POINT [m]&gt;, dist [m], difftime &lt;drtn&gt;, netdisp [m],\n#   speed &lt;dbl&gt;, turnangle &lt;dbl&gt;\n\n\nRemove intermediate files/objects\n\nrm(list=ls()[!ls() %in% c(\"df_filtered\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#summarise-data",
    "href": "User_guide.html#summarise-data",
    "title": "ExMove user guide",
    "section": "7. Summarise cleaned & filtered tracking data",
    "text": "7. Summarise cleaned & filtered tracking data\n\n\n\n\n\n\n🧠 User input required\n\n\n\nSet the units to display sampling rate in the summary table\n\nsampleRateUnits &lt;- \"mins\" \n\nDefine levels of grouping factors to summarise over\nFirstly, down to population level. Here, we are working on data from one population & year, and so use Species as the grouping factor. Add any other relevant grouping factors here (e.g. Country / Year / Season / Age).\n\ngrouping_factors_poplevel &lt;- c(\"Species\")\n\nSecondly, down to individual level (add DeployID for example if relevant).\n\ngrouping_factors_indlevel &lt;- c(\"ID\")\n\n\n\n\nCreate summary tables\nCreate a small function to calculate standard error.\n\nse &lt;- function(x) sqrt(var(x, na.rm = T) / length(x[!is.na(x)]))\n\nCreate a summary table of individual-level summary statistics:\n\ndf_summary_ind &lt;- df_filtered %&gt;%\n  group_by(across(c(all_of(grouping_factors_poplevel), all_of(grouping_factors_indlevel)))) %&gt;%\n  summarise(NoPoints = NROW(ID), # number of fixes\n            NoUniqueDates = length(unique(Date)), # number of tracking dates\n            FirstDate = as.Date(min(Date)), # first tracking date\n            LastDate = as.Date(max(Date)), # last tracking date\n            SampleRate = mean(as.numeric(difftime, units = sampleRateUnits), na.rm = T), # sample rate mean\n            SampleRate_se = se(as.numeric(difftime, units = sampleRateUnits))) # sample rate standard error\n\n`summarise()` has grouped output by 'Species'. You can override using the\n`.groups` argument.\n\ndf_summary_ind\n\n# A tibble: 2 × 8\n# Groups:   Species [1]\n  Species ID      NoPoints NoUniqueDates FirstDate  LastDate   SampleR…¹ Sampl…²\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;         &lt;int&gt; &lt;date&gt;     &lt;date&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 RFB     GY02215       53            11 2022-02-10 2022-02-26      285.    70.9\n2 RFB     GY02216       34             8 2022-02-10 2022-02-20      291.    91.4\n# … with abbreviated variable names ¹​SampleRate, ²​SampleRate_se\n\n\nCreate a table of population-level summary statistics:\n\ndf_summary_pop &lt;- df_summary_ind %&gt;% # use the individual-level summary data\n  group_by(across(grouping_factors_poplevel)) %&gt;%\n  summarise(NoInds = length(unique(ID)), # number of unique individuals\n            NoPoints_total = sum(NoPoints), # total number of tracking locations\n            FirstDate = as.Date(min(FirstDate)), # first tracking date\n            LastDate = as.Date(max(LastDate)), # last tracking date\n            PointsPerBird = mean(NoPoints), # number of locations per individual: mean\n            PointsPerBird_se = se(NoPoints), # number of locations per individual: standard error\n            DatesPerBird = mean(NoUniqueDates), # number of tracking days per bird: mean\n            DatesPerBird_se = se(NoUniqueDates), # number of tracking days per bird: standard error\n            SampleRate_mean = mean(SampleRate), # sample rate mean\n            SampleRate_se = se(SampleRate)) # sample rate standard error\ndf_summary_pop\n\n# A tibble: 1 × 11\n  Species NoInds NoPoint…¹ FirstDate  LastDate   Point…² Point…³ Dates…⁴ Dates…⁵\n  &lt;chr&gt;    &lt;int&gt;     &lt;int&gt; &lt;date&gt;     &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 RFB          2        87 2022-02-10 2022-02-26    43.5     9.5     9.5     1.5\n# … with 2 more variables: SampleRate_mean &lt;dbl&gt;, SampleRate_se &lt;dbl&gt;, and\n#   abbreviated variable names ¹​NoPoints_total, ²​PointsPerBird,\n#   ³​PointsPerBird_se, ⁴​DatesPerBird, ⁵​DatesPerBird_se\n\n\nRemove intermediate files/objects by specifying which objects to keep:\n\nrm(list=ls()[!ls() %in% c(\"df_filtered\", \"df_summary_ind\", \"df_summary_pop\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#save-data",
    "href": "User_guide.html#save-data",
    "title": "ExMove user guide",
    "section": "8. Save filtered and summary data",
    "text": "8. Save filtered and summary data\n\n\n\n\n\n\n🧠 User input required\n\n\n\nFirst we define the folder file path for saving our filtered data and create folder if not already present\n\nfilepath_filtered_out &lt;- here(\"DataOutputs\",\"WorkingDataFrames\")\ndir.create(filepath_filtered_out)\n\nThen we define the file path for saving summary dataframes, again creating folder if needed\n\nfilepath_summary_out &lt;- here(\"DataOutputs\",\"SummaryDataFrames\")\ndir.create(filepath_summary_out)\n\nHere we define file names for saved files, and paste the species code to _summary_, followed by ind (individual level) or pop (population level). You can change this if you want to use a different naming system.\n\nfilename_filtered_out &lt;- paste0(species_code, \"_filtered\")\nfilename_summary_ind_out &lt;- paste0(species_code, \"_summary_ind\")\nfilename_summary_pop_out &lt;- paste0(species_code, \"_summary_pop\")\n\n\n\nNow we can save all our dataframes as .csv files using our defined values\n\nwrite_csv(df_filtered, file = here(filepath_filtered_out, paste0(filename_filtered_out,\".csv\")))\nwrite_csv(df_summary_ind, file = here(filepath_summary_out, paste0(filename_summary_ind_out,\".csv\")))\nwrite_csv(df_summary_pop, file = here(filepath_summary_out, paste0(filename_summary_pop_out,\".csv\")))\n\nLastly we remove intermediate files/objects\n\nrm(list=ls()[!ls() %in% c(\"df_filtered\", \"df_summary_ind\", \"df_summary_pop\", \"species_code\")]) #specify objects to keep"
  },
  {
    "objectID": "User_guide.html#visualisation-i",
    "href": "User_guide.html#visualisation-i",
    "title": "ExMove user guide",
    "section": "9. Visualisation I",
    "text": "9. Visualisation I\n\n\n\n\n\n\nUser input required\n\n\n\nDefine parameters for reading out plots, and define device to read plots out as e.g. tiff/jpeg\n\ndevice &lt;- \"tiff\"\n\nDefine units for plot size (usually mm)\n\nunits &lt;- \"mm\"\n\nDefine plot resolution in dpi (300 is usually good minimum)\n\ndpi &lt;- 300\n\nDefine filepath to read out plots and create folder if absent\n\nout_path &lt;- here(\"DataOutputs\",\"Figures\")\ndir.create(out_path)\n\nWe plot maps over a topography base-layer which can include terrestrial (elevation) and marine (bathymetry/water depth) data. To set legend label for topography data, relevant to your data.\n\ntopo_label = \"Depth (m)\"\n\n\n\nLoad additional libraries for spatial visualisation (optional)\n\n\n\n\n\n\nIf you see a masking warning these are fine. Watch out for packages that aren’t installed yet\n\n\n\n\nlibrary(rnaturalearth)\nlibrary(marmap)\nlibrary(plotly)\n\n\n\nCreate version of data for plotting by transforming required columns to numeric and creating time elapsed columns\n\ndf_plotting &lt;- df_filtered %&gt;%\n  group_by(ID) %&gt;%\n  mutate(diffsecs = as.numeric(difftime),\n         secs_elapsed = cumsum(replace_na(diffsecs, 0)),\n         time_elapsed = as.duration(secs_elapsed),\n         days_elapsed = as.numeric(time_elapsed, \"days\")) %&gt;%\n  mutate(across(c(dist,speed, Lat, Lon), as.numeric))\n\nCreate a map of all points. Set the plot limits as the max and min lat/longs as the tracking data\nFirst set up a basemap to plot over: - Use rnatural earth low res countries basemap - co-ordinates in lat/long to match other spatial data\n\ncountries &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nDefine min and max co-ordinates based on extent of tracking data, for adding bathymetry extracted from NOAA database.\n\nminlon &lt;- min(df_plotting$Lon)\nmaxlon &lt;- max(df_plotting$Lon)\n\nminlat &lt;- min(df_plotting$Lat)\nmaxlat &lt;- max(df_plotting$Lat)\n\nLoad in bathymetry basemap. Set limits slightly beyond tracking data to make a buffer so no gaps when plotting\n\nbase_topography_map &lt;- getNOAA.bathy(\n  lon1 = minlon - 0.1, lon2 = maxlon + 0.1,\n  lat1 = minlat - 0.1, lat2 = maxlat + 0.1, \n  resolution = 1)\n\nFortify bathymetry data for plotting\n\nbase_topography_fort = fortify(base_topography_map)\n\nCreate base map with correct extent, topography, country outlines, etc.,\n\nmap_base &lt;- ggplot() + \n  geom_raster(data = base_topography_fort, aes(x=x, y=y, fill=z), alpha = 0.9) +\n  # add colour scheme for the fill\n  scale_fill_viridis_c(option=\"mako\", name = topo_label) + \n  # add map of countries over the top\n  geom_sf(data = countries, aes(geometry = geometry), fill = NA) + \n  # set plot limits\n  coord_sf(xlim = c(minlon-0.1, maxlon+0.1), \n           ylim = c(minlat-0.1, maxlat+0.1), crs = 4326, expand = F) +\n  # add labels\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  theme(axis.text=element_text(colour=\"black\"),\n        axis.title.x = element_text(size = 15),\n        axis.text.x = element_text(hjust=0.7),\n        axis.title.y = element_text(angle=90, vjust = 0.4, size = 15),\n        axis.text.y = element_text(hjust=0.7, angle=90, vjust=0.3)) +\n  # set a theme\n  theme_light()\nmap_base\n\n\n\n\n\n\nPopulation\nSeveral individuals\nOption: Many individuals\n\n\n\nPlot a combined map of all tracking locations:\n\nmap_alllocs &lt;- map_base + \n  # add GPS points\n  geom_point(data = df_plotting, \n             aes(x = Lon, y = Lat), \n             alpha = 0.8, size = 0.5, col = \"violetred3\") \nmap_alllocs\n\n\n\n\n\n\nPlot a map of individual locations, colouring points by speed, and faceting by ID\n\nmap_individuals &lt;- map_base + \n  # add GPS points and paths between them\n  geom_point(data = df_plotting, aes(x = Lon, y = Lat, col = speed), \n             alpha = 0.8, size = 0.5 ) +\n  geom_path(data = df_plotting, aes(x = Lon, y = Lat, col = speed), \n            alpha = 0.8, size = 0.5 ) +\n  # colour birds using scale_colour_gradient2\n  scale_colour_gradient2(name = \"Speed\", low = \"blue\", mid = \"white\", high = \"red\", \n                         midpoint = (max(df_plotting$speed,na.rm=TRUE)/2)) + # `midpoint` argument ensures an even transition of color across speed value\n  # facet for individual\n  facet_wrap(~ ID, ncol = round(sqrt(n_distinct(df_plotting$ID))))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nmap_individuals\n\n\n\n\n\n\nIn previous plots, we’ve split the population into individual facets. This works fine on the example code, where we only have a few individuals, but if you have more individuals and the facets are too small, you can split the plot onto multiple pages. Use the below code to use facet_wrap_paginate from the ggforce package:\n\n## ** Option ** ##\n## save plot as object to later extract number of pages\n## e.g., with 2 per page:\nmap_individuals &lt;- map_base + \n  # add GPS points and paths between them\n  geom_point(data = df_plotting, aes(x = Lon, y = Lat, col = speed), alpha = 0.8, size = 0.5 ) +\n  geom_path(data = df_plotting, aes(x = Lon, y = Lat, col = speed), alpha = 0.8, size = 0.5 ) +\n  # colour birds using scale_colour_gradient2\n  scale_colour_gradient2(name = \"Speed\", low = \"blue\", mid = \"white\", high = \"red\", midpoint = (max(df_plotting$speed,na.rm=TRUE)/2)) +\n  ##facet for individual\n  facet_wrap_paginate(~ID, ncol = 2, nrow= 1, page = 1)\n\nHow many pages of plots?\n\nn_pages(map_individuals)\n\nRun through different values of page to show each page in turn\n\nmap_individuals\n\n\n\n\nSave maps for further use using ggsave function.\n\nggsave(plot = map_alllocs, \n       filename = paste0(species_code, \"_map_all_locs.tiff\"),\n       device = device,\n       path = out_path, \n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nggsave(plot = map_individuals, \n       filename = paste0(species_code, \"_map_individuals.tiff\"),\n       device = device,\n       path = out_path, \n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nCreate a time series plot of speed, faceted for each individual.\n\nspeed_time_plot &lt;- df_plotting %&gt;% #speed over time\n  ggplot(data=., aes(x=days_elapsed, y=speed, group=ID)) +\n  # add line of speed over time\n  geom_line() + \n  # add axis labels\n  xlab(\"time elapsed (days)\") + ylab(\"speed (m/s)\") +\n  # facet by individual\n  facet_wrap(~ID, nrow= round(sqrt(n_distinct(df_plotting$ID)))) +\n  # set plotting theme\n  theme(axis.text=element_text(colour=\"black\")) +\n  theme_light()\nspeed_time_plot\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWarnings about non-finite values for speed/step length plots are expected and usually refer to the first location for each individual (i.e. number of non-finite values should be equal to number of individuals)\n\n\nSave plot for further use\n\nggsave(plot = speed_time_plot, \n       filename = paste0(species_code, \"_speed_timeseries_plot.tiff\"),\n       device = device,\n       path = out_path, \n       units = units, width = 200, height = 175, dpi = dpi  \n)\n\nCreate a histogram of point to point speeds (can adjust binwidth and x limits manually)\n\nspeed_hist &lt;- df_plotting %&gt;% \n  ggplot(data=., aes(speed)) +\n  geom_histogram(binwidth=0.1, alpha=0.7) + # can adjust binwidth to suite your needs\n  geom_density(aes(y =0.1*..count..)) +\n  # add plot labels\n  xlab(\"speed (m/s)\") + ylab(\"count\") +\n  # facet by individual\n  facet_wrap(~ID, nrow= round(sqrt(n_distinct(df_plotting$ID)))) +\n  # set plotting theme\n  theme(axis.text=element_text(colour=\"black\"))+\n  theme_light()\nspeed_hist\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nSave plot for further use\n\nggsave(plot = speed_hist, \n       filename = paste0(species_code, \"_speed_histogram.tiff\"),\n       device = device,\n       path = out_path,\n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nCreate a time series plot of step lengths (faceted for each individual)\n\nstep_time_plot &lt;- df_plotting %&gt;% #step length over time\n  ggplot(data=., aes(x=days_elapsed, y=as.numeric(netdisp), group=ID)) +\n  geom_line() +\n  # add plot labels\n  xlab(\"time elapsed (days)\") + ylab(\"Distance from first fix (m)\") +\n  # facet by individual\n  facet_wrap(~ID, nrow= round(sqrt(n_distinct(df_plotting$ID)))) +\n  # set plotting theme\n  theme(axis.text=element_text(colour=\"black\")) +\n  theme_light()\nstep_time_plot\n\n\n\n\nSave plot for further use\n\nggsave(plot = step_time_plot, \n       filename = paste0(species_code, \"_step_time_plot.tiff\"),\n       device = device,\n       path = out_path,\n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nCreate a histogram of step lengths (can adjust binwidth and x limits manually)\n\nstep_hist &lt;- df_plotting %&gt;% #step histogram\n  ggplot(data=., aes(as.numeric(dist))) +\n  geom_histogram(binwidth=1, alpha=0.7) + # can adjust binwidth to suite your needs\n  geom_density(aes(y =1*..count..)) +\n  # add plot labels\n  xlab(\"step length (m)\") + ylab(\"count\") +\n  # facet by individual\n  facet_wrap(~ID, nrow= round(sqrt(n_distinct(df_plotting$ID))))+\n  # set plotting theme\n  theme_light()\n\nSave plot for further use\n\nggsave(plot = step_hist, \n       filename = paste0(species_code, \"_step_hist.tiff\"),\n       device = device,\n       path = out_path,\n       units = units, width = 200, height = 175, dpi = dpi,   \n)\n\nLastly, we remove intermediate files/objects if necessary to speed up any post-processing steps\n\nrm(list=ls()[!ls() %in% c(\"species_code\")]) #specify objects to keep"
  }
]